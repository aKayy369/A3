{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01adf4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required modules \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0603bf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to AIT MLflow server as admin; experiment 'A3_st125999' set.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "mlflow.set_tracking_uri(\"https://mlflow.ml.brain.cs.ait.ac.th\")\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"admin\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"password\"\n",
    "\n",
    "mlflow.set_experiment(\"st125999_a3\")\n",
    "\n",
    "print(\"✅ Connected to AIT MLflow server as admin; experiment 'A3_st125999' set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3fb03c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>owner</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>torque</th>\n",
       "      <th>seats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maruti Swift Dzire VDI</td>\n",
       "      <td>2014</td>\n",
       "      <td>450000</td>\n",
       "      <td>145500</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.4 kmpl</td>\n",
       "      <td>1248 CC</td>\n",
       "      <td>74 bhp</td>\n",
       "      <td>190Nm@ 2000rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skoda Rapid 1.5 TDI Ambition</td>\n",
       "      <td>2014</td>\n",
       "      <td>370000</td>\n",
       "      <td>120000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Second Owner</td>\n",
       "      <td>21.14 kmpl</td>\n",
       "      <td>1498 CC</td>\n",
       "      <td>103.52 bhp</td>\n",
       "      <td>250Nm@ 1500-2500rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda City 2017-2020 EXi</td>\n",
       "      <td>2006</td>\n",
       "      <td>158000</td>\n",
       "      <td>140000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Third Owner</td>\n",
       "      <td>17.7 kmpl</td>\n",
       "      <td>1497 CC</td>\n",
       "      <td>78 bhp</td>\n",
       "      <td>12.7@ 2,700(kgm@ rpm)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyundai i20 Sportz Diesel</td>\n",
       "      <td>2010</td>\n",
       "      <td>225000</td>\n",
       "      <td>127000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>23.0 kmpl</td>\n",
       "      <td>1396 CC</td>\n",
       "      <td>90 bhp</td>\n",
       "      <td>22.4 kgm at 1750-2750rpm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maruti Swift VXI BSIII</td>\n",
       "      <td>2007</td>\n",
       "      <td>130000</td>\n",
       "      <td>120000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First Owner</td>\n",
       "      <td>16.1 kmpl</td>\n",
       "      <td>1298 CC</td>\n",
       "      <td>88.2 bhp</td>\n",
       "      <td>11.5@ 4,500(kgm@ rpm)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name  year  selling_price  km_driven    fuel  \\\n",
       "0        Maruti Swift Dzire VDI  2014         450000     145500  Diesel   \n",
       "1  Skoda Rapid 1.5 TDI Ambition  2014         370000     120000  Diesel   \n",
       "2      Honda City 2017-2020 EXi  2006         158000     140000  Petrol   \n",
       "3     Hyundai i20 Sportz Diesel  2010         225000     127000  Diesel   \n",
       "4        Maruti Swift VXI BSIII  2007         130000     120000  Petrol   \n",
       "\n",
       "  seller_type transmission         owner     mileage   engine   max_power  \\\n",
       "0  Individual       Manual   First Owner   23.4 kmpl  1248 CC      74 bhp   \n",
       "1  Individual       Manual  Second Owner  21.14 kmpl  1498 CC  103.52 bhp   \n",
       "2  Individual       Manual   Third Owner   17.7 kmpl  1497 CC      78 bhp   \n",
       "3  Individual       Manual   First Owner   23.0 kmpl  1396 CC      90 bhp   \n",
       "4  Individual       Manual   First Owner   16.1 kmpl  1298 CC    88.2 bhp   \n",
       "\n",
       "                     torque  seats  \n",
       "0            190Nm@ 2000rpm    5.0  \n",
       "1       250Nm@ 1500-2500rpm    5.0  \n",
       "2     12.7@ 2,700(kgm@ rpm)    5.0  \n",
       "3  22.4 kgm at 1750-2750rpm    5.0  \n",
       "4     11.5@ 4,500(kgm@ rpm)    5.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Cars.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f2ef3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8128, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6551cab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mappings = {'First Owner':1,\n",
    "            'Second Owner':2,\n",
    "            'Third Owner':3,\n",
    "            'Fourth & Above Owner':4,\n",
    "            'Test Drive Car':5}\n",
    "df['owner']=df['owner'].map(mappings)\n",
    "df['owner'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa3c302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Diesel', 'Petrol'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[(df['fuel']!='LPG') & (df['fuel']!='CNG')]\n",
    "df['fuel'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe396f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>engine</th>\n",
       "      <th>mileage</th>\n",
       "      <th>max_power</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1248.0</td>\n",
       "      <td>23.40</td>\n",
       "      <td>74.00</td>\n",
       "      <td>Maruti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1498.0</td>\n",
       "      <td>21.14</td>\n",
       "      <td>103.52</td>\n",
       "      <td>Skoda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1497.0</td>\n",
       "      <td>17.70</td>\n",
       "      <td>78.00</td>\n",
       "      <td>Honda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1396.0</td>\n",
       "      <td>23.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>Hyundai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1298.0</td>\n",
       "      <td>16.10</td>\n",
       "      <td>88.20</td>\n",
       "      <td>Maruti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   engine  mileage  max_power    brand\n",
       "0  1248.0    23.40      74.00   Maruti\n",
       "1  1498.0    21.14     103.52    Skoda\n",
       "2  1497.0    17.70      78.00    Honda\n",
       "3  1396.0    23.00      90.00  Hyundai\n",
       "4  1298.0    16.10      88.20   Maruti"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mileage']=df.mileage.str.split()\n",
    "df['mileage']=df['mileage'].str[0].astype(float)\n",
    "\n",
    "df['engine']=df.engine.str.split()\n",
    "df['engine']=df['engine'].str[0].astype(float)\n",
    "\n",
    "df['max_power']=df.max_power.str.split()\n",
    "df['max_power']=df['max_power'].str[0].astype(float)\n",
    "\n",
    "df['brand']=df.name.str.split()\n",
    "df['brand']=df['brand'].str[0].astype(str)\n",
    "                                    \n",
    "\n",
    "df[['engine','mileage','max_power','brand']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "693d6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(labels='torque',axis='columns')\n",
    "df=df.drop(labels='name',axis='columns')\n",
    "df=df[df['owner']!=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c3e1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/4003814106.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"seats\"].fillna(df[\"seats\"].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 2️⃣ Fill all missing values in one step\n",
    "df[[\"mileage\", \"engine\", \"max_power\"]] = (\n",
    "    df.groupby(\"brand\")[[\"mileage\", \"engine\", \"max_power\"]]\n",
    "      .transform(lambda x: x.fillna(x.median()))\n",
    ")\n",
    "\n",
    "# 3️⃣ Fill seats with mode (most frequent value)\n",
    "df[\"seats\"].fillna(df[\"seats\"].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d77cae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"mileage\", \"engine\", \"max_power\"], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516ab2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>owner</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>seats</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>450000</td>\n",
       "      <td>145500</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1</td>\n",
       "      <td>23.40</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Maruti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>370000</td>\n",
       "      <td>120000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>2</td>\n",
       "      <td>21.14</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>103.52</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Skoda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006</td>\n",
       "      <td>158000</td>\n",
       "      <td>140000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>3</td>\n",
       "      <td>17.70</td>\n",
       "      <td>1497.0</td>\n",
       "      <td>78.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Honda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>225000</td>\n",
       "      <td>127000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1</td>\n",
       "      <td>23.00</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>90.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hyundai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>130000</td>\n",
       "      <td>120000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1</td>\n",
       "      <td>16.10</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>88.20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Maruti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  selling_price  km_driven    fuel seller_type transmission  owner  \\\n",
       "0  2014         450000     145500  Diesel  Individual       Manual      1   \n",
       "1  2014         370000     120000  Diesel  Individual       Manual      2   \n",
       "2  2006         158000     140000  Petrol  Individual       Manual      3   \n",
       "3  2010         225000     127000  Diesel  Individual       Manual      1   \n",
       "4  2007         130000     120000  Petrol  Individual       Manual      1   \n",
       "\n",
       "   mileage  engine  max_power  seats    brand  \n",
       "0    23.40  1248.0      74.00    5.0   Maruti  \n",
       "1    21.14  1498.0     103.52    5.0    Skoda  \n",
       "2    17.70  1497.0      78.00    5.0    Honda  \n",
       "3    23.00  1396.0      90.00    5.0  Hyundai  \n",
       "4    16.10  1298.0      88.20    5.0   Maruti  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33f94783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year             0\n",
       "selling_price    0\n",
       "km_driven        0\n",
       "fuel             0\n",
       "seller_type      0\n",
       "transmission     0\n",
       "owner            0\n",
       "mileage          0\n",
       "engine           0\n",
       "max_power        0\n",
       "seats            0\n",
       "brand            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4048ef8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8027, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef0259",
   "metadata": {},
   "source": [
    "#using the preprocessed dataset from a1/a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa58160",
   "metadata": {},
   "source": [
    "A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6008b53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    8.027000e+03\n",
      "mean     6.404663e+05\n",
      "std      8.027251e+05\n",
      "min      2.999900e+04\n",
      "25%      2.600000e+05\n",
      "50%      4.500000e+05\n",
      "75%      6.800000e+05\n",
      "max      1.000000e+07\n",
      "Name: selling_price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#checking the quartile ranges of selling price label\n",
    "print(df[\"selling_price\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfc3178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   selling_price  selling_price_binned\n",
      "0         450000                     1\n",
      "1         370000                     1\n",
      "2         158000                     0\n",
      "3         225000                     0\n",
      "4         130000                     0\n",
      "5         440000                     1\n",
      "6          45000                     0\n",
      "7         350000                     1\n",
      "8         200000                     0\n",
      "9         500000                     2\n",
      "\n",
      "Class distribution:\n",
      "selling_price_binned\n",
      "0    2049\n",
      "1    2044\n",
      "3    1991\n",
      "2    1943\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# biining selling price into 4 classes based on percentiles (using pd.cut())\n",
    "import pandas as pd\n",
    "\n",
    "bins = [0, 260000, 450000, 680000, df[\"selling_price\"].max()]\n",
    "\n",
    "# class labels are 0, 1, 2, 3\n",
    "labels = [0, 1, 2, 3]\n",
    "\n",
    "# binning the selling_price into 4 classes\n",
    "df[\"selling_price_binned\"] = pd.cut(\n",
    "    df[\"selling_price\"],\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    include_lowest=True\n",
    ").astype(int)\n",
    "\n",
    "# printing the binned classes\n",
    "print(df[[\"selling_price\", \"selling_price_binned\"]].head(10))\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df[\"selling_price_binned\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586bceb",
   "metadata": {},
   "source": [
    "#the price classes seem equally balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76800460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>fuel</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>transmission</th>\n",
       "      <th>owner</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>seats</th>\n",
       "      <th>brand</th>\n",
       "      <th>selling_price_binned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>450000</td>\n",
       "      <td>145500</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1</td>\n",
       "      <td>23.40</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Maruti</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>370000</td>\n",
       "      <td>120000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>2</td>\n",
       "      <td>21.14</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>103.52</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Skoda</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006</td>\n",
       "      <td>158000</td>\n",
       "      <td>140000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>3</td>\n",
       "      <td>17.70</td>\n",
       "      <td>1497.0</td>\n",
       "      <td>78.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Honda</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>225000</td>\n",
       "      <td>127000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1</td>\n",
       "      <td>23.00</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>90.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hyundai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>130000</td>\n",
       "      <td>120000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>1</td>\n",
       "      <td>16.10</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>88.20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Maruti</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  selling_price  km_driven    fuel seller_type transmission  owner  \\\n",
       "0  2014         450000     145500  Diesel  Individual       Manual      1   \n",
       "1  2014         370000     120000  Diesel  Individual       Manual      2   \n",
       "2  2006         158000     140000  Petrol  Individual       Manual      3   \n",
       "3  2010         225000     127000  Diesel  Individual       Manual      1   \n",
       "4  2007         130000     120000  Petrol  Individual       Manual      1   \n",
       "\n",
       "   mileage  engine  max_power  seats    brand  selling_price_binned  \n",
       "0    23.40  1248.0      74.00    5.0   Maruti                     1  \n",
       "1    21.14  1498.0     103.52    5.0    Skoda                     1  \n",
       "2    17.70  1497.0      78.00    5.0    Honda                     0  \n",
       "3    23.00  1396.0      90.00    5.0  Hyundai                     0  \n",
       "4    16.10  1298.0      88.20    5.0   Maruti                     0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "760a38de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting to check blance of classes\n",
    "import matplotlib.pyplot as plt\n",
    "df[\"selling_price_binned\"].value_counts().sort_index().plot(kind='bar')\n",
    "plt.xlabel(\"selling price Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd6dbf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                      int64\n",
       "selling_price             int64\n",
       "km_driven                 int64\n",
       "fuel                     object\n",
       "seller_type              object\n",
       "transmission             object\n",
       "owner                     int64\n",
       "mileage                 float64\n",
       "engine                  float64\n",
       "max_power               float64\n",
       "seats                   float64\n",
       "brand                    object\n",
       "selling_price_binned      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb4d98",
   "metadata": {},
   "source": [
    "#As seen above the price classes(created above) are mostly balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3adf4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_brand = LabelEncoder()\n",
    "le_fuel = LabelEncoder()\n",
    "\n",
    "df['brand'] = le_brand.fit_transform(df['brand'])\n",
    "df['fuel'] = le_fuel.fit_transform(df['fuel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9294dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (8027, 5)\n",
      "y shape: (8027,)\n",
      "X type: <class 'pandas.core.frame.DataFrame'>\n",
      "y type: <class 'pandas.core.series.Series'>\n",
      "Unique y values: [1 0 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Features\n",
    "numeric_features = ['year', 'max_power', 'mileage']\n",
    "categorical_features = ['brand', 'fuel']\n",
    "\n",
    "X = df[numeric_features + categorical_features]\n",
    "\n",
    "# Target (binned classes, already 0–3)\n",
    "y = df[\"selling_price_binned\"].astype(int)\n",
    "\n",
    "# Check shapes & types\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"X type:\", type(X))\n",
    "print(\"y type:\", type(y))\n",
    "print(\"Unique y values:\", y.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c11d481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d53cb529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5618,), (2409,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63ed69af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      year  max_power  mileage  brand  fuel\n",
      "6764  2015      72.40    13.58      4     0\n",
      "1485  2013      62.10    15.96     19     0\n",
      "5738  2015      68.05    19.81     11     1\n",
      "1272  2010      90.00    18.80     27     0\n",
      "1411  2010      90.00    23.00     11     0\n",
      "...    ...        ...      ...    ...   ...\n",
      "5226  2013      78.90    20.36     11     1\n",
      "5390  2016      67.04    20.51     20     1\n",
      "860   2015      88.76    20.77     20     0\n",
      "7603  2017     177.00    19.33     13     0\n",
      "7270  2017      97.90    25.60     10     0\n",
      "\n",
      "[5618 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be6f80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# feature scaling helps improve reach convergence faster\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "518c2a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.29292221 -0.5546834  -1.45939196 -1.94723645 -0.89898559]\n",
      " [-0.20765177 -0.84660999 -0.85939564  0.17122126 -0.89898559]\n",
      " [ 0.29292221 -0.67797278  0.11118665 -0.95862285  1.11236488]\n",
      " ...\n",
      " [ 0.29292221 -0.09100194  0.35320197  0.31245177 -0.89898559]\n",
      " [ 0.79349619  2.40993031 -0.00982101 -0.67616183 -0.89898559]\n",
      " [ 0.79349619  0.16804748  1.57084157 -1.09985337 -0.89898559]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86e0f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add intercept to our X\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)  #add intercept\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)  #add intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82a5ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure our y is in the shape of (m, k)\n",
    "# we will convert our output vector in \n",
    "# matrix where no. of columns is equal to the no. of classes. \n",
    "# The values in the matrix will be 0 or 1. For instance the rows \n",
    "# where we have output 2 the column 2 will contain 1 and the rest are all 0.\n",
    "# in simple words, y will be of shape (m, k)\n",
    "k = len(set(y))  # no. of class  (can also use np.unique)\n",
    "m = X_train.shape[0]  # no.of samples\n",
    "n = X_train.shape[1]  # no. of features\n",
    "Y_train_encoded = np.zeros((m, k))\n",
    "for each_class in range(k):\n",
    "    cond = y_train==each_class\n",
    "    Y_train_encoded[np.where(cond), each_class] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d323aeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5618, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "185f421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, k, n, method, alpha=0.001, epochs=500, max_iter=5000, use_penalty=False, lambda_=0.01):\n",
    "       \n",
    "        self.k = k  \n",
    "        self.n = n  \n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.max_iter = max_iter\n",
    "        self.method = method\n",
    "        self.use_penalty = use_penalty\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.W = np.random.rand(self.n, self.k)\n",
    "        self.losses = []\n",
    "\n",
    "        # Steps per epoch for info/logging only\n",
    "        steps_per_epoch = self.max_iter // self.epochs\n",
    "\n",
    "        if self.method == \"batch\":\n",
    "            start_time = time.time()\n",
    "            for i in range(self.max_iter):\n",
    "                loss, grad = self.gradient(X, Y)\n",
    "                self.losses.append(loss)\n",
    "                self.W -= self.alpha * grad\n",
    "                if i % steps_per_epoch == 0:\n",
    "                    epoch_num = i // steps_per_epoch\n",
    "                    print(f\"Epoch {epoch_num}/{self.epochs} | Iter {i} | Loss: {loss:.4f}\")\n",
    "            print(f\"Time taken: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        elif self.method == \"minibatch\":\n",
    "            start_time = time.time()\n",
    "            batch_size = int(0.3 * X.shape[0])\n",
    "            for i in range(self.max_iter):\n",
    "                ix = np.random.randint(0, X.shape[0])\n",
    "                batch_X = X[ix:ix+batch_size]\n",
    "                batch_Y = Y[ix:ix+batch_size]\n",
    "                loss, grad = self.gradient(batch_X, batch_Y)\n",
    "                self.losses.append(loss)\n",
    "                self.W -= self.alpha * grad\n",
    "                if i % steps_per_epoch == 0:\n",
    "                    epoch_num = i // steps_per_epoch\n",
    "                    print(f\"Epoch {epoch_num}/{self.epochs} | Iter {i} | Loss: {loss:.4f}\")\n",
    "            print(f\"Time taken: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        elif self.method == \"sto\":\n",
    "            start_time = time.time()\n",
    "            used_ix = []\n",
    "            for i in range(self.max_iter):\n",
    "                idx = np.random.randint(X.shape[0])\n",
    "                while i in used_ix:\n",
    "                    idx = np.random.randint(X.shape[0])\n",
    "                X_train = X[idx, :].reshape(1, -1)\n",
    "                Y_train = Y[idx]\n",
    "                loss, grad = self.gradient(X_train, Y_train)\n",
    "                self.losses.append(loss)\n",
    "                self.W -= self.alpha * grad\n",
    "\n",
    "                used_ix.append(i)\n",
    "                if len(used_ix) == X.shape[0]:\n",
    "                    used_ix = []\n",
    "\n",
    "                if i % steps_per_epoch == 0:\n",
    "                    epoch_num = i // steps_per_epoch\n",
    "                    print(f\"Epoch {epoch_num}/{self.epochs} | Iter {i} | Loss: {loss:.4f}\")\n",
    "            print(f\"Time taken: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Method must be \"batch\", \"minibatch\" or \"sto\".')\n",
    "\n",
    "    def gradient(self, X, Y):\n",
    "        m = X.shape[0]\n",
    "        h = self.h_theta(X, self.W)\n",
    "        # Cross-entropy loss\n",
    "        loss = - np.sum(Y * np.log(h + 1e-9)) / m\n",
    "        # Add Ridge (L2) penalty\n",
    "        if self.use_penalty:\n",
    "            loss += (self.lambda_ / (2 * m)) * np.sum(self.W ** 2)\n",
    "        error = h - Y\n",
    "        grad = self.softmax_grad(X, error) / m\n",
    "        # Add penalty to gradient\n",
    "        if self.use_penalty:\n",
    "            grad += (self.lambda_ / m) * self.W\n",
    "        return loss, grad\n",
    "\n",
    "    def softmax(self, theta_t_x):\n",
    "        exp_scores = np.exp(theta_t_x - np.max(theta_t_x, axis=1, keepdims=True))  # stability\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def softmax_grad(self, X, error):\n",
    "        return X.T @ error\n",
    "\n",
    "    def h_theta(self, X, W):\n",
    "        return self.softmax(X @ W)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return np.argmax(self.h_theta(X_test, self.W), axis=1)\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.plot(np.arange(len(self.losses)), self.losses, label=\"Train Losses\")\n",
    "        plt.title(\"Loss Curve\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # -----------------------\n",
    "    # Evaluation Metrics\n",
    "    # -----------------------\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        return np.sum(y_true == y_pred) / len(y_true)\n",
    "    \n",
    "    def precision_recall_f1_per_class(self, y_true, y_pred):\n",
    "        classes = np.unique(y_true)\n",
    "        precision, recall, f1, support = {}, {}, {}, {}\n",
    "        for c in classes:\n",
    "            TP = np.sum((y_true == c) & (y_pred == c))\n",
    "            FP = np.sum((y_true != c) & (y_pred == c))\n",
    "            FN = np.sum((y_true == c) & (y_pred != c))\n",
    "            prec = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "            rec = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "            f1_c = (2 * prec * rec) / (prec + rec) if (prec + rec) != 0 else 0\n",
    "            precision[c], recall[c], f1[c], support[c] = prec, rec, f1_c, np.sum(y_true == c)\n",
    "        return precision, recall, f1, support\n",
    "    \n",
    "    def macro_avg(self, precision, recall, f1):\n",
    "        return np.mean(list(precision.values())), np.mean(list(recall.values())), np.mean(list(f1.values()))\n",
    "    \n",
    "    def weighted_avg(self, precision, recall, f1, support):\n",
    "        total = np.sum(list(support.values()))\n",
    "        wp = np.sum([precision[c] * support[c] for c in support]) / total\n",
    "        wr = np.sum([recall[c] * support[c] for c in support]) / total\n",
    "        wf = np.sum([f1[c] * support[c] for c in support]) / total\n",
    "        return wp, wr, wf\n",
    "    \n",
    "    def classification_report(self, y_true, y_pred):\n",
    "        precision, recall, f1, support = self.precision_recall_f1_per_class(y_true, y_pred)\n",
    "        macro_p, macro_r, macro_f = self.macro_avg(precision, recall, f1)\n",
    "        weighted_p, weighted_r, weighted_f = self.weighted_avg(precision, recall, f1, support)\n",
    "        acc = self.accuracy(y_true, y_pred)\n",
    "\n",
    "        print(\"Class\\tPrecision\\tRecall\\tF1-Score\\tSupport\")\n",
    "        print(\"-\" * 60)\n",
    "        for c in precision.keys():\n",
    "            print(f\"{c}\\t{precision[c]:.2f}\\t\\t{recall[c]:.2f}\\t{f1[c]:.2f}\\t\\t{support[c]}\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Macro Avg\\t{macro_p:.2f}\\t\\t{macro_r:.2f}\\t{macro_f:.2f}\")\n",
    "        print(f\"Weighted Avg\\t{weighted_p:.2f}\\t\\t{weighted_r:.2f}\\t{weighted_f:.2f}\")\n",
    "        print(f\"Accuracy\\t{acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfe65930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500 | Iter 0 | Loss: 1.6134\n",
      "Epoch 1/500 | Iter 4 | Loss: 1.5999\n",
      "Epoch 2/500 | Iter 8 | Loss: 1.5865\n",
      "Epoch 3/500 | Iter 12 | Loss: 1.5735\n",
      "Epoch 4/500 | Iter 16 | Loss: 1.5607\n",
      "Epoch 5/500 | Iter 20 | Loss: 1.5481\n",
      "Epoch 6/500 | Iter 24 | Loss: 1.5358\n",
      "Epoch 7/500 | Iter 28 | Loss: 1.5238\n",
      "Epoch 8/500 | Iter 32 | Loss: 1.5119\n",
      "Epoch 9/500 | Iter 36 | Loss: 1.5003\n",
      "Epoch 10/500 | Iter 40 | Loss: 1.4890\n",
      "Epoch 11/500 | Iter 44 | Loss: 1.4778\n",
      "Epoch 12/500 | Iter 48 | Loss: 1.4669\n",
      "Epoch 13/500 | Iter 52 | Loss: 1.4562\n",
      "Epoch 14/500 | Iter 56 | Loss: 1.4457\n",
      "Epoch 15/500 | Iter 60 | Loss: 1.4355\n",
      "Epoch 16/500 | Iter 64 | Loss: 1.4254\n",
      "Epoch 17/500 | Iter 68 | Loss: 1.4156\n",
      "Epoch 18/500 | Iter 72 | Loss: 1.4060\n",
      "Epoch 19/500 | Iter 76 | Loss: 1.3965\n",
      "Epoch 20/500 | Iter 80 | Loss: 1.3873\n",
      "Epoch 21/500 | Iter 84 | Loss: 1.3783\n",
      "Epoch 22/500 | Iter 88 | Loss: 1.3694\n",
      "Epoch 23/500 | Iter 92 | Loss: 1.3608\n",
      "Epoch 24/500 | Iter 96 | Loss: 1.3523\n",
      "Epoch 25/500 | Iter 100 | Loss: 1.3440\n",
      "Epoch 26/500 | Iter 104 | Loss: 1.3359\n",
      "Epoch 27/500 | Iter 108 | Loss: 1.3279\n",
      "Epoch 28/500 | Iter 112 | Loss: 1.3202\n",
      "Epoch 29/500 | Iter 116 | Loss: 1.3126\n",
      "Epoch 30/500 | Iter 120 | Loss: 1.3051\n",
      "Epoch 31/500 | Iter 124 | Loss: 1.2979\n",
      "Epoch 32/500 | Iter 128 | Loss: 1.2908\n",
      "Epoch 33/500 | Iter 132 | Loss: 1.2838\n",
      "Epoch 34/500 | Iter 136 | Loss: 1.2770\n",
      "Epoch 35/500 | Iter 140 | Loss: 1.2704\n",
      "Epoch 36/500 | Iter 144 | Loss: 1.2638\n",
      "Epoch 37/500 | Iter 148 | Loss: 1.2575\n",
      "Epoch 38/500 | Iter 152 | Loss: 1.2513\n",
      "Epoch 39/500 | Iter 156 | Loss: 1.2452\n",
      "Epoch 40/500 | Iter 160 | Loss: 1.2392\n",
      "Epoch 41/500 | Iter 164 | Loss: 1.2334\n",
      "Epoch 42/500 | Iter 168 | Loss: 1.2277\n",
      "Epoch 43/500 | Iter 172 | Loss: 1.2222\n",
      "Epoch 44/500 | Iter 176 | Loss: 1.2167\n",
      "Epoch 45/500 | Iter 180 | Loss: 1.2114\n",
      "Epoch 46/500 | Iter 184 | Loss: 1.2062\n",
      "Epoch 47/500 | Iter 188 | Loss: 1.2011\n",
      "Epoch 48/500 | Iter 192 | Loss: 1.1961\n",
      "Epoch 49/500 | Iter 196 | Loss: 1.1912\n",
      "Epoch 50/500 | Iter 200 | Loss: 1.1865\n",
      "Epoch 51/500 | Iter 204 | Loss: 1.1818\n",
      "Epoch 52/500 | Iter 208 | Loss: 1.1772\n",
      "Epoch 53/500 | Iter 212 | Loss: 1.1728\n",
      "Epoch 54/500 | Iter 216 | Loss: 1.1684\n",
      "Epoch 55/500 | Iter 220 | Loss: 1.1641\n",
      "Epoch 56/500 | Iter 224 | Loss: 1.1599\n",
      "Epoch 57/500 | Iter 228 | Loss: 1.1558\n",
      "Epoch 58/500 | Iter 232 | Loss: 1.1518\n",
      "Epoch 59/500 | Iter 236 | Loss: 1.1479\n",
      "Epoch 60/500 | Iter 240 | Loss: 1.1441\n",
      "Epoch 61/500 | Iter 244 | Loss: 1.1403\n",
      "Epoch 62/500 | Iter 248 | Loss: 1.1366\n",
      "Epoch 63/500 | Iter 252 | Loss: 1.1330\n",
      "Epoch 64/500 | Iter 256 | Loss: 1.1294\n",
      "Epoch 65/500 | Iter 260 | Loss: 1.1260\n",
      "Epoch 66/500 | Iter 264 | Loss: 1.1226\n",
      "Epoch 67/500 | Iter 268 | Loss: 1.1192\n",
      "Epoch 68/500 | Iter 272 | Loss: 1.1160\n",
      "Epoch 69/500 | Iter 276 | Loss: 1.1128\n",
      "Epoch 70/500 | Iter 280 | Loss: 1.1096\n",
      "Epoch 71/500 | Iter 284 | Loss: 1.1066\n",
      "Epoch 72/500 | Iter 288 | Loss: 1.1035\n",
      "Epoch 73/500 | Iter 292 | Loss: 1.1006\n",
      "Epoch 74/500 | Iter 296 | Loss: 1.0977\n",
      "Epoch 75/500 | Iter 300 | Loss: 1.0948\n",
      "Epoch 76/500 | Iter 304 | Loss: 1.0920\n",
      "Epoch 77/500 | Iter 308 | Loss: 1.0893\n",
      "Epoch 78/500 | Iter 312 | Loss: 1.0866\n",
      "Epoch 79/500 | Iter 316 | Loss: 1.0839\n",
      "Epoch 80/500 | Iter 320 | Loss: 1.0813\n",
      "Epoch 81/500 | Iter 324 | Loss: 1.0788\n",
      "Epoch 82/500 | Iter 328 | Loss: 1.0763\n",
      "Epoch 83/500 | Iter 332 | Loss: 1.0738\n",
      "Epoch 84/500 | Iter 336 | Loss: 1.0714\n",
      "Epoch 85/500 | Iter 340 | Loss: 1.0691\n",
      "Epoch 86/500 | Iter 344 | Loss: 1.0667\n",
      "Epoch 87/500 | Iter 348 | Loss: 1.0644\n",
      "Epoch 88/500 | Iter 352 | Loss: 1.0622\n",
      "Epoch 89/500 | Iter 356 | Loss: 1.0600\n",
      "Epoch 90/500 | Iter 360 | Loss: 1.0578\n",
      "Epoch 91/500 | Iter 364 | Loss: 1.0556\n",
      "Epoch 92/500 | Iter 368 | Loss: 1.0535\n",
      "Epoch 93/500 | Iter 372 | Loss: 1.0515\n",
      "Epoch 94/500 | Iter 376 | Loss: 1.0494\n",
      "Epoch 95/500 | Iter 380 | Loss: 1.0474\n",
      "Epoch 96/500 | Iter 384 | Loss: 1.0455\n",
      "Epoch 97/500 | Iter 388 | Loss: 1.0435\n",
      "Epoch 98/500 | Iter 392 | Loss: 1.0416\n",
      "Epoch 99/500 | Iter 396 | Loss: 1.0397\n",
      "Epoch 100/500 | Iter 400 | Loss: 1.0379\n",
      "Epoch 101/500 | Iter 404 | Loss: 1.0361\n",
      "Epoch 102/500 | Iter 408 | Loss: 1.0343\n",
      "Epoch 103/500 | Iter 412 | Loss: 1.0325\n",
      "Epoch 104/500 | Iter 416 | Loss: 1.0308\n",
      "Epoch 105/500 | Iter 420 | Loss: 1.0290\n",
      "Epoch 106/500 | Iter 424 | Loss: 1.0274\n",
      "Epoch 107/500 | Iter 428 | Loss: 1.0257\n",
      "Epoch 108/500 | Iter 432 | Loss: 1.0240\n",
      "Epoch 109/500 | Iter 436 | Loss: 1.0224\n",
      "Epoch 110/500 | Iter 440 | Loss: 1.0208\n",
      "Epoch 111/500 | Iter 444 | Loss: 1.0193\n",
      "Epoch 112/500 | Iter 448 | Loss: 1.0177\n",
      "Epoch 113/500 | Iter 452 | Loss: 1.0162\n",
      "Epoch 114/500 | Iter 456 | Loss: 1.0147\n",
      "Epoch 115/500 | Iter 460 | Loss: 1.0132\n",
      "Epoch 116/500 | Iter 464 | Loss: 1.0117\n",
      "Epoch 117/500 | Iter 468 | Loss: 1.0103\n",
      "Epoch 118/500 | Iter 472 | Loss: 1.0089\n",
      "Epoch 119/500 | Iter 476 | Loss: 1.0074\n",
      "Epoch 120/500 | Iter 480 | Loss: 1.0061\n",
      "Epoch 121/500 | Iter 484 | Loss: 1.0047\n",
      "Epoch 122/500 | Iter 488 | Loss: 1.0033\n",
      "Epoch 123/500 | Iter 492 | Loss: 1.0020\n",
      "Epoch 124/500 | Iter 496 | Loss: 1.0007\n",
      "Epoch 125/500 | Iter 500 | Loss: 0.9994\n",
      "Epoch 126/500 | Iter 504 | Loss: 0.9981\n",
      "Epoch 127/500 | Iter 508 | Loss: 0.9968\n",
      "Epoch 128/500 | Iter 512 | Loss: 0.9955\n",
      "Epoch 129/500 | Iter 516 | Loss: 0.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:99: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return self.softmax(X @ W)\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:99: RuntimeWarning: overflow encountered in matmul\n",
      "  return self.softmax(X @ W)\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:99: RuntimeWarning: invalid value encountered in matmul\n",
      "  return self.softmax(X @ W)\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:96: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X.T @ error\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:96: RuntimeWarning: overflow encountered in matmul\n",
      "  return X.T @ error\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:96: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X.T @ error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/500 | Iter 520 | Loss: 0.9931\n",
      "Epoch 131/500 | Iter 524 | Loss: 0.9919\n",
      "Epoch 132/500 | Iter 528 | Loss: 0.9907\n",
      "Epoch 133/500 | Iter 532 | Loss: 0.9895\n",
      "Epoch 134/500 | Iter 536 | Loss: 0.9883\n",
      "Epoch 135/500 | Iter 540 | Loss: 0.9872\n",
      "Epoch 136/500 | Iter 544 | Loss: 0.9860\n",
      "Epoch 137/500 | Iter 548 | Loss: 0.9849\n",
      "Epoch 138/500 | Iter 552 | Loss: 0.9838\n",
      "Epoch 139/500 | Iter 556 | Loss: 0.9827\n",
      "Epoch 140/500 | Iter 560 | Loss: 0.9816\n",
      "Epoch 141/500 | Iter 564 | Loss: 0.9805\n",
      "Epoch 142/500 | Iter 568 | Loss: 0.9794\n",
      "Epoch 143/500 | Iter 572 | Loss: 0.9784\n",
      "Epoch 144/500 | Iter 576 | Loss: 0.9773\n",
      "Epoch 145/500 | Iter 580 | Loss: 0.9763\n",
      "Epoch 146/500 | Iter 584 | Loss: 0.9753\n",
      "Epoch 147/500 | Iter 588 | Loss: 0.9743\n",
      "Epoch 148/500 | Iter 592 | Loss: 0.9733\n",
      "Epoch 149/500 | Iter 596 | Loss: 0.9723\n",
      "Epoch 150/500 | Iter 600 | Loss: 0.9713\n",
      "Epoch 151/500 | Iter 604 | Loss: 0.9703\n",
      "Epoch 152/500 | Iter 608 | Loss: 0.9693\n",
      "Epoch 153/500 | Iter 612 | Loss: 0.9684\n",
      "Epoch 154/500 | Iter 616 | Loss: 0.9674\n",
      "Epoch 155/500 | Iter 620 | Loss: 0.9665\n",
      "Epoch 156/500 | Iter 624 | Loss: 0.9656\n",
      "Epoch 157/500 | Iter 628 | Loss: 0.9647\n",
      "Epoch 158/500 | Iter 632 | Loss: 0.9638\n",
      "Epoch 159/500 | Iter 636 | Loss: 0.9629\n",
      "Epoch 160/500 | Iter 640 | Loss: 0.9620\n",
      "Epoch 161/500 | Iter 644 | Loss: 0.9611\n",
      "Epoch 162/500 | Iter 648 | Loss: 0.9602\n",
      "Epoch 163/500 | Iter 652 | Loss: 0.9594\n",
      "Epoch 164/500 | Iter 656 | Loss: 0.9585\n",
      "Epoch 165/500 | Iter 660 | Loss: 0.9576\n",
      "Epoch 166/500 | Iter 664 | Loss: 0.9568\n",
      "Epoch 167/500 | Iter 668 | Loss: 0.9560\n",
      "Epoch 168/500 | Iter 672 | Loss: 0.9551\n",
      "Epoch 169/500 | Iter 676 | Loss: 0.9543\n",
      "Epoch 170/500 | Iter 680 | Loss: 0.9535\n",
      "Epoch 171/500 | Iter 684 | Loss: 0.9527\n",
      "Epoch 172/500 | Iter 688 | Loss: 0.9519\n",
      "Epoch 173/500 | Iter 692 | Loss: 0.9511\n",
      "Epoch 174/500 | Iter 696 | Loss: 0.9503\n",
      "Epoch 175/500 | Iter 700 | Loss: 0.9495\n",
      "Epoch 176/500 | Iter 704 | Loss: 0.9488\n",
      "Epoch 177/500 | Iter 708 | Loss: 0.9480\n",
      "Epoch 178/500 | Iter 712 | Loss: 0.9473\n",
      "Epoch 179/500 | Iter 716 | Loss: 0.9465\n",
      "Epoch 180/500 | Iter 720 | Loss: 0.9458\n",
      "Epoch 181/500 | Iter 724 | Loss: 0.9450\n",
      "Epoch 182/500 | Iter 728 | Loss: 0.9443\n",
      "Epoch 183/500 | Iter 732 | Loss: 0.9435\n",
      "Epoch 184/500 | Iter 736 | Loss: 0.9428\n",
      "Epoch 185/500 | Iter 740 | Loss: 0.9421\n",
      "Epoch 186/500 | Iter 744 | Loss: 0.9414\n",
      "Epoch 187/500 | Iter 748 | Loss: 0.9407\n",
      "Epoch 188/500 | Iter 752 | Loss: 0.9400\n",
      "Epoch 189/500 | Iter 756 | Loss: 0.9393\n",
      "Epoch 190/500 | Iter 760 | Loss: 0.9386\n",
      "Epoch 191/500 | Iter 764 | Loss: 0.9379\n",
      "Epoch 192/500 | Iter 768 | Loss: 0.9372\n",
      "Epoch 193/500 | Iter 772 | Loss: 0.9366\n",
      "Epoch 194/500 | Iter 776 | Loss: 0.9359\n",
      "Epoch 195/500 | Iter 780 | Loss: 0.9352\n",
      "Epoch 196/500 | Iter 784 | Loss: 0.9346\n",
      "Epoch 197/500 | Iter 788 | Loss: 0.9339\n",
      "Epoch 198/500 | Iter 792 | Loss: 0.9333\n",
      "Epoch 199/500 | Iter 796 | Loss: 0.9326\n",
      "Epoch 200/500 | Iter 800 | Loss: 0.9320\n",
      "Epoch 201/500 | Iter 804 | Loss: 0.9313\n",
      "Epoch 202/500 | Iter 808 | Loss: 0.9307\n",
      "Epoch 203/500 | Iter 812 | Loss: 0.9301\n",
      "Epoch 204/500 | Iter 816 | Loss: 0.9295\n",
      "Epoch 205/500 | Iter 820 | Loss: 0.9288\n",
      "Epoch 206/500 | Iter 824 | Loss: 0.9282\n",
      "Epoch 207/500 | Iter 828 | Loss: 0.9276\n",
      "Epoch 208/500 | Iter 832 | Loss: 0.9270\n",
      "Epoch 209/500 | Iter 836 | Loss: 0.9264\n",
      "Epoch 210/500 | Iter 840 | Loss: 0.9258\n",
      "Epoch 211/500 | Iter 844 | Loss: 0.9252\n",
      "Epoch 212/500 | Iter 848 | Loss: 0.9246\n",
      "Epoch 213/500 | Iter 852 | Loss: 0.9240\n",
      "Epoch 214/500 | Iter 856 | Loss: 0.9235\n",
      "Epoch 215/500 | Iter 860 | Loss: 0.9229\n",
      "Epoch 216/500 | Iter 864 | Loss: 0.9223\n",
      "Epoch 217/500 | Iter 868 | Loss: 0.9217\n",
      "Epoch 218/500 | Iter 872 | Loss: 0.9212\n",
      "Epoch 219/500 | Iter 876 | Loss: 0.9206\n",
      "Epoch 220/500 | Iter 880 | Loss: 0.9200\n",
      "Epoch 221/500 | Iter 884 | Loss: 0.9195\n",
      "Epoch 222/500 | Iter 888 | Loss: 0.9189\n",
      "Epoch 223/500 | Iter 892 | Loss: 0.9184\n",
      "Epoch 224/500 | Iter 896 | Loss: 0.9178\n",
      "Epoch 225/500 | Iter 900 | Loss: 0.9173\n",
      "Epoch 226/500 | Iter 904 | Loss: 0.9168\n",
      "Epoch 227/500 | Iter 908 | Loss: 0.9162\n",
      "Epoch 228/500 | Iter 912 | Loss: 0.9157\n",
      "Epoch 229/500 | Iter 916 | Loss: 0.9152\n",
      "Epoch 230/500 | Iter 920 | Loss: 0.9146\n",
      "Epoch 231/500 | Iter 924 | Loss: 0.9141\n",
      "Epoch 232/500 | Iter 928 | Loss: 0.9136\n",
      "Epoch 233/500 | Iter 932 | Loss: 0.9131\n",
      "Epoch 234/500 | Iter 936 | Loss: 0.9126\n",
      "Epoch 235/500 | Iter 940 | Loss: 0.9120\n",
      "Epoch 236/500 | Iter 944 | Loss: 0.9115\n",
      "Epoch 237/500 | Iter 948 | Loss: 0.9110\n",
      "Epoch 238/500 | Iter 952 | Loss: 0.9105\n",
      "Epoch 239/500 | Iter 956 | Loss: 0.9100\n",
      "Epoch 240/500 | Iter 960 | Loss: 0.9095\n",
      "Epoch 241/500 | Iter 964 | Loss: 0.9090\n",
      "Epoch 242/500 | Iter 968 | Loss: 0.9086\n",
      "Epoch 243/500 | Iter 972 | Loss: 0.9081\n",
      "Epoch 244/500 | Iter 976 | Loss: 0.9076\n",
      "Epoch 245/500 | Iter 980 | Loss: 0.9071\n",
      "Epoch 246/500 | Iter 984 | Loss: 0.9066\n",
      "Epoch 247/500 | Iter 988 | Loss: 0.9061\n",
      "Epoch 248/500 | Iter 992 | Loss: 0.9057\n",
      "Epoch 249/500 | Iter 996 | Loss: 0.9052\n",
      "Epoch 250/500 | Iter 1000 | Loss: 0.9047\n",
      "Epoch 251/500 | Iter 1004 | Loss: 0.9043\n",
      "Epoch 252/500 | Iter 1008 | Loss: 0.9038\n",
      "Epoch 253/500 | Iter 1012 | Loss: 0.9033\n",
      "Epoch 254/500 | Iter 1016 | Loss: 0.9029\n",
      "Epoch 255/500 | Iter 1020 | Loss: 0.9024\n",
      "Epoch 256/500 | Iter 1024 | Loss: 0.9020\n",
      "Epoch 257/500 | Iter 1028 | Loss: 0.9015\n",
      "Epoch 258/500 | Iter 1032 | Loss: 0.9011\n",
      "Epoch 259/500 | Iter 1036 | Loss: 0.9006\n",
      "Epoch 260/500 | Iter 1040 | Loss: 0.9002\n",
      "Epoch 261/500 | Iter 1044 | Loss: 0.8997\n",
      "Epoch 262/500 | Iter 1048 | Loss: 0.8993\n",
      "Epoch 263/500 | Iter 1052 | Loss: 0.8989\n",
      "Epoch 264/500 | Iter 1056 | Loss: 0.8984\n",
      "Epoch 265/500 | Iter 1060 | Loss: 0.8980\n",
      "Epoch 266/500 | Iter 1064 | Loss: 0.8975\n",
      "Epoch 267/500 | Iter 1068 | Loss: 0.8971\n",
      "Epoch 268/500 | Iter 1072 | Loss: 0.8967\n",
      "Epoch 269/500 | Iter 1076 | Loss: 0.8963\n",
      "Epoch 270/500 | Iter 1080 | Loss: 0.8958\n",
      "Epoch 271/500 | Iter 1084 | Loss: 0.8954\n",
      "Epoch 272/500 | Iter 1088 | Loss: 0.8950\n",
      "Epoch 273/500 | Iter 1092 | Loss: 0.8946\n",
      "Epoch 274/500 | Iter 1096 | Loss: 0.8942\n",
      "Epoch 275/500 | Iter 1100 | Loss: 0.8938\n",
      "Epoch 276/500 | Iter 1104 | Loss: 0.8934\n",
      "Epoch 277/500 | Iter 1108 | Loss: 0.8929\n",
      "Epoch 278/500 | Iter 1112 | Loss: 0.8925\n",
      "Epoch 279/500 | Iter 1116 | Loss: 0.8921\n",
      "Epoch 280/500 | Iter 1120 | Loss: 0.8917\n",
      "Epoch 281/500 | Iter 1124 | Loss: 0.8913\n",
      "Epoch 282/500 | Iter 1128 | Loss: 0.8909\n",
      "Epoch 283/500 | Iter 1132 | Loss: 0.8905\n",
      "Epoch 284/500 | Iter 1136 | Loss: 0.8901\n",
      "Epoch 285/500 | Iter 1140 | Loss: 0.8897\n",
      "Epoch 286/500 | Iter 1144 | Loss: 0.8894\n",
      "Epoch 287/500 | Iter 1148 | Loss: 0.8890\n",
      "Epoch 288/500 | Iter 1152 | Loss: 0.8886\n",
      "Epoch 289/500 | Iter 1156 | Loss: 0.8882\n",
      "Epoch 290/500 | Iter 1160 | Loss: 0.8878\n",
      "Epoch 291/500 | Iter 1164 | Loss: 0.8874\n",
      "Epoch 292/500 | Iter 1168 | Loss: 0.8870\n",
      "Epoch 293/500 | Iter 1172 | Loss: 0.8867\n",
      "Epoch 294/500 | Iter 1176 | Loss: 0.8863\n",
      "Epoch 295/500 | Iter 1180 | Loss: 0.8859\n",
      "Epoch 296/500 | Iter 1184 | Loss: 0.8855\n",
      "Epoch 297/500 | Iter 1188 | Loss: 0.8852\n",
      "Epoch 298/500 | Iter 1192 | Loss: 0.8848\n",
      "Epoch 299/500 | Iter 1196 | Loss: 0.8844\n",
      "Epoch 300/500 | Iter 1200 | Loss: 0.8841\n",
      "Epoch 301/500 | Iter 1204 | Loss: 0.8837\n",
      "Epoch 302/500 | Iter 1208 | Loss: 0.8833\n",
      "Epoch 303/500 | Iter 1212 | Loss: 0.8830\n",
      "Epoch 304/500 | Iter 1216 | Loss: 0.8826\n",
      "Epoch 305/500 | Iter 1220 | Loss: 0.8823\n",
      "Epoch 306/500 | Iter 1224 | Loss: 0.8819\n",
      "Epoch 307/500 | Iter 1228 | Loss: 0.8815\n",
      "Epoch 308/500 | Iter 1232 | Loss: 0.8812\n",
      "Epoch 309/500 | Iter 1236 | Loss: 0.8808\n",
      "Epoch 310/500 | Iter 1240 | Loss: 0.8805\n",
      "Epoch 311/500 | Iter 1244 | Loss: 0.8801\n",
      "Epoch 312/500 | Iter 1248 | Loss: 0.8798\n",
      "Epoch 313/500 | Iter 1252 | Loss: 0.8794\n",
      "Epoch 314/500 | Iter 1256 | Loss: 0.8791\n",
      "Epoch 315/500 | Iter 1260 | Loss: 0.8788\n",
      "Epoch 316/500 | Iter 1264 | Loss: 0.8784\n",
      "Epoch 317/500 | Iter 1268 | Loss: 0.8781\n",
      "Epoch 318/500 | Iter 1272 | Loss: 0.8777\n",
      "Epoch 319/500 | Iter 1276 | Loss: 0.8774\n",
      "Epoch 320/500 | Iter 1280 | Loss: 0.8771\n",
      "Epoch 321/500 | Iter 1284 | Loss: 0.8767\n",
      "Epoch 322/500 | Iter 1288 | Loss: 0.8764\n",
      "Epoch 323/500 | Iter 1292 | Loss: 0.8761\n",
      "Epoch 324/500 | Iter 1296 | Loss: 0.8757\n",
      "Epoch 325/500 | Iter 1300 | Loss: 0.8754\n",
      "Epoch 326/500 | Iter 1304 | Loss: 0.8751\n",
      "Epoch 327/500 | Iter 1308 | Loss: 0.8747\n",
      "Epoch 328/500 | Iter 1312 | Loss: 0.8744\n",
      "Epoch 329/500 | Iter 1316 | Loss: 0.8741\n",
      "Epoch 330/500 | Iter 1320 | Loss: 0.8738\n",
      "Epoch 331/500 | Iter 1324 | Loss: 0.8734\n",
      "Epoch 332/500 | Iter 1328 | Loss: 0.8731\n",
      "Epoch 333/500 | Iter 1332 | Loss: 0.8728\n",
      "Epoch 334/500 | Iter 1336 | Loss: 0.8725\n",
      "Epoch 335/500 | Iter 1340 | Loss: 0.8722\n",
      "Epoch 336/500 | Iter 1344 | Loss: 0.8718\n",
      "Epoch 337/500 | Iter 1348 | Loss: 0.8715\n",
      "Epoch 338/500 | Iter 1352 | Loss: 0.8712\n",
      "Epoch 339/500 | Iter 1356 | Loss: 0.8709\n",
      "Epoch 340/500 | Iter 1360 | Loss: 0.8706\n",
      "Epoch 341/500 | Iter 1364 | Loss: 0.8703\n",
      "Epoch 342/500 | Iter 1368 | Loss: 0.8700\n",
      "Epoch 343/500 | Iter 1372 | Loss: 0.8697\n",
      "Epoch 344/500 | Iter 1376 | Loss: 0.8694\n",
      "Epoch 345/500 | Iter 1380 | Loss: 0.8691\n",
      "Epoch 346/500 | Iter 1384 | Loss: 0.8688\n",
      "Epoch 347/500 | Iter 1388 | Loss: 0.8685\n",
      "Epoch 348/500 | Iter 1392 | Loss: 0.8682\n",
      "Epoch 349/500 | Iter 1396 | Loss: 0.8679\n",
      "Epoch 350/500 | Iter 1400 | Loss: 0.8676\n",
      "Epoch 351/500 | Iter 1404 | Loss: 0.8673\n",
      "Epoch 352/500 | Iter 1408 | Loss: 0.8670\n",
      "Epoch 353/500 | Iter 1412 | Loss: 0.8667\n",
      "Epoch 354/500 | Iter 1416 | Loss: 0.8664\n",
      "Epoch 355/500 | Iter 1420 | Loss: 0.8661\n",
      "Epoch 356/500 | Iter 1424 | Loss: 0.8658\n",
      "Epoch 357/500 | Iter 1428 | Loss: 0.8655\n",
      "Epoch 358/500 | Iter 1432 | Loss: 0.8652\n",
      "Epoch 359/500 | Iter 1436 | Loss: 0.8649\n",
      "Epoch 360/500 | Iter 1440 | Loss: 0.8646\n",
      "Epoch 361/500 | Iter 1444 | Loss: 0.8643\n",
      "Epoch 362/500 | Iter 1448 | Loss: 0.8641\n",
      "Epoch 363/500 | Iter 1452 | Loss: 0.8638\n",
      "Epoch 364/500 | Iter 1456 | Loss: 0.8635\n",
      "Epoch 365/500 | Iter 1460 | Loss: 0.8632\n",
      "Epoch 366/500 | Iter 1464 | Loss: 0.8629\n",
      "Epoch 367/500 | Iter 1468 | Loss: 0.8626\n",
      "Epoch 368/500 | Iter 1472 | Loss: 0.8624\n",
      "Epoch 369/500 | Iter 1476 | Loss: 0.8621\n",
      "Epoch 370/500 | Iter 1480 | Loss: 0.8618\n",
      "Epoch 371/500 | Iter 1484 | Loss: 0.8615\n",
      "Epoch 372/500 | Iter 1488 | Loss: 0.8613\n",
      "Epoch 373/500 | Iter 1492 | Loss: 0.8610\n",
      "Epoch 374/500 | Iter 1496 | Loss: 0.8607\n",
      "Epoch 375/500 | Iter 1500 | Loss: 0.8604\n",
      "Epoch 376/500 | Iter 1504 | Loss: 0.8602\n",
      "Epoch 377/500 | Iter 1508 | Loss: 0.8599\n",
      "Epoch 378/500 | Iter 1512 | Loss: 0.8596\n",
      "Epoch 379/500 | Iter 1516 | Loss: 0.8594\n",
      "Epoch 380/500 | Iter 1520 | Loss: 0.8591\n",
      "Epoch 381/500 | Iter 1524 | Loss: 0.8588\n",
      "Epoch 382/500 | Iter 1528 | Loss: 0.8586\n",
      "Epoch 383/500 | Iter 1532 | Loss: 0.8583\n",
      "Epoch 384/500 | Iter 1536 | Loss: 0.8580\n",
      "Epoch 385/500 | Iter 1540 | Loss: 0.8578\n",
      "Epoch 386/500 | Iter 1544 | Loss: 0.8575\n",
      "Epoch 387/500 | Iter 1548 | Loss: 0.8572\n",
      "Epoch 388/500 | Iter 1552 | Loss: 0.8570\n",
      "Epoch 389/500 | Iter 1556 | Loss: 0.8567\n",
      "Epoch 390/500 | Iter 1560 | Loss: 0.8565\n",
      "Epoch 391/500 | Iter 1564 | Loss: 0.8562\n",
      "Epoch 392/500 | Iter 1568 | Loss: 0.8559\n",
      "Epoch 393/500 | Iter 1572 | Loss: 0.8557\n",
      "Epoch 394/500 | Iter 1576 | Loss: 0.8554\n",
      "Epoch 395/500 | Iter 1580 | Loss: 0.8552\n",
      "Epoch 396/500 | Iter 1584 | Loss: 0.8549\n",
      "Epoch 397/500 | Iter 1588 | Loss: 0.8547\n",
      "Epoch 398/500 | Iter 1592 | Loss: 0.8544\n",
      "Epoch 399/500 | Iter 1596 | Loss: 0.8542\n",
      "Epoch 400/500 | Iter 1600 | Loss: 0.8539\n",
      "Epoch 401/500 | Iter 1604 | Loss: 0.8537\n",
      "Epoch 402/500 | Iter 1608 | Loss: 0.8534\n",
      "Epoch 403/500 | Iter 1612 | Loss: 0.8532\n",
      "Epoch 404/500 | Iter 1616 | Loss: 0.8529\n",
      "Epoch 405/500 | Iter 1620 | Loss: 0.8527\n",
      "Epoch 406/500 | Iter 1624 | Loss: 0.8524\n",
      "Epoch 407/500 | Iter 1628 | Loss: 0.8522\n",
      "Epoch 408/500 | Iter 1632 | Loss: 0.8520\n",
      "Epoch 409/500 | Iter 1636 | Loss: 0.8517\n",
      "Epoch 410/500 | Iter 1640 | Loss: 0.8515\n",
      "Epoch 411/500 | Iter 1644 | Loss: 0.8512\n",
      "Epoch 412/500 | Iter 1648 | Loss: 0.8510\n",
      "Epoch 413/500 | Iter 1652 | Loss: 0.8507\n",
      "Epoch 414/500 | Iter 1656 | Loss: 0.8505\n",
      "Epoch 415/500 | Iter 1660 | Loss: 0.8503\n",
      "Epoch 416/500 | Iter 1664 | Loss: 0.8500\n",
      "Epoch 417/500 | Iter 1668 | Loss: 0.8498\n",
      "Epoch 418/500 | Iter 1672 | Loss: 0.8496\n",
      "Epoch 419/500 | Iter 1676 | Loss: 0.8493\n",
      "Epoch 420/500 | Iter 1680 | Loss: 0.8491\n",
      "Epoch 421/500 | Iter 1684 | Loss: 0.8489\n",
      "Epoch 422/500 | Iter 1688 | Loss: 0.8486\n",
      "Epoch 423/500 | Iter 1692 | Loss: 0.8484\n",
      "Epoch 424/500 | Iter 1696 | Loss: 0.8482\n",
      "Epoch 425/500 | Iter 1700 | Loss: 0.8479\n",
      "Epoch 426/500 | Iter 1704 | Loss: 0.8477\n",
      "Epoch 427/500 | Iter 1708 | Loss: 0.8475\n",
      "Epoch 428/500 | Iter 1712 | Loss: 0.8472\n",
      "Epoch 429/500 | Iter 1716 | Loss: 0.8470\n",
      "Epoch 430/500 | Iter 1720 | Loss: 0.8468\n",
      "Epoch 431/500 | Iter 1724 | Loss: 0.8466\n",
      "Epoch 432/500 | Iter 1728 | Loss: 0.8463\n",
      "Epoch 433/500 | Iter 1732 | Loss: 0.8461\n",
      "Epoch 434/500 | Iter 1736 | Loss: 0.8459\n",
      "Epoch 435/500 | Iter 1740 | Loss: 0.8457\n",
      "Epoch 436/500 | Iter 1744 | Loss: 0.8454\n",
      "Epoch 437/500 | Iter 1748 | Loss: 0.8452\n",
      "Epoch 438/500 | Iter 1752 | Loss: 0.8450\n",
      "Epoch 439/500 | Iter 1756 | Loss: 0.8448\n",
      "Epoch 440/500 | Iter 1760 | Loss: 0.8446\n",
      "Epoch 441/500 | Iter 1764 | Loss: 0.8443\n",
      "Epoch 442/500 | Iter 1768 | Loss: 0.8441\n",
      "Epoch 443/500 | Iter 1772 | Loss: 0.8439\n",
      "Epoch 444/500 | Iter 1776 | Loss: 0.8437\n",
      "Epoch 445/500 | Iter 1780 | Loss: 0.8435\n",
      "Epoch 446/500 | Iter 1784 | Loss: 0.8432\n",
      "Epoch 447/500 | Iter 1788 | Loss: 0.8430\n",
      "Epoch 448/500 | Iter 1792 | Loss: 0.8428\n",
      "Epoch 449/500 | Iter 1796 | Loss: 0.8426\n",
      "Epoch 450/500 | Iter 1800 | Loss: 0.8424\n",
      "Epoch 451/500 | Iter 1804 | Loss: 0.8422\n",
      "Epoch 452/500 | Iter 1808 | Loss: 0.8420\n",
      "Epoch 453/500 | Iter 1812 | Loss: 0.8418\n",
      "Epoch 454/500 | Iter 1816 | Loss: 0.8415\n",
      "Epoch 455/500 | Iter 1820 | Loss: 0.8413\n",
      "Epoch 456/500 | Iter 1824 | Loss: 0.8411\n",
      "Epoch 457/500 | Iter 1828 | Loss: 0.8409\n",
      "Epoch 458/500 | Iter 1832 | Loss: 0.8407\n",
      "Epoch 459/500 | Iter 1836 | Loss: 0.8405\n",
      "Epoch 460/500 | Iter 1840 | Loss: 0.8403\n",
      "Epoch 461/500 | Iter 1844 | Loss: 0.8401\n",
      "Epoch 462/500 | Iter 1848 | Loss: 0.8399\n",
      "Epoch 463/500 | Iter 1852 | Loss: 0.8397\n",
      "Epoch 464/500 | Iter 1856 | Loss: 0.8395\n",
      "Epoch 465/500 | Iter 1860 | Loss: 0.8393\n",
      "Epoch 466/500 | Iter 1864 | Loss: 0.8391\n",
      "Epoch 467/500 | Iter 1868 | Loss: 0.8389\n",
      "Epoch 468/500 | Iter 1872 | Loss: 0.8387\n",
      "Epoch 469/500 | Iter 1876 | Loss: 0.8385\n",
      "Epoch 470/500 | Iter 1880 | Loss: 0.8382\n",
      "Epoch 471/500 | Iter 1884 | Loss: 0.8380\n",
      "Epoch 472/500 | Iter 1888 | Loss: 0.8378\n",
      "Epoch 473/500 | Iter 1892 | Loss: 0.8376\n",
      "Epoch 474/500 | Iter 1896 | Loss: 0.8374\n",
      "Epoch 475/500 | Iter 1900 | Loss: 0.8373\n",
      "Epoch 476/500 | Iter 1904 | Loss: 0.8371\n",
      "Epoch 477/500 | Iter 1908 | Loss: 0.8369\n",
      "Epoch 478/500 | Iter 1912 | Loss: 0.8367\n",
      "Epoch 479/500 | Iter 1916 | Loss: 0.8365\n",
      "Epoch 480/500 | Iter 1920 | Loss: 0.8363\n",
      "Epoch 481/500 | Iter 1924 | Loss: 0.8361\n",
      "Epoch 482/500 | Iter 1928 | Loss: 0.8359\n",
      "Epoch 483/500 | Iter 1932 | Loss: 0.8357\n",
      "Epoch 484/500 | Iter 1936 | Loss: 0.8355\n",
      "Epoch 485/500 | Iter 1940 | Loss: 0.8353\n",
      "Epoch 486/500 | Iter 1944 | Loss: 0.8351\n",
      "Epoch 487/500 | Iter 1948 | Loss: 0.8349\n",
      "Epoch 488/500 | Iter 1952 | Loss: 0.8347\n",
      "Epoch 489/500 | Iter 1956 | Loss: 0.8345\n",
      "Epoch 490/500 | Iter 1960 | Loss: 0.8343\n",
      "Epoch 491/500 | Iter 1964 | Loss: 0.8342\n",
      "Epoch 492/500 | Iter 1968 | Loss: 0.8340\n",
      "Epoch 493/500 | Iter 1972 | Loss: 0.8338\n",
      "Epoch 494/500 | Iter 1976 | Loss: 0.8336\n",
      "Epoch 495/500 | Iter 1980 | Loss: 0.8334\n",
      "Epoch 496/500 | Iter 1984 | Loss: 0.8332\n",
      "Epoch 497/500 | Iter 1988 | Loss: 0.8330\n",
      "Epoch 498/500 | Iter 1992 | Loss: 0.8328\n",
      "Epoch 499/500 | Iter 1996 | Loss: 0.8327\n",
      "Time taken: 0.60s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\tPrecision\tRecall\tF1-Score\tSupport\n",
      "------------------------------------------------------------\n",
      "0\t0.77\t\t0.82\t0.80\t\t639\n",
      "1\t0.57\t\t0.51\t0.54\t\t614\n",
      "2\t0.55\t\t0.53\t0.54\t\t564\n",
      "3\t0.71\t\t0.76\t0.73\t\t592\n",
      "------------------------------------------------------------\n",
      "Macro Avg\t0.65\t\t0.66\t0.65\n",
      "Weighted Avg\t0.65\t\t0.66\t0.65\n",
      "Accuracy\t0.66\n"
     ]
    }
   ],
   "source": [
    "#model = LogisticRegression(k=k, n=n, method=\"batch\", alpha=0.01, max_iter=2000)\n",
    "model = LogisticRegression(k=k, n=n, method=\"batch\", alpha=0.01, max_iter=2000, use_penalty=False, lambda_=0.1)\n",
    "model.fit(X_train, Y_train_encoded)\n",
    "model.plot()\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "model.classification_report(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c717990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500 | Iter 0 | Loss: 1.4658\n",
      "Epoch 1/500 | Iter 4 | Loss: 1.4247\n",
      "Epoch 2/500 | Iter 8 | Loss: 1.4383\n",
      "Epoch 3/500 | Iter 12 | Loss: 1.4071\n",
      "Epoch 4/500 | Iter 16 | Loss: 1.4126\n",
      "Epoch 5/500 | Iter 20 | Loss: 1.3637\n",
      "Epoch 6/500 | Iter 24 | Loss: 1.3891\n",
      "Epoch 7/500 | Iter 28 | Loss: 1.3542\n",
      "Epoch 8/500 | Iter 32 | Loss: 1.3539\n",
      "Epoch 9/500 | Iter 36 | Loss: 1.3448\n",
      "Epoch 10/500 | Iter 40 | Loss: 1.3323\n",
      "Epoch 11/500 | Iter 44 | Loss: 1.3160\n",
      "Epoch 12/500 | Iter 48 | Loss: 1.3075\n",
      "Epoch 13/500 | Iter 52 | Loss: 1.3174\n",
      "Epoch 14/500 | Iter 56 | Loss: 1.2979\n",
      "Epoch 15/500 | Iter 60 | Loss: 1.2802\n",
      "Epoch 16/500 | Iter 64 | Loss: 1.2753\n",
      "Epoch 17/500 | Iter 68 | Loss: 1.3025\n",
      "Epoch 18/500 | Iter 72 | Loss: 1.3064\n",
      "Epoch 19/500 | Iter 76 | Loss: 1.2667\n",
      "Epoch 20/500 | Iter 80 | Loss: 1.2697\n",
      "Epoch 21/500 | Iter 84 | Loss: 1.2543\n",
      "Epoch 22/500 | Iter 88 | Loss: 1.2548\n",
      "Epoch 23/500 | Iter 92 | Loss: 1.2438\n",
      "Epoch 24/500 | Iter 96 | Loss: 1.2333\n",
      "Epoch 25/500 | Iter 100 | Loss: 1.2193\n",
      "Epoch 26/500 | Iter 104 | Loss: 1.2488\n",
      "Epoch 27/500 | Iter 108 | Loss: 1.2017\n",
      "Epoch 28/500 | Iter 112 | Loss: 1.2327\n",
      "Epoch 29/500 | Iter 116 | Loss: 1.2059\n",
      "Epoch 30/500 | Iter 120 | Loss: 1.2153\n",
      "Epoch 31/500 | Iter 124 | Loss: 1.1848\n",
      "Epoch 32/500 | Iter 128 | Loss: 1.1919\n",
      "Epoch 33/500 | Iter 132 | Loss: 1.1841\n",
      "Epoch 34/500 | Iter 136 | Loss: 1.1688\n",
      "Epoch 35/500 | Iter 140 | Loss: 1.1589\n",
      "Epoch 36/500 | Iter 144 | Loss: 1.1604\n",
      "Epoch 37/500 | Iter 148 | Loss: 1.1521\n",
      "Epoch 38/500 | Iter 152 | Loss: 1.1739\n",
      "Epoch 39/500 | Iter 156 | Loss: 1.1476\n",
      "Epoch 40/500 | Iter 160 | Loss: 1.1737\n",
      "Epoch 41/500 | Iter 164 | Loss: 1.1669\n",
      "Epoch 42/500 | Iter 168 | Loss: 1.1839\n",
      "Epoch 43/500 | Iter 172 | Loss: 1.1337\n",
      "Epoch 44/500 | Iter 176 | Loss: 1.1267\n",
      "Epoch 45/500 | Iter 180 | Loss: 1.1531\n",
      "Epoch 46/500 | Iter 184 | Loss: 1.1275\n",
      "Epoch 47/500 | Iter 188 | Loss: 1.1540\n",
      "Epoch 48/500 | Iter 192 | Loss: 1.1452\n",
      "Epoch 49/500 | Iter 196 | Loss: 1.1304\n",
      "Epoch 50/500 | Iter 200 | Loss: 1.1215\n",
      "Epoch 51/500 | Iter 204 | Loss: 1.1305\n",
      "Epoch 52/500 | Iter 208 | Loss: 1.1170\n",
      "Epoch 53/500 | Iter 212 | Loss: 1.0958\n",
      "Epoch 54/500 | Iter 216 | Loss: 1.1149\n",
      "Epoch 55/500 | Iter 220 | Loss: 1.1552\n",
      "Epoch 56/500 | Iter 224 | Loss: 1.0859\n",
      "Epoch 57/500 | Iter 228 | Loss: 1.0906\n",
      "Epoch 58/500 | Iter 232 | Loss: 1.1097\n",
      "Epoch 59/500 | Iter 236 | Loss: 1.0859\n",
      "Epoch 60/500 | Iter 240 | Loss: 1.0774\n",
      "Epoch 61/500 | Iter 244 | Loss: 1.0785\n",
      "Epoch 62/500 | Iter 248 | Loss: 1.1212\n",
      "Epoch 63/500 | Iter 252 | Loss: 1.0667\n",
      "Epoch 64/500 | Iter 256 | Loss: 1.0839\n",
      "Epoch 65/500 | Iter 260 | Loss: 1.0676\n",
      "Epoch 66/500 | Iter 264 | Loss: 1.0646\n",
      "Epoch 67/500 | Iter 268 | Loss: 1.0844\n",
      "Epoch 68/500 | Iter 272 | Loss: 1.0583\n",
      "Epoch 69/500 | Iter 276 | Loss: 1.0545\n",
      "Epoch 70/500 | Iter 280 | Loss: 1.0533\n",
      "Epoch 71/500 | Iter 284 | Loss: 1.0636\n",
      "Epoch 72/500 | Iter 288 | Loss: 1.0731\n",
      "Epoch 73/500 | Iter 292 | Loss: 1.0421\n",
      "Epoch 74/500 | Iter 296 | Loss: 1.0426\n",
      "Epoch 75/500 | Iter 300 | Loss: 1.0506\n",
      "Epoch 76/500 | Iter 304 | Loss: 1.0432\n",
      "Epoch 77/500 | Iter 308 | Loss: 1.0775\n",
      "Epoch 78/500 | Iter 312 | Loss: 1.0309\n",
      "Epoch 79/500 | Iter 316 | Loss: 1.0416\n",
      "Epoch 80/500 | Iter 320 | Loss: 1.0290\n",
      "Epoch 81/500 | Iter 324 | Loss: 1.0299\n",
      "Epoch 82/500 | Iter 328 | Loss: 1.0448\n",
      "Epoch 83/500 | Iter 332 | Loss: 1.0470\n",
      "Epoch 84/500 | Iter 336 | Loss: 1.0237\n",
      "Epoch 85/500 | Iter 340 | Loss: 1.0155\n",
      "Epoch 86/500 | Iter 344 | Loss: 1.0170\n",
      "Epoch 87/500 | Iter 348 | Loss: 1.0169\n",
      "Epoch 88/500 | Iter 352 | Loss: 1.0206\n",
      "Epoch 89/500 | Iter 356 | Loss: 1.0200\n",
      "Epoch 90/500 | Iter 360 | Loss: 1.0347\n",
      "Epoch 91/500 | Iter 364 | Loss: 1.0241\n",
      "Epoch 92/500 | Iter 368 | Loss: 1.0117\n",
      "Epoch 93/500 | Iter 372 | Loss: 1.0067\n",
      "Epoch 94/500 | Iter 376 | Loss: 1.0084\n",
      "Epoch 95/500 | Iter 380 | Loss: 1.0088\n",
      "Epoch 96/500 | Iter 384 | Loss: 1.0071\n",
      "Epoch 97/500 | Iter 388 | Loss: 1.0046\n",
      "Epoch 98/500 | Iter 392 | Loss: 1.0140\n",
      "Epoch 99/500 | Iter 396 | Loss: 1.0276\n",
      "Epoch 100/500 | Iter 400 | Loss: 1.0027\n",
      "Epoch 101/500 | Iter 404 | Loss: 1.0002\n",
      "Epoch 102/500 | Iter 408 | Loss: 1.0000\n",
      "Epoch 103/500 | Iter 412 | Loss: 1.0249\n",
      "Epoch 104/500 | Iter 416 | Loss: 1.0139\n",
      "Epoch 105/500 | Iter 420 | Loss: 0.9916\n",
      "Epoch 106/500 | Iter 424 | Loss: 1.0276\n",
      "Epoch 107/500 | Iter 428 | Loss: 1.0031\n",
      "Epoch 108/500 | Iter 432 | Loss: 0.9956\n",
      "Epoch 109/500 | Iter 436 | Loss: 1.0091\n",
      "Epoch 110/500 | Iter 440 | Loss: 1.0070\n",
      "Epoch 111/500 | Iter 444 | Loss: 1.0061\n",
      "Epoch 112/500 | Iter 448 | Loss: 0.9891\n",
      "Epoch 113/500 | Iter 452 | Loss: 1.0029\n",
      "Epoch 114/500 | Iter 456 | Loss: 0.9856\n",
      "Epoch 115/500 | Iter 460 | Loss: 0.9793\n",
      "Epoch 116/500 | Iter 464 | Loss: 0.9912\n",
      "Epoch 117/500 | Iter 468 | Loss: 1.0022\n",
      "Epoch 118/500 | Iter 472 | Loss: 0.9764\n",
      "Epoch 119/500 | Iter 476 | Loss: 0.9721\n",
      "Epoch 120/500 | Iter 480 | Loss: 0.9876\n",
      "Epoch 121/500 | Iter 484 | Loss: 0.9899\n",
      "Epoch 122/500 | Iter 488 | Loss: 0.9741\n",
      "Epoch 123/500 | Iter 492 | Loss: 0.9986\n",
      "Epoch 124/500 | Iter 496 | Loss: 0.9838\n",
      "Epoch 125/500 | Iter 500 | Loss: 0.9653\n",
      "Epoch 126/500 | Iter 504 | Loss: 0.9987\n",
      "Epoch 127/500 | Iter 508 | Loss: 0.9794\n",
      "Epoch 128/500 | Iter 512 | Loss: 0.9914\n",
      "Epoch 129/500 | Iter 516 | Loss: 0.9870\n",
      "Epoch 130/500 | Iter 520 | Loss: 0.9896\n",
      "Epoch 131/500 | Iter 524 | Loss: 1.0207\n",
      "Epoch 132/500 | Iter 528 | Loss: 0.9882\n",
      "Epoch 133/500 | Iter 532 | Loss: 0.9726\n",
      "Epoch 134/500 | Iter 536 | Loss: 0.9502\n",
      "Epoch 135/500 | Iter 540 | Loss: 0.9630\n",
      "Epoch 136/500 | Iter 544 | Loss: 1.0058\n",
      "Epoch 137/500 | Iter 548 | Loss: 0.9844\n",
      "Epoch 138/500 | Iter 552 | Loss: 0.9829\n",
      "Epoch 139/500 | Iter 556 | Loss: 0.9740\n",
      "Epoch 140/500 | Iter 560 | Loss: 0.9677\n",
      "Epoch 141/500 | Iter 564 | Loss: 0.9578\n",
      "Epoch 142/500 | Iter 568 | Loss: 0.9535\n",
      "Epoch 143/500 | Iter 572 | Loss: 0.9580\n",
      "Epoch 144/500 | Iter 576 | Loss: 0.9691\n",
      "Epoch 145/500 | Iter 580 | Loss: 0.9695\n",
      "Epoch 146/500 | Iter 584 | Loss: 0.9647\n",
      "Epoch 147/500 | Iter 588 | Loss: 0.9531\n",
      "Epoch 148/500 | Iter 592 | Loss: 0.9682\n",
      "Epoch 149/500 | Iter 596 | Loss: 0.9592\n",
      "Epoch 150/500 | Iter 600 | Loss: 0.9441\n",
      "Epoch 151/500 | Iter 604 | Loss: 0.9505\n",
      "Epoch 152/500 | Iter 608 | Loss: 0.9361\n",
      "Epoch 153/500 | Iter 612 | Loss: 0.9473\n",
      "Epoch 154/500 | Iter 616 | Loss: 0.9454\n",
      "Epoch 155/500 | Iter 620 | Loss: 0.9493\n",
      "Epoch 156/500 | Iter 624 | Loss: 0.9434\n",
      "Epoch 157/500 | Iter 628 | Loss: 0.9556\n",
      "Epoch 158/500 | Iter 632 | Loss: 0.9428\n",
      "Epoch 159/500 | Iter 636 | Loss: 0.9622\n",
      "Epoch 160/500 | Iter 640 | Loss: 0.9537\n",
      "Epoch 161/500 | Iter 644 | Loss: 1.0622\n",
      "Epoch 162/500 | Iter 648 | Loss: 0.9441\n",
      "Epoch 163/500 | Iter 652 | Loss: 0.9482\n",
      "Epoch 164/500 | Iter 656 | Loss: 0.9427\n",
      "Epoch 165/500 | Iter 660 | Loss: 0.9875\n",
      "Epoch 166/500 | Iter 664 | Loss: 0.9941\n",
      "Epoch 167/500 | Iter 668 | Loss: 0.9503\n",
      "Epoch 168/500 | Iter 672 | Loss: 0.9358\n",
      "Epoch 169/500 | Iter 676 | Loss: 0.9294\n",
      "Epoch 170/500 | Iter 680 | Loss: 0.9536\n",
      "Epoch 171/500 | Iter 684 | Loss: 0.9202\n",
      "Epoch 172/500 | Iter 688 | Loss: 0.9605\n",
      "Epoch 173/500 | Iter 692 | Loss: 0.9565\n",
      "Epoch 174/500 | Iter 696 | Loss: 0.9196\n",
      "Epoch 175/500 | Iter 700 | Loss: 0.9547\n",
      "Epoch 176/500 | Iter 704 | Loss: 0.9262\n",
      "Epoch 177/500 | Iter 708 | Loss: 1.0625\n",
      "Epoch 178/500 | Iter 712 | Loss: 0.9312\n",
      "Epoch 179/500 | Iter 716 | Loss: 0.9416\n",
      "Epoch 180/500 | Iter 720 | Loss: 0.9620\n",
      "Epoch 181/500 | Iter 724 | Loss: 0.9361\n",
      "Epoch 182/500 | Iter 728 | Loss: 0.9508\n",
      "Epoch 183/500 | Iter 732 | Loss: 0.9227\n",
      "Epoch 184/500 | Iter 736 | Loss: 0.9608\n",
      "Epoch 185/500 | Iter 740 | Loss: 0.9360\n",
      "Epoch 186/500 | Iter 744 | Loss: 0.9210\n",
      "Epoch 187/500 | Iter 748 | Loss: 0.9190\n",
      "Epoch 188/500 | Iter 752 | Loss: 0.9292\n",
      "Epoch 189/500 | Iter 756 | Loss: 0.9213\n",
      "Epoch 190/500 | Iter 760 | Loss: 0.9241\n",
      "Epoch 191/500 | Iter 764 | Loss: 0.9249\n",
      "Epoch 192/500 | Iter 768 | Loss: 0.9085\n",
      "Epoch 193/500 | Iter 772 | Loss: 0.9439\n",
      "Epoch 194/500 | Iter 776 | Loss: 0.9135\n",
      "Epoch 195/500 | Iter 780 | Loss: 0.9380\n",
      "Epoch 196/500 | Iter 784 | Loss: 0.9346\n",
      "Epoch 197/500 | Iter 788 | Loss: 0.9145\n",
      "Epoch 198/500 | Iter 792 | Loss: 0.9182\n",
      "Epoch 199/500 | Iter 796 | Loss: 0.9122\n",
      "Epoch 200/500 | Iter 800 | Loss: 0.9164\n",
      "Epoch 201/500 | Iter 804 | Loss: 0.9143\n",
      "Epoch 202/500 | Iter 808 | Loss: 0.9030\n",
      "Epoch 203/500 | Iter 812 | Loss: 0.9029\n",
      "Epoch 204/500 | Iter 816 | Loss: 0.9154\n",
      "Epoch 205/500 | Iter 820 | Loss: 0.9244\n",
      "Epoch 206/500 | Iter 824 | Loss: 0.9380\n",
      "Epoch 207/500 | Iter 828 | Loss: 0.9022\n",
      "Epoch 208/500 | Iter 832 | Loss: 0.8998\n",
      "Epoch 209/500 | Iter 836 | Loss: 0.9357\n",
      "Epoch 210/500 | Iter 840 | Loss: 0.9123\n",
      "Epoch 211/500 | Iter 844 | Loss: 0.9171\n",
      "Epoch 212/500 | Iter 848 | Loss: 0.9354\n",
      "Epoch 213/500 | Iter 852 | Loss: 0.9599\n",
      "Epoch 214/500 | Iter 856 | Loss: 0.9136\n",
      "Epoch 215/500 | Iter 860 | Loss: 0.9204\n",
      "Epoch 216/500 | Iter 864 | Loss: 0.9275\n",
      "Epoch 217/500 | Iter 868 | Loss: 0.9114\n",
      "Epoch 218/500 | Iter 872 | Loss: 0.9081\n",
      "Epoch 219/500 | Iter 876 | Loss: 0.9000\n",
      "Epoch 220/500 | Iter 880 | Loss: 0.9089\n",
      "Epoch 221/500 | Iter 884 | Loss: 0.9096\n",
      "Epoch 222/500 | Iter 888 | Loss: 0.8958\n",
      "Epoch 223/500 | Iter 892 | Loss: 0.9339\n",
      "Epoch 224/500 | Iter 896 | Loss: 0.9247\n",
      "Epoch 225/500 | Iter 900 | Loss: 0.9178\n",
      "Epoch 226/500 | Iter 904 | Loss: 0.8967\n",
      "Epoch 227/500 | Iter 908 | Loss: 0.9035\n",
      "Epoch 228/500 | Iter 912 | Loss: 0.9092\n",
      "Epoch 229/500 | Iter 916 | Loss: 0.9031\n",
      "Epoch 230/500 | Iter 920 | Loss: 0.8914\n",
      "Epoch 231/500 | Iter 924 | Loss: 0.8989\n",
      "Epoch 232/500 | Iter 928 | Loss: 0.8961\n",
      "Epoch 233/500 | Iter 932 | Loss: 0.9060\n",
      "Epoch 234/500 | Iter 936 | Loss: 0.9064\n",
      "Epoch 235/500 | Iter 940 | Loss: 0.9002\n",
      "Epoch 236/500 | Iter 944 | Loss: 0.9087\n",
      "Epoch 237/500 | Iter 948 | Loss: 0.9130\n",
      "Epoch 238/500 | Iter 952 | Loss: 0.8911\n",
      "Epoch 239/500 | Iter 956 | Loss: 0.9328\n",
      "Epoch 240/500 | Iter 960 | Loss: 0.9045\n",
      "Epoch 241/500 | Iter 964 | Loss: 0.8870\n",
      "Epoch 242/500 | Iter 968 | Loss: 0.8901\n",
      "Epoch 243/500 | Iter 972 | Loss: 0.9212\n",
      "Epoch 244/500 | Iter 976 | Loss: 0.8956\n",
      "Epoch 245/500 | Iter 980 | Loss: 0.9187\n",
      "Epoch 246/500 | Iter 984 | Loss: 0.8987\n",
      "Epoch 247/500 | Iter 988 | Loss: 0.8834\n",
      "Epoch 248/500 | Iter 992 | Loss: 0.9134\n",
      "Epoch 249/500 | Iter 996 | Loss: 0.8973\n",
      "Epoch 250/500 | Iter 1000 | Loss: 0.9004\n",
      "Epoch 251/500 | Iter 1004 | Loss: 0.8983\n",
      "Epoch 252/500 | Iter 1008 | Loss: 0.8911\n",
      "Epoch 253/500 | Iter 1012 | Loss: 0.8951\n",
      "Epoch 254/500 | Iter 1016 | Loss: 0.8931\n",
      "Epoch 255/500 | Iter 1020 | Loss: 0.9139\n",
      "Epoch 256/500 | Iter 1024 | Loss: 0.9023\n",
      "Epoch 257/500 | Iter 1028 | Loss: 0.9187\n",
      "Epoch 258/500 | Iter 1032 | Loss: 0.8904\n",
      "Epoch 259/500 | Iter 1036 | Loss: 0.9005\n",
      "Epoch 260/500 | Iter 1040 | Loss: 0.8903\n",
      "Epoch 261/500 | Iter 1044 | Loss: 0.8933\n",
      "Epoch 262/500 | Iter 1048 | Loss: 0.8782\n",
      "Epoch 263/500 | Iter 1052 | Loss: 0.9012\n",
      "Epoch 264/500 | Iter 1056 | Loss: 0.9088\n",
      "Epoch 265/500 | Iter 1060 | Loss: 0.8950\n",
      "Epoch 266/500 | Iter 1064 | Loss: 0.8807\n",
      "Epoch 267/500 | Iter 1068 | Loss: 0.8844\n",
      "Epoch 268/500 | Iter 1072 | Loss: 0.8814\n",
      "Epoch 269/500 | Iter 1076 | Loss: 0.8958\n",
      "Epoch 270/500 | Iter 1080 | Loss: 0.9821\n",
      "Epoch 271/500 | Iter 1084 | Loss: 0.8914\n",
      "Epoch 272/500 | Iter 1088 | Loss: 0.9047\n",
      "Epoch 273/500 | Iter 1092 | Loss: 0.9059\n",
      "Epoch 274/500 | Iter 1096 | Loss: 0.9061\n",
      "Epoch 275/500 | Iter 1100 | Loss: 0.8731\n",
      "Epoch 276/500 | Iter 1104 | Loss: 0.8749\n",
      "Epoch 277/500 | Iter 1108 | Loss: 0.8899\n",
      "Epoch 278/500 | Iter 1112 | Loss: 0.8797\n",
      "Epoch 279/500 | Iter 1116 | Loss: 0.8827\n",
      "Epoch 280/500 | Iter 1120 | Loss: 0.8921\n",
      "Epoch 281/500 | Iter 1124 | Loss: 0.8963\n",
      "Epoch 282/500 | Iter 1128 | Loss: 0.9014\n",
      "Epoch 283/500 | Iter 1132 | Loss: 0.8729\n",
      "Epoch 284/500 | Iter 1136 | Loss: 0.8810\n",
      "Epoch 285/500 | Iter 1140 | Loss: 0.8834\n",
      "Epoch 286/500 | Iter 1144 | Loss: 0.8682\n",
      "Epoch 287/500 | Iter 1148 | Loss: 0.8719\n",
      "Epoch 288/500 | Iter 1152 | Loss: 0.9029\n",
      "Epoch 289/500 | Iter 1156 | Loss: 0.8944\n",
      "Epoch 290/500 | Iter 1160 | Loss: 0.8704\n",
      "Epoch 291/500 | Iter 1164 | Loss: 0.8751\n",
      "Epoch 292/500 | Iter 1168 | Loss: 0.8986\n",
      "Epoch 293/500 | Iter 1172 | Loss: 0.8840\n",
      "Epoch 294/500 | Iter 1176 | Loss: 0.8629\n",
      "Epoch 295/500 | Iter 1180 | Loss: 0.8782\n",
      "Epoch 296/500 | Iter 1184 | Loss: 0.8994\n",
      "Epoch 297/500 | Iter 1188 | Loss: 0.8679\n",
      "Epoch 298/500 | Iter 1192 | Loss: 0.8868\n",
      "Epoch 299/500 | Iter 1196 | Loss: 0.8669\n",
      "Epoch 300/500 | Iter 1200 | Loss: 0.8758\n",
      "Epoch 301/500 | Iter 1204 | Loss: 0.8818\n",
      "Epoch 302/500 | Iter 1208 | Loss: 0.8970\n",
      "Epoch 303/500 | Iter 1212 | Loss: 0.8935\n",
      "Epoch 304/500 | Iter 1216 | Loss: 0.8760\n",
      "Epoch 305/500 | Iter 1220 | Loss: 0.8963\n",
      "Epoch 306/500 | Iter 1224 | Loss: 0.8588\n",
      "Epoch 307/500 | Iter 1228 | Loss: 0.8629\n",
      "Epoch 308/500 | Iter 1232 | Loss: 0.8604\n",
      "Epoch 309/500 | Iter 1236 | Loss: 0.8606\n",
      "Epoch 310/500 | Iter 1240 | Loss: 0.8644\n",
      "Epoch 311/500 | Iter 1244 | Loss: 0.8700\n",
      "Epoch 312/500 | Iter 1248 | Loss: 0.8609\n",
      "Epoch 313/500 | Iter 1252 | Loss: 0.8726\n",
      "Epoch 314/500 | Iter 1256 | Loss: 0.9138\n",
      "Epoch 315/500 | Iter 1260 | Loss: 0.8987\n",
      "Epoch 316/500 | Iter 1264 | Loss: 0.8593\n",
      "Epoch 317/500 | Iter 1268 | Loss: 0.8610\n",
      "Epoch 318/500 | Iter 1272 | Loss: 0.8554\n",
      "Epoch 319/500 | Iter 1276 | Loss: 0.8617\n",
      "Epoch 320/500 | Iter 1280 | Loss: 0.8746\n",
      "Epoch 321/500 | Iter 1284 | Loss: 0.8671\n",
      "Epoch 322/500 | Iter 1288 | Loss: 0.8821\n",
      "Epoch 323/500 | Iter 1292 | Loss: 0.8610\n",
      "Epoch 324/500 | Iter 1296 | Loss: 0.9132\n",
      "Epoch 325/500 | Iter 1300 | Loss: 0.8731\n",
      "Epoch 326/500 | Iter 1304 | Loss: 0.8563\n",
      "Epoch 327/500 | Iter 1308 | Loss: 0.8780\n",
      "Epoch 328/500 | Iter 1312 | Loss: 0.8934\n",
      "Epoch 329/500 | Iter 1316 | Loss: 0.8795\n",
      "Epoch 330/500 | Iter 1320 | Loss: 0.8751\n",
      "Epoch 331/500 | Iter 1324 | Loss: 0.8804\n",
      "Epoch 332/500 | Iter 1328 | Loss: 0.8554\n",
      "Epoch 333/500 | Iter 1332 | Loss: 0.8846\n",
      "Epoch 334/500 | Iter 1336 | Loss: 0.8669\n",
      "Epoch 335/500 | Iter 1340 | Loss: 0.8688\n",
      "Epoch 336/500 | Iter 1344 | Loss: 0.8493\n",
      "Epoch 337/500 | Iter 1348 | Loss: 0.8617\n",
      "Epoch 338/500 | Iter 1352 | Loss: 0.8553\n",
      "Epoch 339/500 | Iter 1356 | Loss: 0.8611\n",
      "Epoch 340/500 | Iter 1360 | Loss: 0.8771\n",
      "Epoch 341/500 | Iter 1364 | Loss: 0.8934\n",
      "Epoch 342/500 | Iter 1368 | Loss: 0.8679\n",
      "Epoch 343/500 | Iter 1372 | Loss: 0.8533\n",
      "Epoch 344/500 | Iter 1376 | Loss: 0.8583\n",
      "Epoch 345/500 | Iter 1380 | Loss: 0.8738\n",
      "Epoch 346/500 | Iter 1384 | Loss: 0.8536\n",
      "Epoch 347/500 | Iter 1388 | Loss: 0.8538\n",
      "Epoch 348/500 | Iter 1392 | Loss: 0.8807\n",
      "Epoch 349/500 | Iter 1396 | Loss: 0.8639\n",
      "Epoch 350/500 | Iter 1400 | Loss: 0.8495\n",
      "Epoch 351/500 | Iter 1404 | Loss: 0.8469\n",
      "Epoch 352/500 | Iter 1408 | Loss: 0.8604\n",
      "Epoch 353/500 | Iter 1412 | Loss: 0.8526\n",
      "Epoch 354/500 | Iter 1416 | Loss: 0.8566\n",
      "Epoch 355/500 | Iter 1420 | Loss: 0.8668\n",
      "Epoch 356/500 | Iter 1424 | Loss: 0.8472\n",
      "Epoch 357/500 | Iter 1428 | Loss: 0.8740\n",
      "Epoch 358/500 | Iter 1432 | Loss: 0.8676\n",
      "Epoch 359/500 | Iter 1436 | Loss: 0.8753\n",
      "Epoch 360/500 | Iter 1440 | Loss: 0.8491\n",
      "Epoch 361/500 | Iter 1444 | Loss: 0.8757\n",
      "Epoch 362/500 | Iter 1448 | Loss: 0.8510\n",
      "Epoch 363/500 | Iter 1452 | Loss: 0.8751\n",
      "Epoch 364/500 | Iter 1456 | Loss: 0.8593\n",
      "Epoch 365/500 | Iter 1460 | Loss: 0.8596\n",
      "Epoch 366/500 | Iter 1464 | Loss: 0.8678\n",
      "Epoch 367/500 | Iter 1468 | Loss: 0.8697\n",
      "Epoch 368/500 | Iter 1472 | Loss: 0.8567\n",
      "Epoch 369/500 | Iter 1476 | Loss: 0.8849\n",
      "Epoch 370/500 | Iter 1480 | Loss: 0.8403\n",
      "Epoch 371/500 | Iter 1484 | Loss: 0.8862\n",
      "Epoch 372/500 | Iter 1488 | Loss: 0.8439\n",
      "Epoch 373/500 | Iter 1492 | Loss: 0.8423\n",
      "Epoch 374/500 | Iter 1496 | Loss: 0.8714\n",
      "Epoch 375/500 | Iter 1500 | Loss: 0.8592\n",
      "Epoch 376/500 | Iter 1504 | Loss: 0.8507\n",
      "Epoch 377/500 | Iter 1508 | Loss: 0.8506\n",
      "Epoch 378/500 | Iter 1512 | Loss: 0.8509\n",
      "Epoch 379/500 | Iter 1516 | Loss: 0.8535\n",
      "Epoch 380/500 | Iter 1520 | Loss: 0.8747\n",
      "Epoch 381/500 | Iter 1524 | Loss: 0.8651\n",
      "Epoch 382/500 | Iter 1528 | Loss: 0.8452\n",
      "Epoch 383/500 | Iter 1532 | Loss: 0.8486\n",
      "Epoch 384/500 | Iter 1536 | Loss: 0.8560\n",
      "Epoch 385/500 | Iter 1540 | Loss: 0.8812\n",
      "Epoch 386/500 | Iter 1544 | Loss: 0.8509\n",
      "Epoch 387/500 | Iter 1548 | Loss: 0.8509\n",
      "Epoch 388/500 | Iter 1552 | Loss: 0.8607\n",
      "Epoch 389/500 | Iter 1556 | Loss: 0.8505\n",
      "Epoch 390/500 | Iter 1560 | Loss: 0.8391\n",
      "Epoch 391/500 | Iter 1564 | Loss: 0.8576\n",
      "Epoch 392/500 | Iter 1568 | Loss: 0.8596\n",
      "Epoch 393/500 | Iter 1572 | Loss: 0.8386\n",
      "Epoch 394/500 | Iter 1576 | Loss: 0.8422\n",
      "Epoch 395/500 | Iter 1580 | Loss: 0.8480\n",
      "Epoch 396/500 | Iter 1584 | Loss: 0.8525\n",
      "Epoch 397/500 | Iter 1588 | Loss: 0.8384\n",
      "Epoch 398/500 | Iter 1592 | Loss: 0.8474\n",
      "Epoch 399/500 | Iter 1596 | Loss: 0.8443\n",
      "Epoch 400/500 | Iter 1600 | Loss: 0.8492\n",
      "Epoch 401/500 | Iter 1604 | Loss: 0.8572\n",
      "Epoch 402/500 | Iter 1608 | Loss: 0.8534\n",
      "Epoch 403/500 | Iter 1612 | Loss: 0.8450\n",
      "Epoch 404/500 | Iter 1616 | Loss: 0.8601\n",
      "Epoch 405/500 | Iter 1620 | Loss: 0.8559\n",
      "Epoch 406/500 | Iter 1624 | Loss: 0.8409\n",
      "Epoch 407/500 | Iter 1628 | Loss: 0.8512\n",
      "Epoch 408/500 | Iter 1632 | Loss: 0.8391\n",
      "Epoch 409/500 | Iter 1636 | Loss: 0.8390\n",
      "Epoch 410/500 | Iter 1640 | Loss: 0.8454\n",
      "Epoch 411/500 | Iter 1644 | Loss: 0.8582\n",
      "Epoch 412/500 | Iter 1648 | Loss: 0.8565\n",
      "Epoch 413/500 | Iter 1652 | Loss: 0.8637\n",
      "Epoch 414/500 | Iter 1656 | Loss: 0.8342\n",
      "Epoch 415/500 | Iter 1660 | Loss: 0.8493\n",
      "Epoch 416/500 | Iter 1664 | Loss: 0.8756\n",
      "Epoch 417/500 | Iter 1668 | Loss: 0.8300\n",
      "Epoch 418/500 | Iter 1672 | Loss: 0.8636\n",
      "Epoch 419/500 | Iter 1676 | Loss: 0.8433\n",
      "Epoch 420/500 | Iter 1680 | Loss: 0.8417\n",
      "Epoch 421/500 | Iter 1684 | Loss: 0.8413\n",
      "Epoch 422/500 | Iter 1688 | Loss: 0.8422\n",
      "Epoch 423/500 | Iter 1692 | Loss: 0.8335\n",
      "Epoch 424/500 | Iter 1696 | Loss: 0.8548\n",
      "Epoch 425/500 | Iter 1700 | Loss: 0.8388\n",
      "Epoch 426/500 | Iter 1704 | Loss: 0.8402\n",
      "Epoch 427/500 | Iter 1708 | Loss: 0.8503\n",
      "Epoch 428/500 | Iter 1712 | Loss: 0.8552\n",
      "Epoch 429/500 | Iter 1716 | Loss: 0.8584\n",
      "Epoch 430/500 | Iter 1720 | Loss: 0.8442\n",
      "Epoch 431/500 | Iter 1724 | Loss: 0.8333\n",
      "Epoch 432/500 | Iter 1728 | Loss: 0.8414\n",
      "Epoch 433/500 | Iter 1732 | Loss: 0.8391\n",
      "Epoch 434/500 | Iter 1736 | Loss: 0.8492\n",
      "Epoch 435/500 | Iter 1740 | Loss: 0.8613\n",
      "Epoch 436/500 | Iter 1744 | Loss: 0.8517\n",
      "Epoch 437/500 | Iter 1748 | Loss: 0.8436\n",
      "Epoch 438/500 | Iter 1752 | Loss: 0.8519\n",
      "Epoch 439/500 | Iter 1756 | Loss: 0.8472\n",
      "Epoch 440/500 | Iter 1760 | Loss: 0.8493\n",
      "Epoch 441/500 | Iter 1764 | Loss: 0.8766\n",
      "Epoch 442/500 | Iter 1768 | Loss: 0.8313\n",
      "Epoch 443/500 | Iter 1772 | Loss: 0.8598\n",
      "Epoch 444/500 | Iter 1776 | Loss: 0.8290\n",
      "Epoch 445/500 | Iter 1780 | Loss: 0.8312\n",
      "Epoch 446/500 | Iter 1784 | Loss: 0.8305\n",
      "Epoch 447/500 | Iter 1788 | Loss: 0.8368\n",
      "Epoch 448/500 | Iter 1792 | Loss: 0.8498\n",
      "Epoch 449/500 | Iter 1796 | Loss: 0.8562\n",
      "Epoch 450/500 | Iter 1800 | Loss: 0.8341\n",
      "Epoch 451/500 | Iter 1804 | Loss: 0.8243\n",
      "Epoch 452/500 | Iter 1808 | Loss: 0.8291\n",
      "Epoch 453/500 | Iter 1812 | Loss: 0.8254\n",
      "Epoch 454/500 | Iter 1816 | Loss: 0.8345\n",
      "Epoch 455/500 | Iter 1820 | Loss: 0.8483\n",
      "Epoch 456/500 | Iter 1824 | Loss: 0.8336\n",
      "Epoch 457/500 | Iter 1828 | Loss: 0.8396\n",
      "Epoch 458/500 | Iter 1832 | Loss: 0.8196\n",
      "Epoch 459/500 | Iter 1836 | Loss: 0.8486\n",
      "Epoch 460/500 | Iter 1840 | Loss: 0.8310\n",
      "Epoch 461/500 | Iter 1844 | Loss: 0.8266\n",
      "Epoch 462/500 | Iter 1848 | Loss: 0.8360\n",
      "Epoch 463/500 | Iter 1852 | Loss: 0.8290\n",
      "Epoch 464/500 | Iter 1856 | Loss: 0.8432\n",
      "Epoch 465/500 | Iter 1860 | Loss: 0.8345\n",
      "Epoch 466/500 | Iter 1864 | Loss: 0.8319\n",
      "Epoch 467/500 | Iter 1868 | Loss: 0.8415\n",
      "Epoch 468/500 | Iter 1872 | Loss: 0.8416\n",
      "Epoch 469/500 | Iter 1876 | Loss: 0.8302\n",
      "Epoch 470/500 | Iter 1880 | Loss: 0.8288\n",
      "Epoch 471/500 | Iter 1884 | Loss: 0.8793\n",
      "Epoch 472/500 | Iter 1888 | Loss: 0.8290\n",
      "Epoch 473/500 | Iter 1892 | Loss: 0.8498\n",
      "Epoch 474/500 | Iter 1896 | Loss: 0.8459\n",
      "Epoch 475/500 | Iter 1900 | Loss: 0.8307\n",
      "Epoch 476/500 | Iter 1904 | Loss: 0.8295\n",
      "Epoch 477/500 | Iter 1908 | Loss: 0.8440\n",
      "Epoch 478/500 | Iter 1912 | Loss: 0.8451\n",
      "Epoch 479/500 | Iter 1916 | Loss: 0.8384\n",
      "Epoch 480/500 | Iter 1920 | Loss: 0.8484\n",
      "Epoch 481/500 | Iter 1924 | Loss: 0.8340\n",
      "Epoch 482/500 | Iter 1928 | Loss: 0.8348\n",
      "Epoch 483/500 | Iter 1932 | Loss: 0.8358\n",
      "Epoch 484/500 | Iter 1936 | Loss: 0.8803\n",
      "Epoch 485/500 | Iter 1940 | Loss: 0.8195\n",
      "Epoch 486/500 | Iter 1944 | Loss: 0.8250\n",
      "Epoch 487/500 | Iter 1948 | Loss: 0.8270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:99: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return self.softmax(X @ W)\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:99: RuntimeWarning: overflow encountered in matmul\n",
      "  return self.softmax(X @ W)\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:99: RuntimeWarning: invalid value encountered in matmul\n",
      "  return self.softmax(X @ W)\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:96: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X.T @ error\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:96: RuntimeWarning: overflow encountered in matmul\n",
      "  return X.T @ error\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:96: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X.T @ error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 488/500 | Iter 1952 | Loss: 0.8280\n",
      "Epoch 489/500 | Iter 1956 | Loss: 0.8330\n",
      "Epoch 490/500 | Iter 1960 | Loss: 0.8508\n",
      "Epoch 491/500 | Iter 1964 | Loss: 0.8178\n",
      "Epoch 492/500 | Iter 1968 | Loss: 0.8452\n",
      "Epoch 493/500 | Iter 1972 | Loss: 0.8386\n",
      "Epoch 494/500 | Iter 1976 | Loss: 0.8370\n",
      "Epoch 495/500 | Iter 1980 | Loss: 0.8602\n",
      "Epoch 496/500 | Iter 1984 | Loss: 0.8404\n",
      "Epoch 497/500 | Iter 1988 | Loss: 0.8408\n",
      "Epoch 498/500 | Iter 1992 | Loss: 0.8121\n",
      "Epoch 499/500 | Iter 1996 | Loss: 0.8134\n",
      "Time taken: 0.20s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\tPrecision\tRecall\tF1-Score\tSupport\n",
      "------------------------------------------------------------\n",
      "0\t0.78\t\t0.82\t0.80\t\t639\n",
      "1\t0.56\t\t0.52\t0.54\t\t614\n",
      "2\t0.55\t\t0.52\t0.53\t\t564\n",
      "3\t0.72\t\t0.77\t0.74\t\t592\n",
      "------------------------------------------------------------\n",
      "Macro Avg\t0.65\t\t0.66\t0.65\n",
      "Weighted Avg\t0.65\t\t0.66\t0.66\n",
      "Accuracy\t0.66\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with mini-batch + L2 penalty\n",
    "model = LogisticRegression(\n",
    "    k=k,\n",
    "    n=n,\n",
    "    method=\"minibatch\",  \n",
    "    alpha=0.01,\n",
    "    max_iter=2000,\n",
    "    use_penalty=True,    \n",
    "    lambda_=0.1           \n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, Y_train_encoded)\n",
    "\n",
    "# Plot loss curve\n",
    "model.plot()\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "model.classification_report(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08ed1802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to AIT MLflow server as admin; experiment 'A3_st125999' set.\n",
      "\n",
      "🚀 Running BATCH Optimization...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:99: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return self.softmax(X @ W)\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:99: RuntimeWarning: overflow encountered in matmul\n",
      "  return self.softmax(X @ W)\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:99: RuntimeWarning: invalid value encountered in matmul\n",
      "  return self.softmax(X @ W)\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:96: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X.T @ error\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:96: RuntimeWarning: overflow encountered in matmul\n",
      "  return X.T @ error\n",
      "/var/folders/ql/b2rphw6s4db5krtz1b0b3z7r0000gn/T/ipykernel_79961/1517943253.py:96: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X.T @ error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500 | Iter 0 | Loss: 1.6959\n",
      "Epoch 1/500 | Iter 4 | Loss: 1.6943\n",
      "Epoch 2/500 | Iter 8 | Loss: 1.6927\n",
      "Epoch 3/500 | Iter 12 | Loss: 1.6910\n",
      "Epoch 4/500 | Iter 16 | Loss: 1.6894\n",
      "Epoch 5/500 | Iter 20 | Loss: 1.6878\n",
      "Epoch 6/500 | Iter 24 | Loss: 1.6862\n",
      "Epoch 7/500 | Iter 28 | Loss: 1.6846\n",
      "Epoch 8/500 | Iter 32 | Loss: 1.6830\n",
      "Epoch 9/500 | Iter 36 | Loss: 1.6814\n",
      "Epoch 10/500 | Iter 40 | Loss: 1.6798\n",
      "Epoch 11/500 | Iter 44 | Loss: 1.6782\n",
      "Epoch 12/500 | Iter 48 | Loss: 1.6766\n",
      "Epoch 13/500 | Iter 52 | Loss: 1.6750\n",
      "Epoch 14/500 | Iter 56 | Loss: 1.6735\n",
      "Epoch 15/500 | Iter 60 | Loss: 1.6719\n",
      "Epoch 16/500 | Iter 64 | Loss: 1.6703\n",
      "Epoch 17/500 | Iter 68 | Loss: 1.6687\n",
      "Epoch 18/500 | Iter 72 | Loss: 1.6672\n",
      "Epoch 19/500 | Iter 76 | Loss: 1.6656\n",
      "Epoch 20/500 | Iter 80 | Loss: 1.6641\n",
      "Epoch 21/500 | Iter 84 | Loss: 1.6625\n",
      "Epoch 22/500 | Iter 88 | Loss: 1.6609\n",
      "Epoch 23/500 | Iter 92 | Loss: 1.6594\n",
      "Epoch 24/500 | Iter 96 | Loss: 1.6579\n",
      "Epoch 25/500 | Iter 100 | Loss: 1.6563\n",
      "Epoch 26/500 | Iter 104 | Loss: 1.6548\n",
      "Epoch 27/500 | Iter 108 | Loss: 1.6532\n",
      "Epoch 28/500 | Iter 112 | Loss: 1.6517\n",
      "Epoch 29/500 | Iter 116 | Loss: 1.6502\n",
      "Epoch 30/500 | Iter 120 | Loss: 1.6486\n",
      "Epoch 31/500 | Iter 124 | Loss: 1.6471\n",
      "Epoch 32/500 | Iter 128 | Loss: 1.6456\n",
      "Epoch 33/500 | Iter 132 | Loss: 1.6441\n",
      "Epoch 34/500 | Iter 136 | Loss: 1.6426\n",
      "Epoch 35/500 | Iter 140 | Loss: 1.6411\n",
      "Epoch 36/500 | Iter 144 | Loss: 1.6396\n",
      "Epoch 37/500 | Iter 148 | Loss: 1.6381\n",
      "Epoch 38/500 | Iter 152 | Loss: 1.6366\n",
      "Epoch 39/500 | Iter 156 | Loss: 1.6351\n",
      "Epoch 40/500 | Iter 160 | Loss: 1.6336\n",
      "Epoch 41/500 | Iter 164 | Loss: 1.6321\n",
      "Epoch 42/500 | Iter 168 | Loss: 1.6306\n",
      "Epoch 43/500 | Iter 172 | Loss: 1.6291\n",
      "Epoch 44/500 | Iter 176 | Loss: 1.6276\n",
      "Epoch 45/500 | Iter 180 | Loss: 1.6261\n",
      "Epoch 46/500 | Iter 184 | Loss: 1.6247\n",
      "Epoch 47/500 | Iter 188 | Loss: 1.6232\n",
      "Epoch 48/500 | Iter 192 | Loss: 1.6217\n",
      "Epoch 49/500 | Iter 196 | Loss: 1.6203\n",
      "Epoch 50/500 | Iter 200 | Loss: 1.6188\n",
      "Epoch 51/500 | Iter 204 | Loss: 1.6174\n",
      "Epoch 52/500 | Iter 208 | Loss: 1.6159\n",
      "Epoch 53/500 | Iter 212 | Loss: 1.6145\n",
      "Epoch 54/500 | Iter 216 | Loss: 1.6130\n",
      "Epoch 55/500 | Iter 220 | Loss: 1.6116\n",
      "Epoch 56/500 | Iter 224 | Loss: 1.6101\n",
      "Epoch 57/500 | Iter 228 | Loss: 1.6087\n",
      "Epoch 58/500 | Iter 232 | Loss: 1.6072\n",
      "Epoch 59/500 | Iter 236 | Loss: 1.6058\n",
      "Epoch 60/500 | Iter 240 | Loss: 1.6044\n",
      "Epoch 61/500 | Iter 244 | Loss: 1.6030\n",
      "Epoch 62/500 | Iter 248 | Loss: 1.6015\n",
      "Epoch 63/500 | Iter 252 | Loss: 1.6001\n",
      "Epoch 64/500 | Iter 256 | Loss: 1.5987\n",
      "Epoch 65/500 | Iter 260 | Loss: 1.5973\n",
      "Epoch 66/500 | Iter 264 | Loss: 1.5959\n",
      "Epoch 67/500 | Iter 268 | Loss: 1.5945\n",
      "Epoch 68/500 | Iter 272 | Loss: 1.5931\n",
      "Epoch 69/500 | Iter 276 | Loss: 1.5917\n",
      "Epoch 70/500 | Iter 280 | Loss: 1.5903\n",
      "Epoch 71/500 | Iter 284 | Loss: 1.5889\n",
      "Epoch 72/500 | Iter 288 | Loss: 1.5875\n",
      "Epoch 73/500 | Iter 292 | Loss: 1.5861\n",
      "Epoch 74/500 | Iter 296 | Loss: 1.5847\n",
      "Epoch 75/500 | Iter 300 | Loss: 1.5833\n",
      "Epoch 76/500 | Iter 304 | Loss: 1.5820\n",
      "Epoch 77/500 | Iter 308 | Loss: 1.5806\n",
      "Epoch 78/500 | Iter 312 | Loss: 1.5792\n",
      "Epoch 79/500 | Iter 316 | Loss: 1.5779\n",
      "Epoch 80/500 | Iter 320 | Loss: 1.5765\n",
      "Epoch 81/500 | Iter 324 | Loss: 1.5751\n",
      "Epoch 82/500 | Iter 328 | Loss: 1.5738\n",
      "Epoch 83/500 | Iter 332 | Loss: 1.5724\n",
      "Epoch 84/500 | Iter 336 | Loss: 1.5711\n",
      "Epoch 85/500 | Iter 340 | Loss: 1.5697\n",
      "Epoch 86/500 | Iter 344 | Loss: 1.5684\n",
      "Epoch 87/500 | Iter 348 | Loss: 1.5670\n",
      "Epoch 88/500 | Iter 352 | Loss: 1.5657\n",
      "Epoch 89/500 | Iter 356 | Loss: 1.5644\n",
      "Epoch 90/500 | Iter 360 | Loss: 1.5630\n",
      "Epoch 91/500 | Iter 364 | Loss: 1.5617\n",
      "Epoch 92/500 | Iter 368 | Loss: 1.5604\n",
      "Epoch 93/500 | Iter 372 | Loss: 1.5590\n",
      "Epoch 94/500 | Iter 376 | Loss: 1.5577\n",
      "Epoch 95/500 | Iter 380 | Loss: 1.5564\n",
      "Epoch 96/500 | Iter 384 | Loss: 1.5551\n",
      "Epoch 97/500 | Iter 388 | Loss: 1.5538\n",
      "Epoch 98/500 | Iter 392 | Loss: 1.5525\n",
      "Epoch 99/500 | Iter 396 | Loss: 1.5511\n",
      "Epoch 100/500 | Iter 400 | Loss: 1.5498\n",
      "Epoch 101/500 | Iter 404 | Loss: 1.5485\n",
      "Epoch 102/500 | Iter 408 | Loss: 1.5472\n",
      "Epoch 103/500 | Iter 412 | Loss: 1.5460\n",
      "Epoch 104/500 | Iter 416 | Loss: 1.5447\n",
      "Epoch 105/500 | Iter 420 | Loss: 1.5434\n",
      "Epoch 106/500 | Iter 424 | Loss: 1.5421\n",
      "Epoch 107/500 | Iter 428 | Loss: 1.5408\n",
      "Epoch 108/500 | Iter 432 | Loss: 1.5395\n",
      "Epoch 109/500 | Iter 436 | Loss: 1.5382\n",
      "Epoch 110/500 | Iter 440 | Loss: 1.5370\n",
      "Epoch 111/500 | Iter 444 | Loss: 1.5357\n",
      "Epoch 112/500 | Iter 448 | Loss: 1.5344\n",
      "Epoch 113/500 | Iter 452 | Loss: 1.5332\n",
      "Epoch 114/500 | Iter 456 | Loss: 1.5319\n",
      "Epoch 115/500 | Iter 460 | Loss: 1.5306\n",
      "Epoch 116/500 | Iter 464 | Loss: 1.5294\n",
      "Epoch 117/500 | Iter 468 | Loss: 1.5281\n",
      "Epoch 118/500 | Iter 472 | Loss: 1.5269\n",
      "Epoch 119/500 | Iter 476 | Loss: 1.5256\n",
      "Epoch 120/500 | Iter 480 | Loss: 1.5244\n",
      "Epoch 121/500 | Iter 484 | Loss: 1.5232\n",
      "Epoch 122/500 | Iter 488 | Loss: 1.5219\n",
      "Epoch 123/500 | Iter 492 | Loss: 1.5207\n",
      "Epoch 124/500 | Iter 496 | Loss: 1.5194\n",
      "Epoch 125/500 | Iter 500 | Loss: 1.5182\n",
      "Epoch 126/500 | Iter 504 | Loss: 1.5170\n",
      "Epoch 127/500 | Iter 508 | Loss: 1.5158\n",
      "Epoch 128/500 | Iter 512 | Loss: 1.5145\n",
      "Epoch 129/500 | Iter 516 | Loss: 1.5133\n",
      "Epoch 130/500 | Iter 520 | Loss: 1.5121\n",
      "Epoch 131/500 | Iter 524 | Loss: 1.5109\n",
      "Epoch 132/500 | Iter 528 | Loss: 1.5097\n",
      "Epoch 133/500 | Iter 532 | Loss: 1.5085\n",
      "Epoch 134/500 | Iter 536 | Loss: 1.5073\n",
      "Epoch 135/500 | Iter 540 | Loss: 1.5061\n",
      "Epoch 136/500 | Iter 544 | Loss: 1.5049\n",
      "Epoch 137/500 | Iter 548 | Loss: 1.5037\n",
      "Epoch 138/500 | Iter 552 | Loss: 1.5025\n",
      "Epoch 139/500 | Iter 556 | Loss: 1.5013\n",
      "Epoch 140/500 | Iter 560 | Loss: 1.5001\n",
      "Epoch 141/500 | Iter 564 | Loss: 1.4989\n",
      "Epoch 142/500 | Iter 568 | Loss: 1.4977\n",
      "Epoch 143/500 | Iter 572 | Loss: 1.4966\n",
      "Epoch 144/500 | Iter 576 | Loss: 1.4954\n",
      "Epoch 145/500 | Iter 580 | Loss: 1.4942\n",
      "Epoch 146/500 | Iter 584 | Loss: 1.4930\n",
      "Epoch 147/500 | Iter 588 | Loss: 1.4919\n",
      "Epoch 148/500 | Iter 592 | Loss: 1.4907\n",
      "Epoch 149/500 | Iter 596 | Loss: 1.4895\n",
      "Epoch 150/500 | Iter 600 | Loss: 1.4884\n",
      "Epoch 151/500 | Iter 604 | Loss: 1.4872\n",
      "Epoch 152/500 | Iter 608 | Loss: 1.4861\n",
      "Epoch 153/500 | Iter 612 | Loss: 1.4849\n",
      "Epoch 154/500 | Iter 616 | Loss: 1.4838\n",
      "Epoch 155/500 | Iter 620 | Loss: 1.4826\n",
      "Epoch 156/500 | Iter 624 | Loss: 1.4815\n",
      "Epoch 157/500 | Iter 628 | Loss: 1.4803\n",
      "Epoch 158/500 | Iter 632 | Loss: 1.4792\n",
      "Epoch 159/500 | Iter 636 | Loss: 1.4781\n",
      "Epoch 160/500 | Iter 640 | Loss: 1.4769\n",
      "Epoch 161/500 | Iter 644 | Loss: 1.4758\n",
      "Epoch 162/500 | Iter 648 | Loss: 1.4747\n",
      "Epoch 163/500 | Iter 652 | Loss: 1.4735\n",
      "Epoch 164/500 | Iter 656 | Loss: 1.4724\n",
      "Epoch 165/500 | Iter 660 | Loss: 1.4713\n",
      "Epoch 166/500 | Iter 664 | Loss: 1.4702\n",
      "Epoch 167/500 | Iter 668 | Loss: 1.4691\n",
      "Epoch 168/500 | Iter 672 | Loss: 1.4680\n",
      "Epoch 169/500 | Iter 676 | Loss: 1.4669\n",
      "Epoch 170/500 | Iter 680 | Loss: 1.4657\n",
      "Epoch 171/500 | Iter 684 | Loss: 1.4646\n",
      "Epoch 172/500 | Iter 688 | Loss: 1.4635\n",
      "Epoch 173/500 | Iter 692 | Loss: 1.4624\n",
      "Epoch 174/500 | Iter 696 | Loss: 1.4613\n",
      "Epoch 175/500 | Iter 700 | Loss: 1.4603\n",
      "Epoch 176/500 | Iter 704 | Loss: 1.4592\n",
      "Epoch 177/500 | Iter 708 | Loss: 1.4581\n",
      "Epoch 178/500 | Iter 712 | Loss: 1.4570\n",
      "Epoch 179/500 | Iter 716 | Loss: 1.4559\n",
      "Epoch 180/500 | Iter 720 | Loss: 1.4548\n",
      "Epoch 181/500 | Iter 724 | Loss: 1.4537\n",
      "Epoch 182/500 | Iter 728 | Loss: 1.4527\n",
      "Epoch 183/500 | Iter 732 | Loss: 1.4516\n",
      "Epoch 184/500 | Iter 736 | Loss: 1.4505\n",
      "Epoch 185/500 | Iter 740 | Loss: 1.4495\n",
      "Epoch 186/500 | Iter 744 | Loss: 1.4484\n",
      "Epoch 187/500 | Iter 748 | Loss: 1.4473\n",
      "Epoch 188/500 | Iter 752 | Loss: 1.4463\n",
      "Epoch 189/500 | Iter 756 | Loss: 1.4452\n",
      "Epoch 190/500 | Iter 760 | Loss: 1.4442\n",
      "Epoch 191/500 | Iter 764 | Loss: 1.4431\n",
      "Epoch 192/500 | Iter 768 | Loss: 1.4421\n",
      "Epoch 193/500 | Iter 772 | Loss: 1.4410\n",
      "Epoch 194/500 | Iter 776 | Loss: 1.4400\n",
      "Epoch 195/500 | Iter 780 | Loss: 1.4389\n",
      "Epoch 196/500 | Iter 784 | Loss: 1.4379\n",
      "Epoch 197/500 | Iter 788 | Loss: 1.4368\n",
      "Epoch 198/500 | Iter 792 | Loss: 1.4358\n",
      "Epoch 199/500 | Iter 796 | Loss: 1.4348\n",
      "Epoch 200/500 | Iter 800 | Loss: 1.4337\n",
      "Epoch 201/500 | Iter 804 | Loss: 1.4327\n",
      "Epoch 202/500 | Iter 808 | Loss: 1.4317\n",
      "Epoch 203/500 | Iter 812 | Loss: 1.4307\n",
      "Epoch 204/500 | Iter 816 | Loss: 1.4297\n",
      "Epoch 205/500 | Iter 820 | Loss: 1.4286\n",
      "Epoch 206/500 | Iter 824 | Loss: 1.4276\n",
      "Epoch 207/500 | Iter 828 | Loss: 1.4266\n",
      "Epoch 208/500 | Iter 832 | Loss: 1.4256\n",
      "Epoch 209/500 | Iter 836 | Loss: 1.4246\n",
      "Epoch 210/500 | Iter 840 | Loss: 1.4236\n",
      "Epoch 211/500 | Iter 844 | Loss: 1.4226\n",
      "Epoch 212/500 | Iter 848 | Loss: 1.4216\n",
      "Epoch 213/500 | Iter 852 | Loss: 1.4206\n",
      "Epoch 214/500 | Iter 856 | Loss: 1.4196\n",
      "Epoch 215/500 | Iter 860 | Loss: 1.4186\n",
      "Epoch 216/500 | Iter 864 | Loss: 1.4176\n",
      "Epoch 217/500 | Iter 868 | Loss: 1.4166\n",
      "Epoch 218/500 | Iter 872 | Loss: 1.4156\n",
      "Epoch 219/500 | Iter 876 | Loss: 1.4146\n",
      "Epoch 220/500 | Iter 880 | Loss: 1.4137\n",
      "Epoch 221/500 | Iter 884 | Loss: 1.4127\n",
      "Epoch 222/500 | Iter 888 | Loss: 1.4117\n",
      "Epoch 223/500 | Iter 892 | Loss: 1.4107\n",
      "Epoch 224/500 | Iter 896 | Loss: 1.4097\n",
      "Epoch 225/500 | Iter 900 | Loss: 1.4088\n",
      "Epoch 226/500 | Iter 904 | Loss: 1.4078\n",
      "Epoch 227/500 | Iter 908 | Loss: 1.4068\n",
      "Epoch 228/500 | Iter 912 | Loss: 1.4059\n",
      "Epoch 229/500 | Iter 916 | Loss: 1.4049\n",
      "Epoch 230/500 | Iter 920 | Loss: 1.4040\n",
      "Epoch 231/500 | Iter 924 | Loss: 1.4030\n",
      "Epoch 232/500 | Iter 928 | Loss: 1.4021\n",
      "Epoch 233/500 | Iter 932 | Loss: 1.4011\n",
      "Epoch 234/500 | Iter 936 | Loss: 1.4001\n",
      "Epoch 235/500 | Iter 940 | Loss: 1.3992\n",
      "Epoch 236/500 | Iter 944 | Loss: 1.3983\n",
      "Epoch 237/500 | Iter 948 | Loss: 1.3973\n",
      "Epoch 238/500 | Iter 952 | Loss: 1.3964\n",
      "Epoch 239/500 | Iter 956 | Loss: 1.3954\n",
      "Epoch 240/500 | Iter 960 | Loss: 1.3945\n",
      "Epoch 241/500 | Iter 964 | Loss: 1.3936\n",
      "Epoch 242/500 | Iter 968 | Loss: 1.3926\n",
      "Epoch 243/500 | Iter 972 | Loss: 1.3917\n",
      "Epoch 244/500 | Iter 976 | Loss: 1.3908\n",
      "Epoch 245/500 | Iter 980 | Loss: 1.3899\n",
      "Epoch 246/500 | Iter 984 | Loss: 1.3889\n",
      "Epoch 247/500 | Iter 988 | Loss: 1.3880\n",
      "Epoch 248/500 | Iter 992 | Loss: 1.3871\n",
      "Epoch 249/500 | Iter 996 | Loss: 1.3862\n",
      "Epoch 250/500 | Iter 1000 | Loss: 1.3853\n",
      "Epoch 251/500 | Iter 1004 | Loss: 1.3843\n",
      "Epoch 252/500 | Iter 1008 | Loss: 1.3834\n",
      "Epoch 253/500 | Iter 1012 | Loss: 1.3825\n",
      "Epoch 254/500 | Iter 1016 | Loss: 1.3816\n",
      "Epoch 255/500 | Iter 1020 | Loss: 1.3807\n",
      "Epoch 256/500 | Iter 1024 | Loss: 1.3798\n",
      "Epoch 257/500 | Iter 1028 | Loss: 1.3789\n",
      "Epoch 258/500 | Iter 1032 | Loss: 1.3780\n",
      "Epoch 259/500 | Iter 1036 | Loss: 1.3771\n",
      "Epoch 260/500 | Iter 1040 | Loss: 1.3762\n",
      "Epoch 261/500 | Iter 1044 | Loss: 1.3754\n",
      "Epoch 262/500 | Iter 1048 | Loss: 1.3745\n",
      "Epoch 263/500 | Iter 1052 | Loss: 1.3736\n",
      "Epoch 264/500 | Iter 1056 | Loss: 1.3727\n",
      "Epoch 265/500 | Iter 1060 | Loss: 1.3718\n",
      "Epoch 266/500 | Iter 1064 | Loss: 1.3709\n",
      "Epoch 267/500 | Iter 1068 | Loss: 1.3701\n",
      "Epoch 268/500 | Iter 1072 | Loss: 1.3692\n",
      "Epoch 269/500 | Iter 1076 | Loss: 1.3683\n",
      "Epoch 270/500 | Iter 1080 | Loss: 1.3674\n",
      "Epoch 271/500 | Iter 1084 | Loss: 1.3666\n",
      "Epoch 272/500 | Iter 1088 | Loss: 1.3657\n",
      "Epoch 273/500 | Iter 1092 | Loss: 1.3648\n",
      "Epoch 274/500 | Iter 1096 | Loss: 1.3640\n",
      "Epoch 275/500 | Iter 1100 | Loss: 1.3631\n",
      "Epoch 276/500 | Iter 1104 | Loss: 1.3623\n",
      "Epoch 277/500 | Iter 1108 | Loss: 1.3614\n",
      "Epoch 278/500 | Iter 1112 | Loss: 1.3605\n",
      "Epoch 279/500 | Iter 1116 | Loss: 1.3597\n",
      "Epoch 280/500 | Iter 1120 | Loss: 1.3588\n",
      "Epoch 281/500 | Iter 1124 | Loss: 1.3580\n",
      "Epoch 282/500 | Iter 1128 | Loss: 1.3571\n",
      "Epoch 283/500 | Iter 1132 | Loss: 1.3563\n",
      "Epoch 284/500 | Iter 1136 | Loss: 1.3555\n",
      "Epoch 285/500 | Iter 1140 | Loss: 1.3546\n",
      "Epoch 286/500 | Iter 1144 | Loss: 1.3538\n",
      "Epoch 287/500 | Iter 1148 | Loss: 1.3529\n",
      "Epoch 288/500 | Iter 1152 | Loss: 1.3521\n",
      "Epoch 289/500 | Iter 1156 | Loss: 1.3513\n",
      "Epoch 290/500 | Iter 1160 | Loss: 1.3504\n",
      "Epoch 291/500 | Iter 1164 | Loss: 1.3496\n",
      "Epoch 292/500 | Iter 1168 | Loss: 1.3488\n",
      "Epoch 293/500 | Iter 1172 | Loss: 1.3480\n",
      "Epoch 294/500 | Iter 1176 | Loss: 1.3471\n",
      "Epoch 295/500 | Iter 1180 | Loss: 1.3463\n",
      "Epoch 296/500 | Iter 1184 | Loss: 1.3455\n",
      "Epoch 297/500 | Iter 1188 | Loss: 1.3447\n",
      "Epoch 298/500 | Iter 1192 | Loss: 1.3439\n",
      "Epoch 299/500 | Iter 1196 | Loss: 1.3431\n",
      "Epoch 300/500 | Iter 1200 | Loss: 1.3423\n",
      "Epoch 301/500 | Iter 1204 | Loss: 1.3414\n",
      "Epoch 302/500 | Iter 1208 | Loss: 1.3406\n",
      "Epoch 303/500 | Iter 1212 | Loss: 1.3398\n",
      "Epoch 304/500 | Iter 1216 | Loss: 1.3390\n",
      "Epoch 305/500 | Iter 1220 | Loss: 1.3382\n",
      "Epoch 306/500 | Iter 1224 | Loss: 1.3374\n",
      "Epoch 307/500 | Iter 1228 | Loss: 1.3366\n",
      "Epoch 308/500 | Iter 1232 | Loss: 1.3358\n",
      "Epoch 309/500 | Iter 1236 | Loss: 1.3350\n",
      "Epoch 310/500 | Iter 1240 | Loss: 1.3343\n",
      "Epoch 311/500 | Iter 1244 | Loss: 1.3335\n",
      "Epoch 312/500 | Iter 1248 | Loss: 1.3327\n",
      "Epoch 313/500 | Iter 1252 | Loss: 1.3319\n",
      "Epoch 314/500 | Iter 1256 | Loss: 1.3311\n",
      "Epoch 315/500 | Iter 1260 | Loss: 1.3303\n",
      "Epoch 316/500 | Iter 1264 | Loss: 1.3295\n",
      "Epoch 317/500 | Iter 1268 | Loss: 1.3288\n",
      "Epoch 318/500 | Iter 1272 | Loss: 1.3280\n",
      "Epoch 319/500 | Iter 1276 | Loss: 1.3272\n",
      "Epoch 320/500 | Iter 1280 | Loss: 1.3264\n",
      "Epoch 321/500 | Iter 1284 | Loss: 1.3257\n",
      "Epoch 322/500 | Iter 1288 | Loss: 1.3249\n",
      "Epoch 323/500 | Iter 1292 | Loss: 1.3241\n",
      "Epoch 324/500 | Iter 1296 | Loss: 1.3234\n",
      "Epoch 325/500 | Iter 1300 | Loss: 1.3226\n",
      "Epoch 326/500 | Iter 1304 | Loss: 1.3218\n",
      "Epoch 327/500 | Iter 1308 | Loss: 1.3211\n",
      "Epoch 328/500 | Iter 1312 | Loss: 1.3203\n",
      "Epoch 329/500 | Iter 1316 | Loss: 1.3196\n",
      "Epoch 330/500 | Iter 1320 | Loss: 1.3188\n",
      "Epoch 331/500 | Iter 1324 | Loss: 1.3181\n",
      "Epoch 332/500 | Iter 1328 | Loss: 1.3173\n",
      "Epoch 333/500 | Iter 1332 | Loss: 1.3166\n",
      "Epoch 334/500 | Iter 1336 | Loss: 1.3158\n",
      "Epoch 335/500 | Iter 1340 | Loss: 1.3151\n",
      "Epoch 336/500 | Iter 1344 | Loss: 1.3143\n",
      "Epoch 337/500 | Iter 1348 | Loss: 1.3136\n",
      "Epoch 338/500 | Iter 1352 | Loss: 1.3129\n",
      "Epoch 339/500 | Iter 1356 | Loss: 1.3121\n",
      "Epoch 340/500 | Iter 1360 | Loss: 1.3114\n",
      "Epoch 341/500 | Iter 1364 | Loss: 1.3106\n",
      "Epoch 342/500 | Iter 1368 | Loss: 1.3099\n",
      "Epoch 343/500 | Iter 1372 | Loss: 1.3092\n",
      "Epoch 344/500 | Iter 1376 | Loss: 1.3084\n",
      "Epoch 345/500 | Iter 1380 | Loss: 1.3077\n",
      "Epoch 346/500 | Iter 1384 | Loss: 1.3070\n",
      "Epoch 347/500 | Iter 1388 | Loss: 1.3063\n",
      "Epoch 348/500 | Iter 1392 | Loss: 1.3055\n",
      "Epoch 349/500 | Iter 1396 | Loss: 1.3048\n",
      "Epoch 350/500 | Iter 1400 | Loss: 1.3041\n",
      "Epoch 351/500 | Iter 1404 | Loss: 1.3034\n",
      "Epoch 352/500 | Iter 1408 | Loss: 1.3027\n",
      "Epoch 353/500 | Iter 1412 | Loss: 1.3020\n",
      "Epoch 354/500 | Iter 1416 | Loss: 1.3012\n",
      "Epoch 355/500 | Iter 1420 | Loss: 1.3005\n",
      "Epoch 356/500 | Iter 1424 | Loss: 1.2998\n",
      "Epoch 357/500 | Iter 1428 | Loss: 1.2991\n",
      "Epoch 358/500 | Iter 1432 | Loss: 1.2984\n",
      "Epoch 359/500 | Iter 1436 | Loss: 1.2977\n",
      "Epoch 360/500 | Iter 1440 | Loss: 1.2970\n",
      "Epoch 361/500 | Iter 1444 | Loss: 1.2963\n",
      "Epoch 362/500 | Iter 1448 | Loss: 1.2956\n",
      "Epoch 363/500 | Iter 1452 | Loss: 1.2949\n",
      "Epoch 364/500 | Iter 1456 | Loss: 1.2942\n",
      "Epoch 365/500 | Iter 1460 | Loss: 1.2935\n",
      "Epoch 366/500 | Iter 1464 | Loss: 1.2928\n",
      "Epoch 367/500 | Iter 1468 | Loss: 1.2921\n",
      "Epoch 368/500 | Iter 1472 | Loss: 1.2914\n",
      "Epoch 369/500 | Iter 1476 | Loss: 1.2908\n",
      "Epoch 370/500 | Iter 1480 | Loss: 1.2901\n",
      "Epoch 371/500 | Iter 1484 | Loss: 1.2894\n",
      "Epoch 372/500 | Iter 1488 | Loss: 1.2887\n",
      "Epoch 373/500 | Iter 1492 | Loss: 1.2880\n",
      "Epoch 374/500 | Iter 1496 | Loss: 1.2873\n",
      "Epoch 375/500 | Iter 1500 | Loss: 1.2867\n",
      "Epoch 376/500 | Iter 1504 | Loss: 1.2860\n",
      "Epoch 377/500 | Iter 1508 | Loss: 1.2853\n",
      "Epoch 378/500 | Iter 1512 | Loss: 1.2846\n",
      "Epoch 379/500 | Iter 1516 | Loss: 1.2840\n",
      "Epoch 380/500 | Iter 1520 | Loss: 1.2833\n",
      "Epoch 381/500 | Iter 1524 | Loss: 1.2826\n",
      "Epoch 382/500 | Iter 1528 | Loss: 1.2820\n",
      "Epoch 383/500 | Iter 1532 | Loss: 1.2813\n",
      "Epoch 384/500 | Iter 1536 | Loss: 1.2806\n",
      "Epoch 385/500 | Iter 1540 | Loss: 1.2800\n",
      "Epoch 386/500 | Iter 1544 | Loss: 1.2793\n",
      "Epoch 387/500 | Iter 1548 | Loss: 1.2787\n",
      "Epoch 388/500 | Iter 1552 | Loss: 1.2780\n",
      "Epoch 389/500 | Iter 1556 | Loss: 1.2773\n",
      "Epoch 390/500 | Iter 1560 | Loss: 1.2767\n",
      "Epoch 391/500 | Iter 1564 | Loss: 1.2760\n",
      "Epoch 392/500 | Iter 1568 | Loss: 1.2754\n",
      "Epoch 393/500 | Iter 1572 | Loss: 1.2747\n",
      "Epoch 394/500 | Iter 1576 | Loss: 1.2741\n",
      "Epoch 395/500 | Iter 1580 | Loss: 1.2734\n",
      "Epoch 396/500 | Iter 1584 | Loss: 1.2728\n",
      "Epoch 397/500 | Iter 1588 | Loss: 1.2722\n",
      "Epoch 398/500 | Iter 1592 | Loss: 1.2715\n",
      "Epoch 399/500 | Iter 1596 | Loss: 1.2709\n",
      "Epoch 400/500 | Iter 1600 | Loss: 1.2702\n",
      "Epoch 401/500 | Iter 1604 | Loss: 1.2696\n",
      "Epoch 402/500 | Iter 1608 | Loss: 1.2690\n",
      "Epoch 403/500 | Iter 1612 | Loss: 1.2683\n",
      "Epoch 404/500 | Iter 1616 | Loss: 1.2677\n",
      "Epoch 405/500 | Iter 1620 | Loss: 1.2671\n",
      "Epoch 406/500 | Iter 1624 | Loss: 1.2664\n",
      "Epoch 407/500 | Iter 1628 | Loss: 1.2658\n",
      "Epoch 408/500 | Iter 1632 | Loss: 1.2652\n",
      "Epoch 409/500 | Iter 1636 | Loss: 1.2645\n",
      "Epoch 410/500 | Iter 1640 | Loss: 1.2639\n",
      "Epoch 411/500 | Iter 1644 | Loss: 1.2633\n",
      "Epoch 412/500 | Iter 1648 | Loss: 1.2627\n",
      "Epoch 413/500 | Iter 1652 | Loss: 1.2621\n",
      "Epoch 414/500 | Iter 1656 | Loss: 1.2614\n",
      "Epoch 415/500 | Iter 1660 | Loss: 1.2608\n",
      "Epoch 416/500 | Iter 1664 | Loss: 1.2602\n",
      "Epoch 417/500 | Iter 1668 | Loss: 1.2596\n",
      "Epoch 418/500 | Iter 1672 | Loss: 1.2590\n",
      "Epoch 419/500 | Iter 1676 | Loss: 1.2584\n",
      "Epoch 420/500 | Iter 1680 | Loss: 1.2578\n",
      "Epoch 421/500 | Iter 1684 | Loss: 1.2571\n",
      "Epoch 422/500 | Iter 1688 | Loss: 1.2565\n",
      "Epoch 423/500 | Iter 1692 | Loss: 1.2559\n",
      "Epoch 424/500 | Iter 1696 | Loss: 1.2553\n",
      "Epoch 425/500 | Iter 1700 | Loss: 1.2547\n",
      "Epoch 426/500 | Iter 1704 | Loss: 1.2541\n",
      "Epoch 427/500 | Iter 1708 | Loss: 1.2535\n",
      "Epoch 428/500 | Iter 1712 | Loss: 1.2529\n",
      "Epoch 429/500 | Iter 1716 | Loss: 1.2523\n",
      "Epoch 430/500 | Iter 1720 | Loss: 1.2517\n",
      "Epoch 431/500 | Iter 1724 | Loss: 1.2511\n",
      "Epoch 432/500 | Iter 1728 | Loss: 1.2505\n",
      "Epoch 433/500 | Iter 1732 | Loss: 1.2500\n",
      "Epoch 434/500 | Iter 1736 | Loss: 1.2494\n",
      "Epoch 435/500 | Iter 1740 | Loss: 1.2488\n",
      "Epoch 436/500 | Iter 1744 | Loss: 1.2482\n",
      "Epoch 437/500 | Iter 1748 | Loss: 1.2476\n",
      "Epoch 438/500 | Iter 1752 | Loss: 1.2470\n",
      "Epoch 439/500 | Iter 1756 | Loss: 1.2464\n",
      "Epoch 440/500 | Iter 1760 | Loss: 1.2458\n",
      "Epoch 441/500 | Iter 1764 | Loss: 1.2453\n",
      "Epoch 442/500 | Iter 1768 | Loss: 1.2447\n",
      "Epoch 443/500 | Iter 1772 | Loss: 1.2441\n",
      "Epoch 444/500 | Iter 1776 | Loss: 1.2435\n",
      "Epoch 445/500 | Iter 1780 | Loss: 1.2430\n",
      "Epoch 446/500 | Iter 1784 | Loss: 1.2424\n",
      "Epoch 447/500 | Iter 1788 | Loss: 1.2418\n",
      "Epoch 448/500 | Iter 1792 | Loss: 1.2412\n",
      "Epoch 449/500 | Iter 1796 | Loss: 1.2407\n",
      "Epoch 450/500 | Iter 1800 | Loss: 1.2401\n",
      "Epoch 451/500 | Iter 1804 | Loss: 1.2395\n",
      "Epoch 452/500 | Iter 1808 | Loss: 1.2390\n",
      "Epoch 453/500 | Iter 1812 | Loss: 1.2384\n",
      "Epoch 454/500 | Iter 1816 | Loss: 1.2378\n",
      "Epoch 455/500 | Iter 1820 | Loss: 1.2373\n",
      "Epoch 456/500 | Iter 1824 | Loss: 1.2367\n",
      "Epoch 457/500 | Iter 1828 | Loss: 1.2362\n",
      "Epoch 458/500 | Iter 1832 | Loss: 1.2356\n",
      "Epoch 459/500 | Iter 1836 | Loss: 1.2350\n",
      "Epoch 460/500 | Iter 1840 | Loss: 1.2345\n",
      "Epoch 461/500 | Iter 1844 | Loss: 1.2339\n",
      "Epoch 462/500 | Iter 1848 | Loss: 1.2334\n",
      "Epoch 463/500 | Iter 1852 | Loss: 1.2328\n",
      "Epoch 464/500 | Iter 1856 | Loss: 1.2323\n",
      "Epoch 465/500 | Iter 1860 | Loss: 1.2317\n",
      "Epoch 466/500 | Iter 1864 | Loss: 1.2312\n",
      "Epoch 467/500 | Iter 1868 | Loss: 1.2306\n",
      "Epoch 468/500 | Iter 1872 | Loss: 1.2301\n",
      "Epoch 469/500 | Iter 1876 | Loss: 1.2295\n",
      "Epoch 470/500 | Iter 1880 | Loss: 1.2290\n",
      "Epoch 471/500 | Iter 1884 | Loss: 1.2284\n",
      "Epoch 472/500 | Iter 1888 | Loss: 1.2279\n",
      "Epoch 473/500 | Iter 1892 | Loss: 1.2274\n",
      "Epoch 474/500 | Iter 1896 | Loss: 1.2268\n",
      "Epoch 475/500 | Iter 1900 | Loss: 1.2263\n",
      "Epoch 476/500 | Iter 1904 | Loss: 1.2258\n",
      "Epoch 477/500 | Iter 1908 | Loss: 1.2252\n",
      "Epoch 478/500 | Iter 1912 | Loss: 1.2247\n",
      "Epoch 479/500 | Iter 1916 | Loss: 1.2241\n",
      "Epoch 480/500 | Iter 1920 | Loss: 1.2236\n",
      "Epoch 481/500 | Iter 1924 | Loss: 1.2231\n",
      "Epoch 482/500 | Iter 1928 | Loss: 1.2226\n",
      "Epoch 483/500 | Iter 1932 | Loss: 1.2220\n",
      "Epoch 484/500 | Iter 1936 | Loss: 1.2215\n",
      "Epoch 485/500 | Iter 1940 | Loss: 1.2210\n",
      "Epoch 486/500 | Iter 1944 | Loss: 1.2205\n",
      "Epoch 487/500 | Iter 1948 | Loss: 1.2199\n",
      "Epoch 488/500 | Iter 1952 | Loss: 1.2194\n",
      "Epoch 489/500 | Iter 1956 | Loss: 1.2189\n",
      "Epoch 490/500 | Iter 1960 | Loss: 1.2184\n",
      "Epoch 491/500 | Iter 1964 | Loss: 1.2179\n",
      "Epoch 492/500 | Iter 1968 | Loss: 1.2173\n",
      "Epoch 493/500 | Iter 1972 | Loss: 1.2168\n",
      "Epoch 494/500 | Iter 1976 | Loss: 1.2163\n",
      "Epoch 495/500 | Iter 1980 | Loss: 1.2158\n",
      "Epoch 496/500 | Iter 1984 | Loss: 1.2153\n",
      "Epoch 497/500 | Iter 1988 | Loss: 1.2148\n",
      "Epoch 498/500 | Iter 1992 | Loss: 1.2143\n",
      "Epoch 499/500 | Iter 1996 | Loss: 1.2137\n",
      "Time taken: 0.60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/05 11:10:44 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/10/05 11:10:50 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BATCH DONE | Accuracy: 0.4707 | Macro F1: 0.4383\n",
      "🏃 View run batch_L2_reg at: https://mlflow.ml.brain.cs.ait.ac.th/#/experiments/930466277354100292/runs/8e8aa5dc196049d695b376f278a40ae7\n",
      "🧪 View experiment at: https://mlflow.ml.brain.cs.ait.ac.th/#/experiments/930466277354100292\n",
      "\n",
      "🚀 Running MINIBATCH Optimization...\n",
      "\n",
      "Epoch 0/500 | Iter 0 | Loss: 1.2669\n",
      "Epoch 1/500 | Iter 4 | Loss: 1.2736\n",
      "Epoch 2/500 | Iter 8 | Loss: 1.3219\n",
      "Epoch 3/500 | Iter 12 | Loss: 1.2901\n",
      "Epoch 4/500 | Iter 16 | Loss: 1.2686\n",
      "Epoch 5/500 | Iter 20 | Loss: 1.2719\n",
      "Epoch 6/500 | Iter 24 | Loss: 1.2756\n",
      "Epoch 7/500 | Iter 28 | Loss: 1.2644\n",
      "Epoch 8/500 | Iter 32 | Loss: 1.2946\n",
      "Epoch 9/500 | Iter 36 | Loss: 1.2682\n",
      "Epoch 10/500 | Iter 40 | Loss: 1.2650\n",
      "Epoch 11/500 | Iter 44 | Loss: 1.2800\n",
      "Epoch 12/500 | Iter 48 | Loss: 1.2788\n",
      "Epoch 13/500 | Iter 52 | Loss: 1.2573\n",
      "Epoch 14/500 | Iter 56 | Loss: 1.2603\n",
      "Epoch 15/500 | Iter 60 | Loss: 1.2633\n",
      "Epoch 16/500 | Iter 64 | Loss: 1.2590\n",
      "Epoch 17/500 | Iter 68 | Loss: 1.2836\n",
      "Epoch 18/500 | Iter 72 | Loss: 1.3265\n",
      "Epoch 19/500 | Iter 76 | Loss: 1.2592\n",
      "Epoch 20/500 | Iter 80 | Loss: 1.2828\n",
      "Epoch 21/500 | Iter 84 | Loss: 1.2555\n",
      "Epoch 22/500 | Iter 88 | Loss: 1.2612\n",
      "Epoch 23/500 | Iter 92 | Loss: 1.2752\n",
      "Epoch 24/500 | Iter 96 | Loss: 1.2565\n",
      "Epoch 25/500 | Iter 100 | Loss: 1.2648\n",
      "Epoch 26/500 | Iter 104 | Loss: 1.3168\n",
      "Epoch 27/500 | Iter 108 | Loss: 1.2796\n",
      "Epoch 28/500 | Iter 112 | Loss: 1.2627\n",
      "Epoch 29/500 | Iter 116 | Loss: 1.2525\n",
      "Epoch 30/500 | Iter 120 | Loss: 1.2539\n",
      "Epoch 31/500 | Iter 124 | Loss: 1.2682\n",
      "Epoch 32/500 | Iter 128 | Loss: 1.2818\n",
      "Epoch 33/500 | Iter 132 | Loss: 1.2508\n",
      "Epoch 34/500 | Iter 136 | Loss: 1.2557\n",
      "Epoch 35/500 | Iter 140 | Loss: 1.2520\n",
      "Epoch 36/500 | Iter 144 | Loss: 1.2499\n",
      "Epoch 37/500 | Iter 148 | Loss: 1.2676\n",
      "Epoch 38/500 | Iter 152 | Loss: 1.2621\n",
      "Epoch 39/500 | Iter 156 | Loss: 1.2637\n",
      "Epoch 40/500 | Iter 160 | Loss: 1.2879\n",
      "Epoch 41/500 | Iter 164 | Loss: 1.2657\n",
      "Epoch 42/500 | Iter 168 | Loss: 1.3236\n",
      "Epoch 43/500 | Iter 172 | Loss: 1.2589\n",
      "Epoch 44/500 | Iter 176 | Loss: 1.2622\n",
      "Epoch 45/500 | Iter 180 | Loss: 1.2467\n",
      "Epoch 46/500 | Iter 184 | Loss: 1.2462\n",
      "Epoch 47/500 | Iter 188 | Loss: 1.2490\n",
      "Epoch 48/500 | Iter 192 | Loss: 1.2962\n",
      "Epoch 49/500 | Iter 196 | Loss: 1.3753\n",
      "Epoch 50/500 | Iter 200 | Loss: 1.2540\n",
      "Epoch 51/500 | Iter 204 | Loss: 1.2348\n",
      "Epoch 52/500 | Iter 208 | Loss: 1.2797\n",
      "Epoch 53/500 | Iter 212 | Loss: 1.2414\n",
      "Epoch 54/500 | Iter 216 | Loss: 1.2411\n",
      "Epoch 55/500 | Iter 220 | Loss: 1.2606\n",
      "Epoch 56/500 | Iter 224 | Loss: 1.3038\n",
      "Epoch 57/500 | Iter 228 | Loss: 1.2433\n",
      "Epoch 58/500 | Iter 232 | Loss: 1.2390\n",
      "Epoch 59/500 | Iter 236 | Loss: 1.2479\n",
      "Epoch 60/500 | Iter 240 | Loss: 1.2812\n",
      "Epoch 61/500 | Iter 244 | Loss: 1.2368\n",
      "Epoch 62/500 | Iter 248 | Loss: 1.2407\n",
      "Epoch 63/500 | Iter 252 | Loss: 1.2448\n",
      "Epoch 64/500 | Iter 256 | Loss: 1.2280\n",
      "Epoch 65/500 | Iter 260 | Loss: 1.2367\n",
      "Epoch 66/500 | Iter 264 | Loss: 1.2425\n",
      "Epoch 67/500 | Iter 268 | Loss: 1.2989\n",
      "Epoch 68/500 | Iter 272 | Loss: 1.2529\n",
      "Epoch 69/500 | Iter 276 | Loss: 1.2342\n",
      "Epoch 70/500 | Iter 280 | Loss: 1.2325\n",
      "Epoch 71/500 | Iter 284 | Loss: 1.2357\n",
      "Epoch 72/500 | Iter 288 | Loss: 1.2296\n",
      "Epoch 73/500 | Iter 292 | Loss: 1.2281\n",
      "Epoch 74/500 | Iter 296 | Loss: 1.2296\n",
      "Epoch 75/500 | Iter 300 | Loss: 1.2886\n",
      "Epoch 76/500 | Iter 304 | Loss: 1.2360\n",
      "Epoch 77/500 | Iter 308 | Loss: 1.2411\n",
      "Epoch 78/500 | Iter 312 | Loss: 1.2260\n",
      "Epoch 79/500 | Iter 316 | Loss: 1.2272\n",
      "Epoch 80/500 | Iter 320 | Loss: 1.2247\n",
      "Epoch 81/500 | Iter 324 | Loss: 1.2238\n",
      "Epoch 82/500 | Iter 328 | Loss: 1.2440\n",
      "Epoch 83/500 | Iter 332 | Loss: 1.2711\n",
      "Epoch 84/500 | Iter 336 | Loss: 1.2247\n",
      "Epoch 85/500 | Iter 340 | Loss: 1.2208\n",
      "Epoch 86/500 | Iter 344 | Loss: 1.2311\n",
      "Epoch 87/500 | Iter 348 | Loss: 1.2189\n",
      "Epoch 88/500 | Iter 352 | Loss: 1.2250\n",
      "Epoch 89/500 | Iter 356 | Loss: 1.2218\n",
      "Epoch 90/500 | Iter 360 | Loss: 1.2909\n",
      "Epoch 91/500 | Iter 364 | Loss: 1.2392\n",
      "Epoch 92/500 | Iter 368 | Loss: 1.2198\n",
      "Epoch 93/500 | Iter 372 | Loss: 1.3065\n",
      "Epoch 94/500 | Iter 376 | Loss: 1.2340\n",
      "Epoch 95/500 | Iter 380 | Loss: 1.2137\n",
      "Epoch 96/500 | Iter 384 | Loss: 1.2158\n",
      "Epoch 97/500 | Iter 388 | Loss: 1.2178\n",
      "Epoch 98/500 | Iter 392 | Loss: 1.2286\n",
      "Epoch 99/500 | Iter 396 | Loss: 1.2676\n",
      "Epoch 100/500 | Iter 400 | Loss: 1.2139\n",
      "Epoch 101/500 | Iter 404 | Loss: 1.2297\n",
      "Epoch 102/500 | Iter 408 | Loss: 1.2181\n",
      "Epoch 103/500 | Iter 412 | Loss: 1.2078\n",
      "Epoch 104/500 | Iter 416 | Loss: 1.2249\n",
      "Epoch 105/500 | Iter 420 | Loss: 1.2143\n",
      "Epoch 106/500 | Iter 424 | Loss: 1.2240\n",
      "Epoch 107/500 | Iter 428 | Loss: 1.2352\n",
      "Epoch 108/500 | Iter 432 | Loss: 1.2205\n",
      "Epoch 109/500 | Iter 436 | Loss: 1.2114\n",
      "Epoch 110/500 | Iter 440 | Loss: 1.2189\n",
      "Epoch 111/500 | Iter 444 | Loss: 1.2287\n",
      "Epoch 112/500 | Iter 448 | Loss: 1.2155\n",
      "Epoch 113/500 | Iter 452 | Loss: 1.2279\n",
      "Epoch 114/500 | Iter 456 | Loss: 1.2074\n",
      "Epoch 115/500 | Iter 460 | Loss: 1.2069\n",
      "Epoch 116/500 | Iter 464 | Loss: 1.2076\n",
      "Epoch 117/500 | Iter 468 | Loss: 1.2054\n",
      "Epoch 118/500 | Iter 472 | Loss: 1.2103\n",
      "Epoch 119/500 | Iter 476 | Loss: 1.2200\n",
      "Epoch 120/500 | Iter 480 | Loss: 1.2181\n",
      "Epoch 121/500 | Iter 484 | Loss: 1.2017\n",
      "Epoch 122/500 | Iter 488 | Loss: 1.2003\n",
      "Epoch 123/500 | Iter 492 | Loss: 1.2243\n",
      "Epoch 124/500 | Iter 496 | Loss: 1.2167\n",
      "Epoch 125/500 | Iter 500 | Loss: 1.2208\n",
      "Epoch 126/500 | Iter 504 | Loss: 1.2108\n",
      "Epoch 127/500 | Iter 508 | Loss: 1.2037\n",
      "Epoch 128/500 | Iter 512 | Loss: 1.2051\n",
      "Epoch 129/500 | Iter 516 | Loss: 1.2176\n",
      "Epoch 130/500 | Iter 520 | Loss: 1.2217\n",
      "Epoch 131/500 | Iter 524 | Loss: 1.1991\n",
      "Epoch 132/500 | Iter 528 | Loss: 1.1976\n",
      "Epoch 133/500 | Iter 532 | Loss: 1.1935\n",
      "Epoch 134/500 | Iter 536 | Loss: 1.1999\n",
      "Epoch 135/500 | Iter 540 | Loss: 1.2163\n",
      "Epoch 136/500 | Iter 544 | Loss: 1.1932\n",
      "Epoch 137/500 | Iter 548 | Loss: 1.2226\n",
      "Epoch 138/500 | Iter 552 | Loss: 1.2047\n",
      "Epoch 139/500 | Iter 556 | Loss: 1.1902\n",
      "Epoch 140/500 | Iter 560 | Loss: 1.1936\n",
      "Epoch 141/500 | Iter 564 | Loss: 1.2214\n",
      "Epoch 142/500 | Iter 568 | Loss: 1.1936\n",
      "Epoch 143/500 | Iter 572 | Loss: 1.2371\n",
      "Epoch 144/500 | Iter 576 | Loss: 1.1967\n",
      "Epoch 145/500 | Iter 580 | Loss: 1.2059\n",
      "Epoch 146/500 | Iter 584 | Loss: 1.1908\n",
      "Epoch 147/500 | Iter 588 | Loss: 1.2042\n",
      "Epoch 148/500 | Iter 592 | Loss: 1.1971\n",
      "Epoch 149/500 | Iter 596 | Loss: 1.1981\n",
      "Epoch 150/500 | Iter 600 | Loss: 1.1883\n",
      "Epoch 151/500 | Iter 604 | Loss: 1.2129\n",
      "Epoch 152/500 | Iter 608 | Loss: 1.2558\n",
      "Epoch 153/500 | Iter 612 | Loss: 1.1903\n",
      "Epoch 154/500 | Iter 616 | Loss: 1.2380\n",
      "Epoch 155/500 | Iter 620 | Loss: 1.2072\n",
      "Epoch 156/500 | Iter 624 | Loss: 1.2047\n",
      "Epoch 157/500 | Iter 628 | Loss: 1.2059\n",
      "Epoch 158/500 | Iter 632 | Loss: 1.2271\n",
      "Epoch 159/500 | Iter 636 | Loss: 1.1960\n",
      "Epoch 160/500 | Iter 640 | Loss: 1.1846\n",
      "Epoch 161/500 | Iter 644 | Loss: 1.2419\n",
      "Epoch 162/500 | Iter 648 | Loss: 1.1801\n",
      "Epoch 163/500 | Iter 652 | Loss: 1.2048\n",
      "Epoch 164/500 | Iter 656 | Loss: 1.1984\n",
      "Epoch 165/500 | Iter 660 | Loss: 1.2052\n",
      "Epoch 166/500 | Iter 664 | Loss: 1.1950\n",
      "Epoch 167/500 | Iter 668 | Loss: 1.2058\n",
      "Epoch 168/500 | Iter 672 | Loss: 1.1807\n",
      "Epoch 169/500 | Iter 676 | Loss: 1.2012\n",
      "Epoch 170/500 | Iter 680 | Loss: 1.2304\n",
      "Epoch 171/500 | Iter 684 | Loss: 1.1864\n",
      "Epoch 172/500 | Iter 688 | Loss: 1.1898\n",
      "Epoch 173/500 | Iter 692 | Loss: 1.1819\n",
      "Epoch 174/500 | Iter 696 | Loss: 1.2688\n",
      "Epoch 175/500 | Iter 700 | Loss: 1.1911\n",
      "Epoch 176/500 | Iter 704 | Loss: 1.1765\n",
      "Epoch 177/500 | Iter 708 | Loss: 1.2391\n",
      "Epoch 178/500 | Iter 712 | Loss: 1.1727\n",
      "Epoch 179/500 | Iter 716 | Loss: 1.2024\n",
      "Epoch 180/500 | Iter 720 | Loss: 1.1959\n",
      "Epoch 181/500 | Iter 724 | Loss: 1.1958\n",
      "Epoch 182/500 | Iter 728 | Loss: 1.1980\n",
      "Epoch 183/500 | Iter 732 | Loss: 1.1938\n",
      "Epoch 184/500 | Iter 736 | Loss: 1.1941\n",
      "Epoch 185/500 | Iter 740 | Loss: 1.2006\n",
      "Epoch 186/500 | Iter 744 | Loss: 1.1803\n",
      "Epoch 187/500 | Iter 748 | Loss: 1.1783\n",
      "Epoch 188/500 | Iter 752 | Loss: 1.1840\n",
      "Epoch 189/500 | Iter 756 | Loss: 1.1684\n",
      "Epoch 190/500 | Iter 760 | Loss: 1.1922\n",
      "Epoch 191/500 | Iter 764 | Loss: 1.1907\n",
      "Epoch 192/500 | Iter 768 | Loss: 1.1847\n",
      "Epoch 193/500 | Iter 772 | Loss: 1.1697\n",
      "Epoch 194/500 | Iter 776 | Loss: 1.2117\n",
      "Epoch 195/500 | Iter 780 | Loss: 1.1705\n",
      "Epoch 196/500 | Iter 784 | Loss: 1.1890\n",
      "Epoch 197/500 | Iter 788 | Loss: 1.1654\n",
      "Epoch 198/500 | Iter 792 | Loss: 1.1793\n",
      "Epoch 199/500 | Iter 796 | Loss: 1.1754\n",
      "Epoch 200/500 | Iter 800 | Loss: 1.1729\n",
      "Epoch 201/500 | Iter 804 | Loss: 1.1678\n",
      "Epoch 202/500 | Iter 808 | Loss: 1.1883\n",
      "Epoch 203/500 | Iter 812 | Loss: 1.1790\n",
      "Epoch 204/500 | Iter 816 | Loss: 1.1657\n",
      "Epoch 205/500 | Iter 820 | Loss: 1.1712\n",
      "Epoch 206/500 | Iter 824 | Loss: 1.2038\n",
      "Epoch 207/500 | Iter 828 | Loss: 1.1817\n",
      "Epoch 208/500 | Iter 832 | Loss: 1.1706\n",
      "Epoch 209/500 | Iter 836 | Loss: 1.1842\n",
      "Epoch 210/500 | Iter 840 | Loss: 1.1623\n",
      "Epoch 211/500 | Iter 844 | Loss: 1.1638\n",
      "Epoch 212/500 | Iter 848 | Loss: 1.1723\n",
      "Epoch 213/500 | Iter 852 | Loss: 1.1698\n",
      "Epoch 214/500 | Iter 856 | Loss: 1.1784\n",
      "Epoch 215/500 | Iter 860 | Loss: 1.1777\n",
      "Epoch 216/500 | Iter 864 | Loss: 1.1642\n",
      "Epoch 217/500 | Iter 868 | Loss: 1.1612\n",
      "Epoch 218/500 | Iter 872 | Loss: 1.2013\n",
      "Epoch 219/500 | Iter 876 | Loss: 1.2250\n",
      "Epoch 220/500 | Iter 880 | Loss: 1.1777\n",
      "Epoch 221/500 | Iter 884 | Loss: 1.1710\n",
      "Epoch 222/500 | Iter 888 | Loss: 1.2150\n",
      "Epoch 223/500 | Iter 892 | Loss: 1.1667\n",
      "Epoch 224/500 | Iter 896 | Loss: 1.1609\n",
      "Epoch 225/500 | Iter 900 | Loss: 1.1681\n",
      "Epoch 226/500 | Iter 904 | Loss: 1.1562\n",
      "Epoch 227/500 | Iter 908 | Loss: 1.1611\n",
      "Epoch 228/500 | Iter 912 | Loss: 1.1787\n",
      "Epoch 229/500 | Iter 916 | Loss: 1.1583\n",
      "Epoch 230/500 | Iter 920 | Loss: 1.1577\n",
      "Epoch 231/500 | Iter 924 | Loss: 1.1636\n",
      "Epoch 232/500 | Iter 928 | Loss: 1.2019\n",
      "Epoch 233/500 | Iter 932 | Loss: 1.1923\n",
      "Epoch 234/500 | Iter 936 | Loss: 1.1581\n",
      "Epoch 235/500 | Iter 940 | Loss: 1.1577\n",
      "Epoch 236/500 | Iter 944 | Loss: 1.1498\n",
      "Epoch 237/500 | Iter 948 | Loss: 1.1525\n",
      "Epoch 238/500 | Iter 952 | Loss: 1.1661\n",
      "Epoch 239/500 | Iter 956 | Loss: 1.2137\n",
      "Epoch 240/500 | Iter 960 | Loss: 1.1718\n",
      "Epoch 241/500 | Iter 964 | Loss: 1.1896\n",
      "Epoch 242/500 | Iter 968 | Loss: 1.1690\n",
      "Epoch 243/500 | Iter 972 | Loss: 1.1618\n",
      "Epoch 244/500 | Iter 976 | Loss: 1.1587\n",
      "Epoch 245/500 | Iter 980 | Loss: 1.1858\n",
      "Epoch 246/500 | Iter 984 | Loss: 1.1715\n",
      "Epoch 247/500 | Iter 988 | Loss: 1.1731\n",
      "Epoch 248/500 | Iter 992 | Loss: 1.1715\n",
      "Epoch 249/500 | Iter 996 | Loss: 1.1653\n",
      "Epoch 250/500 | Iter 1000 | Loss: 1.1688\n",
      "Epoch 251/500 | Iter 1004 | Loss: 1.1499\n",
      "Epoch 252/500 | Iter 1008 | Loss: 1.1473\n",
      "Epoch 253/500 | Iter 1012 | Loss: 1.1518\n",
      "Epoch 254/500 | Iter 1016 | Loss: 1.1518\n",
      "Epoch 255/500 | Iter 1020 | Loss: 1.1502\n",
      "Epoch 256/500 | Iter 1024 | Loss: 1.1416\n",
      "Epoch 257/500 | Iter 1028 | Loss: 1.1675\n",
      "Epoch 258/500 | Iter 1032 | Loss: 1.1975\n",
      "Epoch 259/500 | Iter 1036 | Loss: 1.1539\n",
      "Epoch 260/500 | Iter 1040 | Loss: 1.1923\n",
      "Epoch 261/500 | Iter 1044 | Loss: 1.1454\n",
      "Epoch 262/500 | Iter 1048 | Loss: 1.1488\n",
      "Epoch 263/500 | Iter 1052 | Loss: 1.1472\n",
      "Epoch 264/500 | Iter 1056 | Loss: 1.1792\n",
      "Epoch 265/500 | Iter 1060 | Loss: 1.1427\n",
      "Epoch 266/500 | Iter 1064 | Loss: 1.1608\n",
      "Epoch 267/500 | Iter 1068 | Loss: 1.1433\n",
      "Epoch 268/500 | Iter 1072 | Loss: 1.1580\n",
      "Epoch 269/500 | Iter 1076 | Loss: 1.1680\n",
      "Epoch 270/500 | Iter 1080 | Loss: 1.1491\n",
      "Epoch 271/500 | Iter 1084 | Loss: 1.1599\n",
      "Epoch 272/500 | Iter 1088 | Loss: 1.1433\n",
      "Epoch 273/500 | Iter 1092 | Loss: 1.1437\n",
      "Epoch 274/500 | Iter 1096 | Loss: 1.1576\n",
      "Epoch 275/500 | Iter 1100 | Loss: 1.1766\n",
      "Epoch 276/500 | Iter 1104 | Loss: 1.1395\n",
      "Epoch 277/500 | Iter 1108 | Loss: 1.1565\n",
      "Epoch 278/500 | Iter 1112 | Loss: 1.1336\n",
      "Epoch 279/500 | Iter 1116 | Loss: 1.1616\n",
      "Epoch 280/500 | Iter 1120 | Loss: 1.1382\n",
      "Epoch 281/500 | Iter 1124 | Loss: 1.1420\n",
      "Epoch 282/500 | Iter 1128 | Loss: 1.1538\n",
      "Epoch 283/500 | Iter 1132 | Loss: 1.1378\n",
      "Epoch 284/500 | Iter 1136 | Loss: 1.1461\n",
      "Epoch 285/500 | Iter 1140 | Loss: 1.1540\n",
      "Epoch 286/500 | Iter 1144 | Loss: 1.1386\n",
      "Epoch 287/500 | Iter 1148 | Loss: 1.1307\n",
      "Epoch 288/500 | Iter 1152 | Loss: 1.1358\n",
      "Epoch 289/500 | Iter 1156 | Loss: 1.1379\n",
      "Epoch 290/500 | Iter 1160 | Loss: 1.1569\n",
      "Epoch 291/500 | Iter 1164 | Loss: 1.1534\n",
      "Epoch 292/500 | Iter 1168 | Loss: 1.1535\n",
      "Epoch 293/500 | Iter 1172 | Loss: 1.1489\n",
      "Epoch 294/500 | Iter 1176 | Loss: 1.1347\n",
      "Epoch 295/500 | Iter 1180 | Loss: 1.2177\n",
      "Epoch 296/500 | Iter 1184 | Loss: 1.1294\n",
      "Epoch 297/500 | Iter 1188 | Loss: 1.1329\n",
      "Epoch 298/500 | Iter 1192 | Loss: 1.1337\n",
      "Epoch 299/500 | Iter 1196 | Loss: 1.1332\n",
      "Epoch 300/500 | Iter 1200 | Loss: 1.1298\n",
      "Epoch 301/500 | Iter 1204 | Loss: 1.1321\n",
      "Epoch 302/500 | Iter 1208 | Loss: 1.1551\n",
      "Epoch 303/500 | Iter 1212 | Loss: 1.1494\n",
      "Epoch 304/500 | Iter 1216 | Loss: 1.1372\n",
      "Epoch 305/500 | Iter 1220 | Loss: 1.1362\n",
      "Epoch 306/500 | Iter 1224 | Loss: 1.1312\n",
      "Epoch 307/500 | Iter 1228 | Loss: 1.1540\n",
      "Epoch 308/500 | Iter 1232 | Loss: 1.1698\n",
      "Epoch 309/500 | Iter 1236 | Loss: 1.2158\n",
      "Epoch 310/500 | Iter 1240 | Loss: 1.1491\n",
      "Epoch 311/500 | Iter 1244 | Loss: 1.1793\n",
      "Epoch 312/500 | Iter 1248 | Loss: 1.1436\n",
      "Epoch 313/500 | Iter 1252 | Loss: 1.1319\n",
      "Epoch 314/500 | Iter 1256 | Loss: 1.1396\n",
      "Epoch 315/500 | Iter 1260 | Loss: 1.1274\n",
      "Epoch 316/500 | Iter 1264 | Loss: 1.1662\n",
      "Epoch 317/500 | Iter 1268 | Loss: 1.1205\n",
      "Epoch 318/500 | Iter 1272 | Loss: 1.1253\n",
      "Epoch 319/500 | Iter 1276 | Loss: 1.1403\n",
      "Epoch 320/500 | Iter 1280 | Loss: 1.1413\n",
      "Epoch 321/500 | Iter 1284 | Loss: 1.1292\n",
      "Epoch 322/500 | Iter 1288 | Loss: 1.1482\n",
      "Epoch 323/500 | Iter 1292 | Loss: 1.1395\n",
      "Epoch 324/500 | Iter 1296 | Loss: 1.1262\n",
      "Epoch 325/500 | Iter 1300 | Loss: 1.1339\n",
      "Epoch 326/500 | Iter 1304 | Loss: 1.1726\n",
      "Epoch 327/500 | Iter 1308 | Loss: 1.1209\n",
      "Epoch 328/500 | Iter 1312 | Loss: 1.1707\n",
      "Epoch 329/500 | Iter 1316 | Loss: 1.1429\n",
      "Epoch 330/500 | Iter 1320 | Loss: 1.1660\n",
      "Epoch 331/500 | Iter 1324 | Loss: 1.1276\n",
      "Epoch 332/500 | Iter 1328 | Loss: 1.1254\n",
      "Epoch 333/500 | Iter 1332 | Loss: 1.1197\n",
      "Epoch 334/500 | Iter 1336 | Loss: 1.1165\n",
      "Epoch 335/500 | Iter 1340 | Loss: 1.1718\n",
      "Epoch 336/500 | Iter 1344 | Loss: 1.1193\n",
      "Epoch 337/500 | Iter 1348 | Loss: 1.1489\n",
      "Epoch 338/500 | Iter 1352 | Loss: 1.1687\n",
      "Epoch 339/500 | Iter 1356 | Loss: 1.1442\n",
      "Epoch 340/500 | Iter 1360 | Loss: 1.1364\n",
      "Epoch 341/500 | Iter 1364 | Loss: 1.1203\n",
      "Epoch 342/500 | Iter 1368 | Loss: 1.1205\n",
      "Epoch 343/500 | Iter 1372 | Loss: 1.1363\n",
      "Epoch 344/500 | Iter 1376 | Loss: 1.1311\n",
      "Epoch 345/500 | Iter 1380 | Loss: 1.1443\n",
      "Epoch 346/500 | Iter 1384 | Loss: 1.1367\n",
      "Epoch 347/500 | Iter 1388 | Loss: 1.1308\n",
      "Epoch 348/500 | Iter 1392 | Loss: 1.1163\n",
      "Epoch 349/500 | Iter 1396 | Loss: 1.1206\n",
      "Epoch 350/500 | Iter 1400 | Loss: 1.1521\n",
      "Epoch 351/500 | Iter 1404 | Loss: 1.1358\n",
      "Epoch 352/500 | Iter 1408 | Loss: 1.1335\n",
      "Epoch 353/500 | Iter 1412 | Loss: 1.1349\n",
      "Epoch 354/500 | Iter 1416 | Loss: 1.1310\n",
      "Epoch 355/500 | Iter 1420 | Loss: 1.1219\n",
      "Epoch 356/500 | Iter 1424 | Loss: 1.1174\n",
      "Epoch 357/500 | Iter 1428 | Loss: 1.1163\n",
      "Epoch 358/500 | Iter 1432 | Loss: 1.1139\n",
      "Epoch 359/500 | Iter 1436 | Loss: 1.1161\n",
      "Epoch 360/500 | Iter 1440 | Loss: 1.1470\n",
      "Epoch 361/500 | Iter 1444 | Loss: 1.1863\n",
      "Epoch 362/500 | Iter 1448 | Loss: 1.1134\n",
      "Epoch 363/500 | Iter 1452 | Loss: 1.1287\n",
      "Epoch 364/500 | Iter 1456 | Loss: 1.1345\n",
      "Epoch 365/500 | Iter 1460 | Loss: 1.1105\n",
      "Epoch 366/500 | Iter 1464 | Loss: 1.1212\n",
      "Epoch 367/500 | Iter 1468 | Loss: 1.1188\n",
      "Epoch 368/500 | Iter 1472 | Loss: 1.1081\n",
      "Epoch 369/500 | Iter 1476 | Loss: 1.1109\n",
      "Epoch 370/500 | Iter 1480 | Loss: 1.1102\n",
      "Epoch 371/500 | Iter 1484 | Loss: 1.1095\n",
      "Epoch 372/500 | Iter 1488 | Loss: 1.1105\n",
      "Epoch 373/500 | Iter 1492 | Loss: 1.1065\n",
      "Epoch 374/500 | Iter 1496 | Loss: 1.1081\n",
      "Epoch 375/500 | Iter 1500 | Loss: 1.1063\n",
      "Epoch 376/500 | Iter 1504 | Loss: 1.1051\n",
      "Epoch 377/500 | Iter 1508 | Loss: 1.1104\n",
      "Epoch 378/500 | Iter 1512 | Loss: 1.1119\n",
      "Epoch 379/500 | Iter 1516 | Loss: 1.1205\n",
      "Epoch 380/500 | Iter 1520 | Loss: 1.1423\n",
      "Epoch 381/500 | Iter 1524 | Loss: 1.1061\n",
      "Epoch 382/500 | Iter 1528 | Loss: 1.1252\n",
      "Epoch 383/500 | Iter 1532 | Loss: 1.1300\n",
      "Epoch 384/500 | Iter 1536 | Loss: 1.1001\n",
      "Epoch 385/500 | Iter 1540 | Loss: 1.1264\n",
      "Epoch 386/500 | Iter 1544 | Loss: 1.1036\n",
      "Epoch 387/500 | Iter 1548 | Loss: 1.1197\n",
      "Epoch 388/500 | Iter 1552 | Loss: 1.1271\n",
      "Epoch 389/500 | Iter 1556 | Loss: 1.1024\n",
      "Epoch 390/500 | Iter 1560 | Loss: 1.1425\n",
      "Epoch 391/500 | Iter 1564 | Loss: 1.1189\n",
      "Epoch 392/500 | Iter 1568 | Loss: 1.1254\n",
      "Epoch 393/500 | Iter 1572 | Loss: 1.1069\n",
      "Epoch 394/500 | Iter 1576 | Loss: 1.1055\n",
      "Epoch 395/500 | Iter 1580 | Loss: 1.1018\n",
      "Epoch 396/500 | Iter 1584 | Loss: 1.1456\n",
      "Epoch 397/500 | Iter 1588 | Loss: 1.1058\n",
      "Epoch 398/500 | Iter 1592 | Loss: 1.1147\n",
      "Epoch 399/500 | Iter 1596 | Loss: 1.1075\n",
      "Epoch 400/500 | Iter 1600 | Loss: 1.1428\n",
      "Epoch 401/500 | Iter 1604 | Loss: 1.0978\n",
      "Epoch 402/500 | Iter 1608 | Loss: 1.1236\n",
      "Epoch 403/500 | Iter 1612 | Loss: 1.1034\n",
      "Epoch 404/500 | Iter 1616 | Loss: 1.1102\n",
      "Epoch 405/500 | Iter 1620 | Loss: 1.1361\n",
      "Epoch 406/500 | Iter 1624 | Loss: 1.1411\n",
      "Epoch 407/500 | Iter 1628 | Loss: 1.0984\n",
      "Epoch 408/500 | Iter 1632 | Loss: 1.1198\n",
      "Epoch 409/500 | Iter 1636 | Loss: 1.1076\n",
      "Epoch 410/500 | Iter 1640 | Loss: 1.1219\n",
      "Epoch 411/500 | Iter 1644 | Loss: 1.1014\n",
      "Epoch 412/500 | Iter 1648 | Loss: 1.0964\n",
      "Epoch 413/500 | Iter 1652 | Loss: 1.0973\n",
      "Epoch 414/500 | Iter 1656 | Loss: 1.1224\n",
      "Epoch 415/500 | Iter 1660 | Loss: 1.0952\n",
      "Epoch 416/500 | Iter 1664 | Loss: 1.1039\n",
      "Epoch 417/500 | Iter 1668 | Loss: 1.1050\n",
      "Epoch 418/500 | Iter 1672 | Loss: 1.1358\n",
      "Epoch 419/500 | Iter 1676 | Loss: 1.1152\n",
      "Epoch 420/500 | Iter 1680 | Loss: 1.0949\n",
      "Epoch 421/500 | Iter 1684 | Loss: 1.0987\n",
      "Epoch 422/500 | Iter 1688 | Loss: 1.0938\n",
      "Epoch 423/500 | Iter 1692 | Loss: 1.1357\n",
      "Epoch 424/500 | Iter 1696 | Loss: 1.1101\n",
      "Epoch 425/500 | Iter 1700 | Loss: 1.0966\n",
      "Epoch 426/500 | Iter 1704 | Loss: 1.1157\n",
      "Epoch 427/500 | Iter 1708 | Loss: 1.1018\n",
      "Epoch 428/500 | Iter 1712 | Loss: 1.1084\n",
      "Epoch 429/500 | Iter 1716 | Loss: 1.0933\n",
      "Epoch 430/500 | Iter 1720 | Loss: 1.1132\n",
      "Epoch 431/500 | Iter 1724 | Loss: 1.0987\n",
      "Epoch 432/500 | Iter 1728 | Loss: 1.1312\n",
      "Epoch 433/500 | Iter 1732 | Loss: 1.1189\n",
      "Epoch 434/500 | Iter 1736 | Loss: 1.1004\n",
      "Epoch 435/500 | Iter 1740 | Loss: 1.0914\n",
      "Epoch 436/500 | Iter 1744 | Loss: 1.0894\n",
      "Epoch 437/500 | Iter 1748 | Loss: 1.0959\n",
      "Epoch 438/500 | Iter 1752 | Loss: 1.1096\n",
      "Epoch 439/500 | Iter 1756 | Loss: 1.0938\n",
      "Epoch 440/500 | Iter 1760 | Loss: 1.1067\n",
      "Epoch 441/500 | Iter 1764 | Loss: 1.1304\n",
      "Epoch 442/500 | Iter 1768 | Loss: 1.1098\n",
      "Epoch 443/500 | Iter 1772 | Loss: 1.0900\n",
      "Epoch 444/500 | Iter 1776 | Loss: 1.0933\n",
      "Epoch 445/500 | Iter 1780 | Loss: 1.1239\n",
      "Epoch 446/500 | Iter 1784 | Loss: 1.0934\n",
      "Epoch 447/500 | Iter 1788 | Loss: 1.1152\n",
      "Epoch 448/500 | Iter 1792 | Loss: 1.1256\n",
      "Epoch 449/500 | Iter 1796 | Loss: 1.0841\n",
      "Epoch 450/500 | Iter 1800 | Loss: 1.1296\n",
      "Epoch 451/500 | Iter 1804 | Loss: 1.0887\n",
      "Epoch 452/500 | Iter 1808 | Loss: 1.0987\n",
      "Epoch 453/500 | Iter 1812 | Loss: 1.0810\n",
      "Epoch 454/500 | Iter 1816 | Loss: 1.0928\n",
      "Epoch 455/500 | Iter 1820 | Loss: 1.0851\n",
      "Epoch 456/500 | Iter 1824 | Loss: 1.0985\n",
      "Epoch 457/500 | Iter 1828 | Loss: 1.1388\n",
      "Epoch 458/500 | Iter 1832 | Loss: 1.1099\n",
      "Epoch 459/500 | Iter 1836 | Loss: 1.0882\n",
      "Epoch 460/500 | Iter 1840 | Loss: 1.1062\n",
      "Epoch 461/500 | Iter 1844 | Loss: 1.1051\n",
      "Epoch 462/500 | Iter 1848 | Loss: 1.0903\n",
      "Epoch 463/500 | Iter 1852 | Loss: 1.0948\n",
      "Epoch 464/500 | Iter 1856 | Loss: 1.0897\n",
      "Epoch 465/500 | Iter 1860 | Loss: 1.0994\n",
      "Epoch 466/500 | Iter 1864 | Loss: 1.1742\n",
      "Epoch 467/500 | Iter 1868 | Loss: 1.1187\n",
      "Epoch 468/500 | Iter 1872 | Loss: 1.0879\n",
      "Epoch 469/500 | Iter 1876 | Loss: 1.0820\n",
      "Epoch 470/500 | Iter 1880 | Loss: 1.1223\n",
      "Epoch 471/500 | Iter 1884 | Loss: 1.0864\n",
      "Epoch 472/500 | Iter 1888 | Loss: 1.0846\n",
      "Epoch 473/500 | Iter 1892 | Loss: 1.0800\n",
      "Epoch 474/500 | Iter 1896 | Loss: 1.1300\n",
      "Epoch 475/500 | Iter 1900 | Loss: 1.0958\n",
      "Epoch 476/500 | Iter 1904 | Loss: 1.1110\n",
      "Epoch 477/500 | Iter 1908 | Loss: 1.1311\n",
      "Epoch 478/500 | Iter 1912 | Loss: 1.0752\n",
      "Epoch 479/500 | Iter 1916 | Loss: 1.0880\n",
      "Epoch 480/500 | Iter 1920 | Loss: 1.0960\n",
      "Epoch 481/500 | Iter 1924 | Loss: 1.0750\n",
      "Epoch 482/500 | Iter 1928 | Loss: 1.0805\n",
      "Epoch 483/500 | Iter 1932 | Loss: 1.1147\n",
      "Epoch 484/500 | Iter 1936 | Loss: 1.0798\n",
      "Epoch 485/500 | Iter 1940 | Loss: 1.0779\n",
      "Epoch 486/500 | Iter 1944 | Loss: 1.0804\n",
      "Epoch 487/500 | Iter 1948 | Loss: 1.0812\n",
      "Epoch 488/500 | Iter 1952 | Loss: 1.0773\n",
      "Epoch 489/500 | Iter 1956 | Loss: 1.1254\n",
      "Epoch 490/500 | Iter 1960 | Loss: 1.1045\n",
      "Epoch 491/500 | Iter 1964 | Loss: 1.1001\n",
      "Epoch 492/500 | Iter 1968 | Loss: 1.0983\n",
      "Epoch 493/500 | Iter 1972 | Loss: 1.0877\n",
      "Epoch 494/500 | Iter 1976 | Loss: 1.0775\n",
      "Epoch 495/500 | Iter 1980 | Loss: 1.0758\n",
      "Epoch 496/500 | Iter 1984 | Loss: 1.0956\n",
      "Epoch 497/500 | Iter 1988 | Loss: 1.0776\n",
      "Epoch 498/500 | Iter 1992 | Loss: 1.0894\n",
      "Epoch 499/500 | Iter 1996 | Loss: 1.0831\n",
      "Time taken: 0.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/05 11:11:00 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/10/05 11:11:06 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MINIBATCH DONE | Accuracy: 0.5152 | Macro F1: 0.4806\n",
      "🏃 View run minibatch_L2_reg at: https://mlflow.ml.brain.cs.ait.ac.th/#/experiments/930466277354100292/runs/a084ea94c082449e8bd5c4eca2e26550\n",
      "🧪 View experiment at: https://mlflow.ml.brain.cs.ait.ac.th/#/experiments/930466277354100292\n",
      "\n",
      "🚀 Running STO Optimization...\n",
      "\n",
      "Epoch 0/500 | Iter 0 | Loss: 1.7261\n",
      "Epoch 1/500 | Iter 4 | Loss: 1.8655\n",
      "Epoch 2/500 | Iter 8 | Loss: 1.9884\n",
      "Epoch 3/500 | Iter 12 | Loss: 2.6462\n",
      "Epoch 4/500 | Iter 16 | Loss: 1.9592\n",
      "Epoch 5/500 | Iter 20 | Loss: 1.5263\n",
      "Epoch 6/500 | Iter 24 | Loss: 2.5097\n",
      "Epoch 7/500 | Iter 28 | Loss: 1.6009\n",
      "Epoch 8/500 | Iter 32 | Loss: 2.0320\n",
      "Epoch 9/500 | Iter 36 | Loss: 1.1813\n",
      "Epoch 10/500 | Iter 40 | Loss: 1.3907\n",
      "Epoch 11/500 | Iter 44 | Loss: 1.4577\n",
      "Epoch 12/500 | Iter 48 | Loss: 2.0690\n",
      "Epoch 13/500 | Iter 52 | Loss: 1.5618\n",
      "Epoch 14/500 | Iter 56 | Loss: 2.5515\n",
      "Epoch 15/500 | Iter 60 | Loss: 3.0700\n",
      "Epoch 16/500 | Iter 64 | Loss: 1.3791\n",
      "Epoch 17/500 | Iter 68 | Loss: 1.9269\n",
      "Epoch 18/500 | Iter 72 | Loss: 1.3277\n",
      "Epoch 19/500 | Iter 76 | Loss: 1.9222\n",
      "Epoch 20/500 | Iter 80 | Loss: 1.5835\n",
      "Epoch 21/500 | Iter 84 | Loss: 1.1874\n",
      "Epoch 22/500 | Iter 88 | Loss: 1.6640\n",
      "Epoch 23/500 | Iter 92 | Loss: 2.5916\n",
      "Epoch 24/500 | Iter 96 | Loss: 2.0527\n",
      "Epoch 25/500 | Iter 100 | Loss: 3.0298\n",
      "Epoch 26/500 | Iter 104 | Loss: 2.5061\n",
      "Epoch 27/500 | Iter 108 | Loss: 1.7027\n",
      "Epoch 28/500 | Iter 112 | Loss: 2.4276\n",
      "Epoch 29/500 | Iter 116 | Loss: 2.8088\n",
      "Epoch 30/500 | Iter 120 | Loss: 1.7461\n",
      "Epoch 31/500 | Iter 124 | Loss: 1.7264\n",
      "Epoch 32/500 | Iter 128 | Loss: 2.0195\n",
      "Epoch 33/500 | Iter 132 | Loss: 2.3932\n",
      "Epoch 34/500 | Iter 136 | Loss: 1.3756\n",
      "Epoch 35/500 | Iter 140 | Loss: 1.9125\n",
      "Epoch 36/500 | Iter 144 | Loss: 1.7440\n",
      "Epoch 37/500 | Iter 148 | Loss: 1.2349\n",
      "Epoch 38/500 | Iter 152 | Loss: 1.8004\n",
      "Epoch 39/500 | Iter 156 | Loss: 1.9223\n",
      "Epoch 40/500 | Iter 160 | Loss: 1.8844\n",
      "Epoch 41/500 | Iter 164 | Loss: 1.2449\n",
      "Epoch 42/500 | Iter 168 | Loss: 1.9193\n",
      "Epoch 43/500 | Iter 172 | Loss: 1.1366\n",
      "Epoch 44/500 | Iter 176 | Loss: 1.5963\n",
      "Epoch 45/500 | Iter 180 | Loss: 1.6006\n",
      "Epoch 46/500 | Iter 184 | Loss: 1.3149\n",
      "Epoch 47/500 | Iter 188 | Loss: 1.3915\n",
      "Epoch 48/500 | Iter 192 | Loss: 1.8469\n",
      "Epoch 49/500 | Iter 196 | Loss: 1.5765\n",
      "Epoch 50/500 | Iter 200 | Loss: 1.5475\n",
      "Epoch 51/500 | Iter 204 | Loss: 0.7068\n",
      "Epoch 52/500 | Iter 208 | Loss: 1.7183\n",
      "Epoch 53/500 | Iter 212 | Loss: 2.4029\n",
      "Epoch 54/500 | Iter 216 | Loss: 1.1397\n",
      "Epoch 55/500 | Iter 220 | Loss: 1.9210\n",
      "Epoch 56/500 | Iter 224 | Loss: 2.5041\n",
      "Epoch 57/500 | Iter 228 | Loss: 1.9779\n",
      "Epoch 58/500 | Iter 232 | Loss: 2.9896\n",
      "Epoch 59/500 | Iter 236 | Loss: 1.2553\n",
      "Epoch 60/500 | Iter 240 | Loss: 2.4309\n",
      "Epoch 61/500 | Iter 244 | Loss: 1.4347\n",
      "Epoch 62/500 | Iter 248 | Loss: 2.1349\n",
      "Epoch 63/500 | Iter 252 | Loss: 2.4039\n",
      "Epoch 64/500 | Iter 256 | Loss: 1.0329\n",
      "Epoch 65/500 | Iter 260 | Loss: 1.8557\n",
      "Epoch 66/500 | Iter 264 | Loss: 1.6490\n",
      "Epoch 67/500 | Iter 268 | Loss: 1.8201\n",
      "Epoch 68/500 | Iter 272 | Loss: 1.2076\n",
      "Epoch 69/500 | Iter 276 | Loss: 2.5355\n",
      "Epoch 70/500 | Iter 280 | Loss: 1.6661\n",
      "Epoch 71/500 | Iter 284 | Loss: 2.3540\n",
      "Epoch 72/500 | Iter 288 | Loss: 2.9712\n",
      "Epoch 73/500 | Iter 292 | Loss: 1.5866\n",
      "Epoch 74/500 | Iter 296 | Loss: 1.4869\n",
      "Epoch 75/500 | Iter 300 | Loss: 1.4846\n",
      "Epoch 76/500 | Iter 304 | Loss: 1.7673\n",
      "Epoch 77/500 | Iter 308 | Loss: 2.2390\n",
      "Epoch 78/500 | Iter 312 | Loss: 2.5064\n",
      "Epoch 79/500 | Iter 316 | Loss: 1.5995\n",
      "Epoch 80/500 | Iter 320 | Loss: 1.7292\n",
      "Epoch 81/500 | Iter 324 | Loss: 2.1611\n",
      "Epoch 82/500 | Iter 328 | Loss: 1.9300\n",
      "Epoch 83/500 | Iter 332 | Loss: 1.4589\n",
      "Epoch 84/500 | Iter 336 | Loss: 2.2946\n",
      "Epoch 85/500 | Iter 340 | Loss: 2.2613\n",
      "Epoch 86/500 | Iter 344 | Loss: 1.9003\n",
      "Epoch 87/500 | Iter 348 | Loss: 1.7634\n",
      "Epoch 88/500 | Iter 352 | Loss: 2.1781\n",
      "Epoch 89/500 | Iter 356 | Loss: 2.0643\n",
      "Epoch 90/500 | Iter 360 | Loss: 2.4994\n",
      "Epoch 91/500 | Iter 364 | Loss: 2.2844\n",
      "Epoch 92/500 | Iter 368 | Loss: 1.9235\n",
      "Epoch 93/500 | Iter 372 | Loss: 2.2264\n",
      "Epoch 94/500 | Iter 376 | Loss: 1.3825\n",
      "Epoch 95/500 | Iter 380 | Loss: 1.4064\n",
      "Epoch 96/500 | Iter 384 | Loss: 1.5933\n",
      "Epoch 97/500 | Iter 388 | Loss: 1.5932\n",
      "Epoch 98/500 | Iter 392 | Loss: 1.7696\n",
      "Epoch 99/500 | Iter 396 | Loss: 1.9468\n",
      "Epoch 100/500 | Iter 400 | Loss: 2.0845\n",
      "Epoch 101/500 | Iter 404 | Loss: 1.8975\n",
      "Epoch 102/500 | Iter 408 | Loss: 1.1149\n",
      "Epoch 103/500 | Iter 412 | Loss: 1.5229\n",
      "Epoch 104/500 | Iter 416 | Loss: 1.1045\n",
      "Epoch 105/500 | Iter 420 | Loss: 1.6143\n",
      "Epoch 106/500 | Iter 424 | Loss: 1.9492\n",
      "Epoch 107/500 | Iter 428 | Loss: 1.2374\n",
      "Epoch 108/500 | Iter 432 | Loss: 1.4291\n",
      "Epoch 109/500 | Iter 436 | Loss: 1.4907\n",
      "Epoch 110/500 | Iter 440 | Loss: 1.5784\n",
      "Epoch 111/500 | Iter 444 | Loss: 2.0642\n",
      "Epoch 112/500 | Iter 448 | Loss: 2.8634\n",
      "Epoch 113/500 | Iter 452 | Loss: 2.1458\n",
      "Epoch 114/500 | Iter 456 | Loss: 2.8393\n",
      "Epoch 115/500 | Iter 460 | Loss: 1.7068\n",
      "Epoch 116/500 | Iter 464 | Loss: 1.0122\n",
      "Epoch 117/500 | Iter 468 | Loss: 2.5330\n",
      "Epoch 118/500 | Iter 472 | Loss: 2.0292\n",
      "Epoch 119/500 | Iter 476 | Loss: 1.4461\n",
      "Epoch 120/500 | Iter 480 | Loss: 0.6776\n",
      "Epoch 121/500 | Iter 484 | Loss: 1.9743\n",
      "Epoch 122/500 | Iter 488 | Loss: 1.0256\n",
      "Epoch 123/500 | Iter 492 | Loss: 1.1321\n",
      "Epoch 124/500 | Iter 496 | Loss: 2.7377\n",
      "Epoch 125/500 | Iter 500 | Loss: 1.6152\n",
      "Epoch 126/500 | Iter 504 | Loss: 2.1568\n",
      "Epoch 127/500 | Iter 508 | Loss: 1.0789\n",
      "Epoch 128/500 | Iter 512 | Loss: 1.5408\n",
      "Epoch 129/500 | Iter 516 | Loss: 1.9996\n",
      "Epoch 130/500 | Iter 520 | Loss: 1.7167\n",
      "Epoch 131/500 | Iter 524 | Loss: 1.2077\n",
      "Epoch 132/500 | Iter 528 | Loss: 2.1302\n",
      "Epoch 133/500 | Iter 532 | Loss: 1.3558\n",
      "Epoch 134/500 | Iter 536 | Loss: 0.9819\n",
      "Epoch 135/500 | Iter 540 | Loss: 1.2625\n",
      "Epoch 136/500 | Iter 544 | Loss: 1.2522\n",
      "Epoch 137/500 | Iter 548 | Loss: 1.6174\n",
      "Epoch 138/500 | Iter 552 | Loss: 1.4866\n",
      "Epoch 139/500 | Iter 556 | Loss: 1.1739\n",
      "Epoch 140/500 | Iter 560 | Loss: 1.5175\n",
      "Epoch 141/500 | Iter 564 | Loss: 2.4786\n",
      "Epoch 142/500 | Iter 568 | Loss: 1.7842\n",
      "Epoch 143/500 | Iter 572 | Loss: 2.3790\n",
      "Epoch 144/500 | Iter 576 | Loss: 1.9831\n",
      "Epoch 145/500 | Iter 580 | Loss: 1.6538\n",
      "Epoch 146/500 | Iter 584 | Loss: 2.4507\n",
      "Epoch 147/500 | Iter 588 | Loss: 1.5184\n",
      "Epoch 148/500 | Iter 592 | Loss: 2.3779\n",
      "Epoch 149/500 | Iter 596 | Loss: 1.4557\n",
      "Epoch 150/500 | Iter 600 | Loss: 1.3249\n",
      "Epoch 151/500 | Iter 604 | Loss: 1.1314\n",
      "Epoch 152/500 | Iter 608 | Loss: 2.1061\n",
      "Epoch 153/500 | Iter 612 | Loss: 1.7216\n",
      "Epoch 154/500 | Iter 616 | Loss: 1.8402\n",
      "Epoch 155/500 | Iter 620 | Loss: 1.3486\n",
      "Epoch 156/500 | Iter 624 | Loss: 1.4205\n",
      "Epoch 157/500 | Iter 628 | Loss: 2.5507\n",
      "Epoch 158/500 | Iter 632 | Loss: 2.2871\n",
      "Epoch 159/500 | Iter 636 | Loss: 2.6346\n",
      "Epoch 160/500 | Iter 640 | Loss: 1.4963\n",
      "Epoch 161/500 | Iter 644 | Loss: 1.0719\n",
      "Epoch 162/500 | Iter 648 | Loss: 1.0213\n",
      "Epoch 163/500 | Iter 652 | Loss: 1.4973\n",
      "Epoch 164/500 | Iter 656 | Loss: 2.5319\n",
      "Epoch 165/500 | Iter 660 | Loss: 2.3238\n",
      "Epoch 166/500 | Iter 664 | Loss: 1.2388\n",
      "Epoch 167/500 | Iter 668 | Loss: 1.5718\n",
      "Epoch 168/500 | Iter 672 | Loss: 1.2148\n",
      "Epoch 169/500 | Iter 676 | Loss: 1.3748\n",
      "Epoch 170/500 | Iter 680 | Loss: 1.3654\n",
      "Epoch 171/500 | Iter 684 | Loss: 1.9074\n",
      "Epoch 172/500 | Iter 688 | Loss: 1.1929\n",
      "Epoch 173/500 | Iter 692 | Loss: 1.8719\n",
      "Epoch 174/500 | Iter 696 | Loss: 2.8489\n",
      "Epoch 175/500 | Iter 700 | Loss: 2.0328\n",
      "Epoch 176/500 | Iter 704 | Loss: 1.0888\n",
      "Epoch 177/500 | Iter 708 | Loss: 1.8228\n",
      "Epoch 178/500 | Iter 712 | Loss: 1.7389\n",
      "Epoch 179/500 | Iter 716 | Loss: 1.6822\n",
      "Epoch 180/500 | Iter 720 | Loss: 1.5789\n",
      "Epoch 181/500 | Iter 724 | Loss: 1.8694\n",
      "Epoch 182/500 | Iter 728 | Loss: 1.3070\n",
      "Epoch 183/500 | Iter 732 | Loss: 1.8842\n",
      "Epoch 184/500 | Iter 736 | Loss: 1.5005\n",
      "Epoch 185/500 | Iter 740 | Loss: 2.6557\n",
      "Epoch 186/500 | Iter 744 | Loss: 1.1199\n",
      "Epoch 187/500 | Iter 748 | Loss: 2.2576\n",
      "Epoch 188/500 | Iter 752 | Loss: 1.1394\n",
      "Epoch 189/500 | Iter 756 | Loss: 1.0372\n",
      "Epoch 190/500 | Iter 760 | Loss: 1.2170\n",
      "Epoch 191/500 | Iter 764 | Loss: 1.1628\n",
      "Epoch 192/500 | Iter 768 | Loss: 2.4542\n",
      "Epoch 193/500 | Iter 772 | Loss: 1.3316\n",
      "Epoch 194/500 | Iter 776 | Loss: 0.8477\n",
      "Epoch 195/500 | Iter 780 | Loss: 2.0114\n",
      "Epoch 196/500 | Iter 784 | Loss: 1.3993\n",
      "Epoch 197/500 | Iter 788 | Loss: 1.2623\n",
      "Epoch 198/500 | Iter 792 | Loss: 1.1242\n",
      "Epoch 199/500 | Iter 796 | Loss: 1.4570\n",
      "Epoch 200/500 | Iter 800 | Loss: 1.8283\n",
      "Epoch 201/500 | Iter 804 | Loss: 1.4195\n",
      "Epoch 202/500 | Iter 808 | Loss: 1.6525\n",
      "Epoch 203/500 | Iter 812 | Loss: 1.3439\n",
      "Epoch 204/500 | Iter 816 | Loss: 1.3824\n",
      "Epoch 205/500 | Iter 820 | Loss: 0.9054\n",
      "Epoch 206/500 | Iter 824 | Loss: 1.4244\n",
      "Epoch 207/500 | Iter 828 | Loss: 1.3199\n",
      "Epoch 208/500 | Iter 832 | Loss: 2.2088\n",
      "Epoch 209/500 | Iter 836 | Loss: 2.6776\n",
      "Epoch 210/500 | Iter 840 | Loss: 1.3440\n",
      "Epoch 211/500 | Iter 844 | Loss: 0.8796\n",
      "Epoch 212/500 | Iter 848 | Loss: 1.2933\n",
      "Epoch 213/500 | Iter 852 | Loss: 2.5818\n",
      "Epoch 214/500 | Iter 856 | Loss: 1.5810\n",
      "Epoch 215/500 | Iter 860 | Loss: 1.5559\n",
      "Epoch 216/500 | Iter 864 | Loss: 1.5892\n",
      "Epoch 217/500 | Iter 868 | Loss: 1.2394\n",
      "Epoch 218/500 | Iter 872 | Loss: 1.0889\n",
      "Epoch 219/500 | Iter 876 | Loss: 1.6752\n",
      "Epoch 220/500 | Iter 880 | Loss: 1.4102\n",
      "Epoch 221/500 | Iter 884 | Loss: 1.7066\n",
      "Epoch 222/500 | Iter 888 | Loss: 1.3624\n",
      "Epoch 223/500 | Iter 892 | Loss: 1.4706\n",
      "Epoch 224/500 | Iter 896 | Loss: 1.3879\n",
      "Epoch 225/500 | Iter 900 | Loss: 2.1162\n",
      "Epoch 226/500 | Iter 904 | Loss: 1.0978\n",
      "Epoch 227/500 | Iter 908 | Loss: 1.5052\n",
      "Epoch 228/500 | Iter 912 | Loss: 2.4494\n",
      "Epoch 229/500 | Iter 916 | Loss: 2.0690\n",
      "Epoch 230/500 | Iter 920 | Loss: 1.4271\n",
      "Epoch 231/500 | Iter 924 | Loss: 1.3430\n",
      "Epoch 232/500 | Iter 928 | Loss: 1.2929\n",
      "Epoch 233/500 | Iter 932 | Loss: 1.4857\n",
      "Epoch 234/500 | Iter 936 | Loss: 2.3159\n",
      "Epoch 235/500 | Iter 940 | Loss: 2.4124\n",
      "Epoch 236/500 | Iter 944 | Loss: 1.3639\n",
      "Epoch 237/500 | Iter 948 | Loss: 1.2004\n",
      "Epoch 238/500 | Iter 952 | Loss: 1.0279\n",
      "Epoch 239/500 | Iter 956 | Loss: 1.7034\n",
      "Epoch 240/500 | Iter 960 | Loss: 1.2220\n",
      "Epoch 241/500 | Iter 964 | Loss: 1.5218\n",
      "Epoch 242/500 | Iter 968 | Loss: 1.6710\n",
      "Epoch 243/500 | Iter 972 | Loss: 2.1483\n",
      "Epoch 244/500 | Iter 976 | Loss: 1.4448\n",
      "Epoch 245/500 | Iter 980 | Loss: 1.8512\n",
      "Epoch 246/500 | Iter 984 | Loss: 1.8396\n",
      "Epoch 247/500 | Iter 988 | Loss: 1.4960\n",
      "Epoch 248/500 | Iter 992 | Loss: 2.7215\n",
      "Epoch 249/500 | Iter 996 | Loss: 1.3462\n",
      "Epoch 250/500 | Iter 1000 | Loss: 0.9731\n",
      "Epoch 251/500 | Iter 1004 | Loss: 1.6399\n",
      "Epoch 252/500 | Iter 1008 | Loss: 1.3952\n",
      "Epoch 253/500 | Iter 1012 | Loss: 1.5727\n",
      "Epoch 254/500 | Iter 1016 | Loss: 0.9911\n",
      "Epoch 255/500 | Iter 1020 | Loss: 1.4926\n",
      "Epoch 256/500 | Iter 1024 | Loss: 2.1903\n",
      "Epoch 257/500 | Iter 1028 | Loss: 1.9682\n",
      "Epoch 258/500 | Iter 1032 | Loss: 1.3922\n",
      "Epoch 259/500 | Iter 1036 | Loss: 2.4778\n",
      "Epoch 260/500 | Iter 1040 | Loss: 1.8749\n",
      "Epoch 261/500 | Iter 1044 | Loss: 0.8218\n",
      "Epoch 262/500 | Iter 1048 | Loss: 1.2086\n",
      "Epoch 263/500 | Iter 1052 | Loss: 1.5712\n",
      "Epoch 264/500 | Iter 1056 | Loss: 1.5686\n",
      "Epoch 265/500 | Iter 1060 | Loss: 1.5850\n",
      "Epoch 266/500 | Iter 1064 | Loss: 1.3708\n",
      "Epoch 267/500 | Iter 1068 | Loss: 1.2366\n",
      "Epoch 268/500 | Iter 1072 | Loss: 1.4181\n",
      "Epoch 269/500 | Iter 1076 | Loss: 1.0445\n",
      "Epoch 270/500 | Iter 1080 | Loss: 1.2310\n",
      "Epoch 271/500 | Iter 1084 | Loss: 1.1639\n",
      "Epoch 272/500 | Iter 1088 | Loss: 1.2050\n",
      "Epoch 273/500 | Iter 1092 | Loss: 1.4875\n",
      "Epoch 274/500 | Iter 1096 | Loss: 1.2167\n",
      "Epoch 275/500 | Iter 1100 | Loss: 2.0050\n",
      "Epoch 276/500 | Iter 1104 | Loss: 1.4205\n",
      "Epoch 277/500 | Iter 1108 | Loss: 1.0319\n",
      "Epoch 278/500 | Iter 1112 | Loss: 1.1299\n",
      "Epoch 279/500 | Iter 1116 | Loss: 1.6628\n",
      "Epoch 280/500 | Iter 1120 | Loss: 1.2882\n",
      "Epoch 281/500 | Iter 1124 | Loss: 2.5052\n",
      "Epoch 282/500 | Iter 1128 | Loss: 0.9425\n",
      "Epoch 283/500 | Iter 1132 | Loss: 1.2401\n",
      "Epoch 284/500 | Iter 1136 | Loss: 1.7363\n",
      "Epoch 285/500 | Iter 1140 | Loss: 1.1227\n",
      "Epoch 286/500 | Iter 1144 | Loss: 0.8997\n",
      "Epoch 287/500 | Iter 1148 | Loss: 1.5139\n",
      "Epoch 288/500 | Iter 1152 | Loss: 1.7548\n",
      "Epoch 289/500 | Iter 1156 | Loss: 1.5349\n",
      "Epoch 290/500 | Iter 1160 | Loss: 1.3815\n",
      "Epoch 291/500 | Iter 1164 | Loss: 1.5193\n",
      "Epoch 292/500 | Iter 1168 | Loss: 1.6963\n",
      "Epoch 293/500 | Iter 1172 | Loss: 0.8054\n",
      "Epoch 294/500 | Iter 1176 | Loss: 1.2450\n",
      "Epoch 295/500 | Iter 1180 | Loss: 1.1644\n",
      "Epoch 296/500 | Iter 1184 | Loss: 1.6179\n",
      "Epoch 297/500 | Iter 1188 | Loss: 1.2611\n",
      "Epoch 298/500 | Iter 1192 | Loss: 1.4364\n",
      "Epoch 299/500 | Iter 1196 | Loss: 1.8180\n",
      "Epoch 300/500 | Iter 1200 | Loss: 1.0022\n",
      "Epoch 301/500 | Iter 1204 | Loss: 1.2014\n",
      "Epoch 302/500 | Iter 1208 | Loss: 2.4438\n",
      "Epoch 303/500 | Iter 1212 | Loss: 1.8937\n",
      "Epoch 304/500 | Iter 1216 | Loss: 1.4852\n",
      "Epoch 305/500 | Iter 1220 | Loss: 1.8730\n",
      "Epoch 306/500 | Iter 1224 | Loss: 1.3675\n",
      "Epoch 307/500 | Iter 1228 | Loss: 1.3401\n",
      "Epoch 308/500 | Iter 1232 | Loss: 1.6161\n",
      "Epoch 309/500 | Iter 1236 | Loss: 2.2405\n",
      "Epoch 310/500 | Iter 1240 | Loss: 0.6555\n",
      "Epoch 311/500 | Iter 1244 | Loss: 2.2788\n",
      "Epoch 312/500 | Iter 1248 | Loss: 1.2148\n",
      "Epoch 313/500 | Iter 1252 | Loss: 1.0637\n",
      "Epoch 314/500 | Iter 1256 | Loss: 0.9851\n",
      "Epoch 315/500 | Iter 1260 | Loss: 1.4030\n",
      "Epoch 316/500 | Iter 1264 | Loss: 1.0886\n",
      "Epoch 317/500 | Iter 1268 | Loss: 2.6357\n",
      "Epoch 318/500 | Iter 1272 | Loss: 1.4451\n",
      "Epoch 319/500 | Iter 1276 | Loss: 0.7394\n",
      "Epoch 320/500 | Iter 1280 | Loss: 1.7443\n",
      "Epoch 321/500 | Iter 1284 | Loss: 1.1413\n",
      "Epoch 322/500 | Iter 1288 | Loss: 1.1020\n",
      "Epoch 323/500 | Iter 1292 | Loss: 1.5273\n",
      "Epoch 324/500 | Iter 1296 | Loss: 2.1868\n",
      "Epoch 325/500 | Iter 1300 | Loss: 1.7289\n",
      "Epoch 326/500 | Iter 1304 | Loss: 1.3988\n",
      "Epoch 327/500 | Iter 1308 | Loss: 1.0226\n",
      "Epoch 328/500 | Iter 1312 | Loss: 1.5976\n",
      "Epoch 329/500 | Iter 1316 | Loss: 1.1115\n",
      "Epoch 330/500 | Iter 1320 | Loss: 2.3615\n",
      "Epoch 331/500 | Iter 1324 | Loss: 2.0592\n",
      "Epoch 332/500 | Iter 1328 | Loss: 1.4557\n",
      "Epoch 333/500 | Iter 1332 | Loss: 1.9932\n",
      "Epoch 334/500 | Iter 1336 | Loss: 1.9518\n",
      "Epoch 335/500 | Iter 1340 | Loss: 2.0771\n",
      "Epoch 336/500 | Iter 1344 | Loss: 0.7349\n",
      "Epoch 337/500 | Iter 1348 | Loss: 1.8373\n",
      "Epoch 338/500 | Iter 1352 | Loss: 1.3536\n",
      "Epoch 339/500 | Iter 1356 | Loss: 1.1973\n",
      "Epoch 340/500 | Iter 1360 | Loss: 1.3877\n",
      "Epoch 341/500 | Iter 1364 | Loss: 0.9191\n",
      "Epoch 342/500 | Iter 1368 | Loss: 1.5110\n",
      "Epoch 343/500 | Iter 1372 | Loss: 1.4438\n",
      "Epoch 344/500 | Iter 1376 | Loss: 1.5880\n",
      "Epoch 345/500 | Iter 1380 | Loss: 1.5990\n",
      "Epoch 346/500 | Iter 1384 | Loss: 1.1639\n",
      "Epoch 347/500 | Iter 1388 | Loss: 1.2979\n",
      "Epoch 348/500 | Iter 1392 | Loss: 0.7564\n",
      "Epoch 349/500 | Iter 1396 | Loss: 1.3379\n",
      "Epoch 350/500 | Iter 1400 | Loss: 1.2560\n",
      "Epoch 351/500 | Iter 1404 | Loss: 1.6495\n",
      "Epoch 352/500 | Iter 1408 | Loss: 2.2171\n",
      "Epoch 353/500 | Iter 1412 | Loss: 0.7004\n",
      "Epoch 354/500 | Iter 1416 | Loss: 2.1086\n",
      "Epoch 355/500 | Iter 1420 | Loss: 1.9828\n",
      "Epoch 356/500 | Iter 1424 | Loss: 1.4032\n",
      "Epoch 357/500 | Iter 1428 | Loss: 1.1000\n",
      "Epoch 358/500 | Iter 1432 | Loss: 0.8473\n",
      "Epoch 359/500 | Iter 1436 | Loss: 1.7895\n",
      "Epoch 360/500 | Iter 1440 | Loss: 1.1583\n",
      "Epoch 361/500 | Iter 1444 | Loss: 1.4489\n",
      "Epoch 362/500 | Iter 1448 | Loss: 0.7620\n",
      "Epoch 363/500 | Iter 1452 | Loss: 1.5096\n",
      "Epoch 364/500 | Iter 1456 | Loss: 2.3005\n",
      "Epoch 365/500 | Iter 1460 | Loss: 1.0392\n",
      "Epoch 366/500 | Iter 1464 | Loss: 1.2266\n",
      "Epoch 367/500 | Iter 1468 | Loss: 1.3050\n",
      "Epoch 368/500 | Iter 1472 | Loss: 1.5755\n",
      "Epoch 369/500 | Iter 1476 | Loss: 1.3856\n",
      "Epoch 370/500 | Iter 1480 | Loss: 1.6785\n",
      "Epoch 371/500 | Iter 1484 | Loss: 1.6716\n",
      "Epoch 372/500 | Iter 1488 | Loss: 0.6624\n",
      "Epoch 373/500 | Iter 1492 | Loss: 1.7065\n",
      "Epoch 374/500 | Iter 1496 | Loss: 1.1968\n",
      "Epoch 375/500 | Iter 1500 | Loss: 2.0402\n",
      "Epoch 376/500 | Iter 1504 | Loss: 1.4927\n",
      "Epoch 377/500 | Iter 1508 | Loss: 1.5301\n",
      "Epoch 378/500 | Iter 1512 | Loss: 0.8719\n",
      "Epoch 379/500 | Iter 1516 | Loss: 2.1351\n",
      "Epoch 380/500 | Iter 1520 | Loss: 1.5404\n",
      "Epoch 381/500 | Iter 1524 | Loss: 1.4400\n",
      "Epoch 382/500 | Iter 1528 | Loss: 1.4422\n",
      "Epoch 383/500 | Iter 1532 | Loss: 1.2841\n",
      "Epoch 384/500 | Iter 1536 | Loss: 1.0731\n",
      "Epoch 385/500 | Iter 1540 | Loss: 0.9452\n",
      "Epoch 386/500 | Iter 1544 | Loss: 1.7408\n",
      "Epoch 387/500 | Iter 1548 | Loss: 1.4317\n",
      "Epoch 388/500 | Iter 1552 | Loss: 0.8979\n",
      "Epoch 389/500 | Iter 1556 | Loss: 1.2144\n",
      "Epoch 390/500 | Iter 1560 | Loss: 1.9861\n",
      "Epoch 391/500 | Iter 1564 | Loss: 2.5117\n",
      "Epoch 392/500 | Iter 1568 | Loss: 1.2239\n",
      "Epoch 393/500 | Iter 1572 | Loss: 1.6496\n",
      "Epoch 394/500 | Iter 1576 | Loss: 0.9641\n",
      "Epoch 395/500 | Iter 1580 | Loss: 1.3899\n",
      "Epoch 396/500 | Iter 1584 | Loss: 1.5669\n",
      "Epoch 397/500 | Iter 1588 | Loss: 1.3253\n",
      "Epoch 398/500 | Iter 1592 | Loss: 1.3822\n",
      "Epoch 399/500 | Iter 1596 | Loss: 1.3809\n",
      "Epoch 400/500 | Iter 1600 | Loss: 1.9252\n",
      "Epoch 401/500 | Iter 1604 | Loss: 1.4473\n",
      "Epoch 402/500 | Iter 1608 | Loss: 1.3470\n",
      "Epoch 403/500 | Iter 1612 | Loss: 1.5084\n",
      "Epoch 404/500 | Iter 1616 | Loss: 0.8704\n",
      "Epoch 405/500 | Iter 1620 | Loss: 1.7280\n",
      "Epoch 406/500 | Iter 1624 | Loss: 1.3985\n",
      "Epoch 407/500 | Iter 1628 | Loss: 1.1524\n",
      "Epoch 408/500 | Iter 1632 | Loss: 1.6078\n",
      "Epoch 409/500 | Iter 1636 | Loss: 1.5298\n",
      "Epoch 410/500 | Iter 1640 | Loss: 2.2959\n",
      "Epoch 411/500 | Iter 1644 | Loss: 1.6257\n",
      "Epoch 412/500 | Iter 1648 | Loss: 2.0816\n",
      "Epoch 413/500 | Iter 1652 | Loss: 1.1909\n",
      "Epoch 414/500 | Iter 1656 | Loss: 1.2354\n",
      "Epoch 415/500 | Iter 1660 | Loss: 2.0657\n",
      "Epoch 416/500 | Iter 1664 | Loss: 0.8554\n",
      "Epoch 417/500 | Iter 1668 | Loss: 1.3512\n",
      "Epoch 418/500 | Iter 1672 | Loss: 1.2806\n",
      "Epoch 419/500 | Iter 1676 | Loss: 0.9441\n",
      "Epoch 420/500 | Iter 1680 | Loss: 1.3964\n",
      "Epoch 421/500 | Iter 1684 | Loss: 1.5302\n",
      "Epoch 422/500 | Iter 1688 | Loss: 1.4800\n",
      "Epoch 423/500 | Iter 1692 | Loss: 1.0641\n",
      "Epoch 424/500 | Iter 1696 | Loss: 0.9450\n",
      "Epoch 425/500 | Iter 1700 | Loss: 1.0635\n",
      "Epoch 426/500 | Iter 1704 | Loss: 0.6779\n",
      "Epoch 427/500 | Iter 1708 | Loss: 1.0521\n",
      "Epoch 428/500 | Iter 1712 | Loss: 1.7816\n",
      "Epoch 429/500 | Iter 1716 | Loss: 1.7759\n",
      "Epoch 430/500 | Iter 1720 | Loss: 1.5882\n",
      "Epoch 431/500 | Iter 1724 | Loss: 2.1911\n",
      "Epoch 432/500 | Iter 1728 | Loss: 2.2093\n",
      "Epoch 433/500 | Iter 1732 | Loss: 1.1140\n",
      "Epoch 434/500 | Iter 1736 | Loss: 1.4906\n",
      "Epoch 435/500 | Iter 1740 | Loss: 1.0807\n",
      "Epoch 436/500 | Iter 1744 | Loss: 2.2713\n",
      "Epoch 437/500 | Iter 1748 | Loss: 1.0068\n",
      "Epoch 438/500 | Iter 1752 | Loss: 1.4218\n",
      "Epoch 439/500 | Iter 1756 | Loss: 1.5536\n",
      "Epoch 440/500 | Iter 1760 | Loss: 1.6719\n",
      "Epoch 441/500 | Iter 1764 | Loss: 2.0480\n",
      "Epoch 442/500 | Iter 1768 | Loss: 2.1478\n",
      "Epoch 443/500 | Iter 1772 | Loss: 1.1824\n",
      "Epoch 444/500 | Iter 1776 | Loss: 1.8892\n",
      "Epoch 445/500 | Iter 1780 | Loss: 2.0671\n",
      "Epoch 446/500 | Iter 1784 | Loss: 1.8689\n",
      "Epoch 447/500 | Iter 1788 | Loss: 1.2852\n",
      "Epoch 448/500 | Iter 1792 | Loss: 1.2174\n",
      "Epoch 449/500 | Iter 1796 | Loss: 1.2099\n",
      "Epoch 450/500 | Iter 1800 | Loss: 1.5323\n",
      "Epoch 451/500 | Iter 1804 | Loss: 2.1535\n",
      "Epoch 452/500 | Iter 1808 | Loss: 1.4747\n",
      "Epoch 453/500 | Iter 1812 | Loss: 1.7060\n",
      "Epoch 454/500 | Iter 1816 | Loss: 1.6066\n",
      "Epoch 455/500 | Iter 1820 | Loss: 1.6073\n",
      "Epoch 456/500 | Iter 1824 | Loss: 2.0416\n",
      "Epoch 457/500 | Iter 1828 | Loss: 1.5000\n",
      "Epoch 458/500 | Iter 1832 | Loss: 0.6961\n",
      "Epoch 459/500 | Iter 1836 | Loss: 1.9809\n",
      "Epoch 460/500 | Iter 1840 | Loss: 0.7079\n",
      "Epoch 461/500 | Iter 1844 | Loss: 1.2277\n",
      "Epoch 462/500 | Iter 1848 | Loss: 0.6393\n",
      "Epoch 463/500 | Iter 1852 | Loss: 1.1086\n",
      "Epoch 464/500 | Iter 1856 | Loss: 2.4301\n",
      "Epoch 465/500 | Iter 1860 | Loss: 1.6057\n",
      "Epoch 466/500 | Iter 1864 | Loss: 1.2840\n",
      "Epoch 467/500 | Iter 1868 | Loss: 1.1743\n",
      "Epoch 468/500 | Iter 1872 | Loss: 0.5923\n",
      "Epoch 469/500 | Iter 1876 | Loss: 1.3151\n",
      "Epoch 470/500 | Iter 1880 | Loss: 1.5424\n",
      "Epoch 471/500 | Iter 1884 | Loss: 1.4006\n",
      "Epoch 472/500 | Iter 1888 | Loss: 1.2397\n",
      "Epoch 473/500 | Iter 1892 | Loss: 1.9055\n",
      "Epoch 474/500 | Iter 1896 | Loss: 1.5821\n",
      "Epoch 475/500 | Iter 1900 | Loss: 1.1776\n",
      "Epoch 476/500 | Iter 1904 | Loss: 1.4140\n",
      "Epoch 477/500 | Iter 1908 | Loss: 1.3201\n",
      "Epoch 478/500 | Iter 1912 | Loss: 1.1906\n",
      "Epoch 479/500 | Iter 1916 | Loss: 2.0651\n",
      "Epoch 480/500 | Iter 1920 | Loss: 1.9534\n",
      "Epoch 481/500 | Iter 1924 | Loss: 1.0591\n",
      "Epoch 482/500 | Iter 1928 | Loss: 1.7313\n",
      "Epoch 483/500 | Iter 1932 | Loss: 1.4022\n",
      "Epoch 484/500 | Iter 1936 | Loss: 1.6342\n",
      "Epoch 485/500 | Iter 1940 | Loss: 1.6003\n",
      "Epoch 486/500 | Iter 1944 | Loss: 1.6694\n",
      "Epoch 487/500 | Iter 1948 | Loss: 1.2983\n",
      "Epoch 488/500 | Iter 1952 | Loss: 1.6091\n",
      "Epoch 489/500 | Iter 1956 | Loss: 1.2972\n",
      "Epoch 490/500 | Iter 1960 | Loss: 0.8510\n",
      "Epoch 491/500 | Iter 1964 | Loss: 1.0545\n",
      "Epoch 492/500 | Iter 1968 | Loss: 0.6445\n",
      "Epoch 493/500 | Iter 1972 | Loss: 1.7177\n",
      "Epoch 494/500 | Iter 1976 | Loss: 1.9133\n",
      "Epoch 495/500 | Iter 1980 | Loss: 1.2332\n",
      "Epoch 496/500 | Iter 1984 | Loss: 1.5077\n",
      "Epoch 497/500 | Iter 1988 | Loss: 1.4588\n",
      "Epoch 498/500 | Iter 1992 | Loss: 0.6908\n",
      "Epoch 499/500 | Iter 1996 | Loss: 0.7510\n",
      "Time taken: 0.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/05 11:11:16 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/10/05 11:11:22 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ STO DONE | Accuracy: 0.5450 | Macro F1: 0.5025\n",
      "🏃 View run sto_L2_reg at: https://mlflow.ml.brain.cs.ait.ac.th/#/experiments/930466277354100292/runs/8283955e72664249a58510343f298943\n",
      "🧪 View experiment at: https://mlflow.ml.brain.cs.ait.ac.th/#/experiments/930466277354100292\n",
      "\n",
      "🏁 All optimization experiments completed and logged to MLflow!\n",
      "🔗 View results: https://mlflow.ml.brain.cs.ait.ac.th\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import mlflow\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ✅ Connect to AIT MLflow server\n",
    "mlflow.set_tracking_uri(\"https://mlflow.ml.brain.cs.ait.ac.th\")\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"admin\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"password\"\n",
    "mlflow.set_experiment(\"st125999_a3\")\n",
    "\n",
    "print(\"✅ Connected to AIT MLflow server as admin; experiment 'A3_st125999' set.\")\n",
    "\n",
    "# ✅ Run all three optimization methods\n",
    "methods = [\"batch\", \"minibatch\", \"sto\"]\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\n🚀 Running {method.upper()} Optimization...\\n\")\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        k=Y_train_encoded.shape[1],\n",
    "        n=X_train.shape[1],\n",
    "        method=method,\n",
    "        alpha=0.001,\n",
    "        epochs=500,          # ✅ Added epochs\n",
    "        max_iter=2000,\n",
    "        use_penalty=True,\n",
    "        lambda_=0.1\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{method}_L2_reg\"):\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params({\n",
    "            \"method\": method,\n",
    "            \"alpha\": model.alpha,\n",
    "            \"epochs\": model.epochs,      # ✅ Log epochs\n",
    "            \"max_iter\": model.max_iter,\n",
    "            \"lambda_\": model.lambda_,\n",
    "            \"use_penalty\": model.use_penalty,\n",
    "            \"n_classes\": model.k,\n",
    "            \"n_features\": model.n\n",
    "        })\n",
    "\n",
    "        # ⏱ Train model\n",
    "        model.fit(X_train, Y_train_encoded)\n",
    "\n",
    "        # 🔮 Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # 🎯 Evaluate\n",
    "        acc = model.accuracy(y_test, y_pred)\n",
    "        precision, recall, f1, support = model.precision_recall_f1_per_class(y_test, y_pred)\n",
    "        macro_p, macro_r, macro_f = model.macro_avg(precision, recall, f1)\n",
    "        weighted_p, weighted_r, weighted_f = model.weighted_avg(precision, recall, f1, support)\n",
    "\n",
    "        # 📈 Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_precision\": macro_p,\n",
    "            \"macro_recall\": macro_r,\n",
    "            \"macro_f1\": macro_f,\n",
    "            \"weighted_precision\": weighted_p,\n",
    "            \"weighted_recall\": weighted_r,\n",
    "            \"weighted_f1\": weighted_f\n",
    "        })\n",
    "\n",
    "        # Per-class metrics\n",
    "        for c in precision.keys():\n",
    "            mlflow.log_metric(f\"precision_class_{c}\", precision[c])\n",
    "            mlflow.log_metric(f\"recall_class_{c}\", recall[c])\n",
    "            mlflow.log_metric(f\"f1_class_{c}\", f1[c])\n",
    "\n",
    "        # 📉 Plot loss curve\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(model.losses, label=f\"{method} Loss\")\n",
    "        plt.title(f\"Loss Curve ({method})\")\n",
    "        plt.xlabel(\"Epoch\")              # ✅ More appropriate now\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        fname = f\"loss_curve_{method}.png\"\n",
    "        plt.savefig(fname)\n",
    "        plt.close()\n",
    "\n",
    "        # Log artifact\n",
    "        mlflow.log_artifact(fname)\n",
    "\n",
    "        # 💾 Save & log trained model ✅\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=f\"{method}_model\"\n",
    "        )\n",
    "\n",
    "        print(f\"✅ {method.upper()} DONE | Accuracy: {acc:.4f} | Macro F1: {macro_f:.4f}\")\n",
    "\n",
    "print(\"\\n🏁 All optimization experiments completed and logged to MLflow!\")\n",
    "print(\"🔗 View results: https://mlflow.ml.brain.cs.ait.ac.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0094fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best run: 71cb9116e48a4541814066e11916fb48 | Accuracy: 0.5496\n",
      "📁 Found model artifact path: 'minibatch_model'\n",
      "📦 Registering model from: runs:/71cb9116e48a4541814066e11916fb48/minibatch_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'st125999-a3-model' already exists. Creating a new version of this model...\n",
      "2025/10/05 11:11:27 WARNING mlflow.tracking._model_registry.fluent: Run with id 71cb9116e48a4541814066e11916fb48 has no artifacts at artifact path 'minibatch_model', registering model based on models:/m-3c6f2e7cc28e4991bc34a0e3d36a2625 instead\n",
      "2025/10/05 11:11:28 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: st125999-a3-model, version 8\n",
      "Created version '8' of model 'st125999-a3-model'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Registered 'st125999-a3-model' version 8\n",
      "🎯 Set alias 'Staging' for st125999-a3-model v8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.exceptions import MlflowException\n",
    "\n",
    "# ✅ Connect\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"admin\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"password\"\n",
    "mlflow.set_tracking_uri(\"https://mlflow.ml.brain.cs.ait.ac.th\")\n",
    "\n",
    "experiment_name = \"st125999_a3\"\n",
    "model_name = \"st125999-a3-model\"\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "if experiment is None:\n",
    "    raise ValueError(f\"❌ Experiment '{experiment_name}' not found!\")\n",
    "\n",
    "# ✅ Find best run by accuracy\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"metrics.accuracy DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "if not runs:\n",
    "    raise ValueError(\"❌ No runs found.\")\n",
    "\n",
    "best_run = runs[0]\n",
    "best_run_id = best_run.info.run_id\n",
    "best_acc = best_run.data.metrics.get(\"accuracy\", 0.0)\n",
    "print(f\"🏆 Best run: {best_run_id} | Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "# ✅ Find which artifact path exists in this run\n",
    "possible_paths = [\"batch_model\", \"minibatch_model\", \"sto_model\"]\n",
    "found_path = None\n",
    "for path in possible_paths:\n",
    "    try:\n",
    "        artifacts = client.list_artifacts(best_run_id, path)\n",
    "        if artifacts:  # if directory exists\n",
    "            found_path = path\n",
    "            break\n",
    "    except MlflowException:\n",
    "        continue\n",
    "\n",
    "if found_path is None:\n",
    "    raise ValueError(f\"❌ No logged model found under any expected path in run {best_run_id}\")\n",
    "\n",
    "print(f\"📁 Found model artifact path: '{found_path}'\")\n",
    "\n",
    "# ✅ Register model\n",
    "model_uri = f\"runs:/{best_run_id}/{found_path}\"\n",
    "print(f\"📦 Registering model from: {model_uri}\")\n",
    "registered = mlflow.register_model(model_uri=model_uri, name=model_name)\n",
    "new_version = registered.version\n",
    "print(f\"✅ Registered '{model_name}' version {new_version}\")\n",
    "\n",
    "# ✅ Wait and set alias\n",
    "time.sleep(5)\n",
    "try:\n",
    "    client.set_registered_model_alias(\n",
    "        name=model_name,\n",
    "        alias=\"Staging\",\n",
    "        version=new_version\n",
    "    )\n",
    "    print(f\"🎯 Set alias 'Staging' for {model_name} v{new_version}\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Alias assignment failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6956936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ModelVersion: aliases=['Staging'], creation_timestamp=1759637478699, current_stage='None', deployment_job_state=<ModelVersionDeploymentJobState: current_task_name='', job_id='', job_state='DEPLOYMENT_JOB_CONNECTION_STATE_UNSPECIFIED', run_id='', run_state='DEPLOYMENT_JOB_RUN_STATE_UNSPECIFIED'>, description='', last_updated_timestamp=1759637478699, metrics=None, model_id=None, name='st125999-a3-model', params=None, run_id='71cb9116e48a4541814066e11916fb48', run_link='', source='models:/m-3c6f2e7cc28e4991bc34a0e3d36a2625', status='READY', status_message=None, tags={}, user_id='', version='8'>,\n",
       " <ModelVersion: aliases=[], creation_timestamp=1759600653314, current_stage='None', deployment_job_state=<ModelVersionDeploymentJobState: current_task_name='', job_id='', job_state='DEPLOYMENT_JOB_CONNECTION_STATE_UNSPECIFIED', run_id='', run_state='DEPLOYMENT_JOB_RUN_STATE_UNSPECIFIED'>, description='', last_updated_timestamp=1759600653314, metrics=None, model_id=None, name='st125999-a3-model', params=None, run_id='71cb9116e48a4541814066e11916fb48', run_link='', source='models:/m-3c6f2e7cc28e4991bc34a0e3d36a2625', status='READY', status_message=None, tags={}, user_id='', version='7'>,\n",
       " <ModelVersion: aliases=[], creation_timestamp=1759599883298, current_stage='None', deployment_job_state=<ModelVersionDeploymentJobState: current_task_name='', job_id='', job_state='DEPLOYMENT_JOB_CONNECTION_STATE_UNSPECIFIED', run_id='', run_state='DEPLOYMENT_JOB_RUN_STATE_UNSPECIFIED'>, description='', last_updated_timestamp=1759599883298, metrics=None, model_id=None, name='st125999-a3-model', params=None, run_id='3e8a61b2a72f48eda2167f7ac7b56fc9', run_link='', source='models:/m-8698f97fadb7490095abe47f3127dd5d', status='READY', status_message=None, tags={}, user_id='', version='6'>,\n",
       " <ModelVersion: aliases=[], creation_timestamp=1759599042670, current_stage='None', deployment_job_state=<ModelVersionDeploymentJobState: current_task_name='', job_id='', job_state='DEPLOYMENT_JOB_CONNECTION_STATE_UNSPECIFIED', run_id='', run_state='DEPLOYMENT_JOB_RUN_STATE_UNSPECIFIED'>, description='', last_updated_timestamp=1759599042670, metrics=None, model_id=None, name='st125999-a3-model', params=None, run_id='3e8a61b2a72f48eda2167f7ac7b56fc9', run_link='', source='models:/m-8698f97fadb7490095abe47f3127dd5d', status='READY', status_message=None, tags={}, user_id='', version='5'>,\n",
       " <ModelVersion: aliases=[], creation_timestamp=1759595413065, current_stage='None', deployment_job_state=<ModelVersionDeploymentJobState: current_task_name='', job_id='', job_state='DEPLOYMENT_JOB_CONNECTION_STATE_UNSPECIFIED', run_id='', run_state='DEPLOYMENT_JOB_RUN_STATE_UNSPECIFIED'>, description='', last_updated_timestamp=1759595413065, metrics=None, model_id=None, name='st125999-a3-model', params=None, run_id='3e8a61b2a72f48eda2167f7ac7b56fc9', run_link='', source='models:/m-8698f97fadb7490095abe47f3127dd5d', status='READY', status_message=None, tags={}, user_id='', version='4'>,\n",
       " <ModelVersion: aliases=[], creation_timestamp=1759594051614, current_stage='None', deployment_job_state=<ModelVersionDeploymentJobState: current_task_name='', job_id='', job_state='DEPLOYMENT_JOB_CONNECTION_STATE_UNSPECIFIED', run_id='', run_state='DEPLOYMENT_JOB_RUN_STATE_UNSPECIFIED'>, description='', last_updated_timestamp=1759594051614, metrics=None, model_id=None, name='st125999-a3-model', params=None, run_id='3e8a61b2a72f48eda2167f7ac7b56fc9', run_link='', source='models:/st125999-a3-model/2', status='READY', status_message=None, tags={}, user_id='', version='3'>,\n",
       " <ModelVersion: aliases=[], creation_timestamp=1759593347536, current_stage='None', deployment_job_state=<ModelVersionDeploymentJobState: current_task_name='', job_id='', job_state='DEPLOYMENT_JOB_CONNECTION_STATE_UNSPECIFIED', run_id='', run_state='DEPLOYMENT_JOB_RUN_STATE_UNSPECIFIED'>, description='', last_updated_timestamp=1759593347536, metrics=None, model_id=None, name='st125999-a3-model', params=None, run_id='3e8a61b2a72f48eda2167f7ac7b56fc9', run_link='', source='mlflow-artifacts:/930466277354100292/models/m-8698f97fadb7490095abe47f3127dd5d/artifacts', status='READY', status_message=None, tags={}, user_id='', version='2'>,\n",
       " <ModelVersion: aliases=[], creation_timestamp=1759593154067, current_stage='None', deployment_job_state=<ModelVersionDeploymentJobState: current_task_name='', job_id='', job_state='DEPLOYMENT_JOB_CONNECTION_STATE_UNSPECIFIED', run_id='', run_state='DEPLOYMENT_JOB_RUN_STATE_UNSPECIFIED'>, description='', last_updated_timestamp=1759593154067, metrics=None, model_id=None, name='st125999-a3-model', params=None, run_id='3e8a61b2a72f48eda2167f7ac7b56fc9', run_link='', source='models:/m-8698f97fadb7490095abe47f3127dd5d', status='READY', status_message=None, tags={}, user_id='', version='1'>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.search_model_versions(\"name='st125999-a3-model'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67dffde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 8 | Stage: None | Status: READY\n",
      "Version 7 | Stage: None | Status: READY\n",
      "Version 6 | Stage: None | Status: READY\n",
      "Version 5 | Stage: None | Status: READY\n",
      "Version 4 | Stage: None | Status: READY\n",
      "Version 3 | Stage: None | Status: READY\n",
      "Version 2 | Stage: None | Status: READY\n",
      "Version 1 | Stage: None | Status: READY\n"
     ]
    }
   ],
   "source": [
    "import os, mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"admin\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"password\"\n",
    "\n",
    "mlflow.set_tracking_uri(\"https://mlflow.ml.brain.cs.ait.ac.th\")\n",
    "client = MlflowClient()\n",
    "\n",
    "# Check if registry responds to GET calls\n",
    "for mv in client.search_model_versions(\"name='st125999-a3-model'\"):\n",
    "    print(f\"Version {mv.version} | Stage: {mv.current_stage} | Status: {mv.status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8216185e",
   "metadata": {},
   "source": [
    "#inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a728f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Loading model from MLflow registry...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 19.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "\n",
      "📋 New test samples:\n",
      "   year  max_power  mileage    brand    fuel  true_label\n",
      "0  2018       85.0     20.0   Maruti  Petrol           2\n",
      "1  2014      120.0     16.0  Hyundai  Diesel           3\n",
      "2  2020       90.0     22.5    Honda  Petrol           1\n",
      "3  2015       70.0     24.0     Tata  Diesel           0\n",
      "4  2019      110.0     18.0     Ford  Petrol           2\n",
      "\n",
      "🧭 Sanity Check: X_infer shape = (5, 6)\n",
      "\n",
      "✅ Predictions:\n",
      "   year    brand    fuel  true_label  predicted_class\n",
      "0  2018   Maruti  Petrol           2                2\n",
      "1  2014  Hyundai  Diesel           3                3\n",
      "2  2020    Honda  Petrol           1                2\n",
      "3  2015     Tata  Diesel           0                1\n",
      "4  2019     Ford  Petrol           2                2\n",
      "\n",
      "📊 Classification Report:\n",
      "Class\tPrecision\tRecall\tF1-Score\tSupport\n",
      "------------------------------------------------------------\n",
      "0\t0.00\t\t0.00\t0.00\t\t1\n",
      "1\t0.00\t\t0.00\t0.00\t\t1\n",
      "2\t0.67\t\t1.00\t0.80\t\t2\n",
      "3\t1.00\t\t1.00\t1.00\t\t1\n",
      "------------------------------------------------------------\n",
      "Macro Avg\t0.42\t\t0.50\t0.45\n",
      "Weighted Avg\t0.47\t\t0.60\t0.52\n",
      "Accuracy\t0.60\n",
      "\n",
      "— Extras —\n",
      "Accuracy:        0.6000\n",
      "Macro Precision: 0.4167 | Macro Recall: 0.5000 | Macro F1: 0.4500\n",
      "Wtd Precision:   0.4667 | Wtd Recall:   0.6000 | Wtd F1:   0.5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# =====================================\n",
    "# 1) Load model from MLflow\n",
    "# =====================================\n",
    "mlflow.set_tracking_uri(\"https://mlflow.ml.brain.cs.ait.ac.th\")\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"admin\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"password\"\n",
    "\n",
    "MODEL_NAME = \"st125999-a3-model\"\n",
    "ALIAS = \"Staging\"\n",
    "\n",
    "print(\"🔗 Loading model from MLflow registry...\")\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{MODEL_NAME}@{ALIAS}\")\n",
    "print(\"✅ Model loaded successfully!\\n\")\n",
    "\n",
    "# =====================================\n",
    "# 2) New test data (with true labels)\n",
    "#    Edit/expand these rows as you like\n",
    "# =====================================\n",
    "new_data = pd.DataFrame([\n",
    "    [2018,  85.0, 20.0, \"Maruti\",  \"Petrol\", 2],\n",
    "    [2014, 120.0, 16.0, \"Hyundai\", \"Diesel\", 3],\n",
    "    [2020,  90.0, 22.5, \"Honda\",   \"Petrol\", 1],\n",
    "    [2015,  70.0, 24.0, \"Tata\",    \"Diesel\", 0],\n",
    "    [2019, 110.0, 18.0, \"Ford\",    \"Petrol\", 2],\n",
    "], columns=[\"year\", \"max_power\", \"mileage\", \"brand\", \"fuel\", \"true_label\"])\n",
    "\n",
    "print(\"📋 New test samples:\")\n",
    "print(new_data)\n",
    "\n",
    "# =====================================\n",
    "# 3) Preprocess EXACTLY like training\n",
    "#    (LabelEncode -> StandardScale -> add intercept)\n",
    "# =====================================\n",
    "X_raw = new_data[[\"year\", \"max_power\", \"mileage\", \"brand\", \"fuel\"]].copy()\n",
    "y_true = new_data[\"true_label\"].astype(int).values\n",
    "\n",
    "le_brand = LabelEncoder()\n",
    "le_fuel  = LabelEncoder()\n",
    "X_raw[\"brand\"] = le_brand.fit_transform(X_raw[\"brand\"])\n",
    "X_raw[\"fuel\"]  = le_fuel.fit_transform(X_raw[\"fuel\"])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw.values)\n",
    "\n",
    "X_infer = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]  # add bias\n",
    "print(\"\\n🧭 Sanity Check: X_infer shape =\", X_infer.shape)\n",
    "\n",
    "# =====================================\n",
    "# 4) Predict with the loaded model\n",
    "# =====================================\n",
    "y_pred = model.predict(X_infer).astype(int)\n",
    "new_data[\"predicted_class\"] = y_pred\n",
    "\n",
    "print(\"\\n✅ Predictions:\")\n",
    "print(new_data[[\"year\", \"brand\", \"fuel\", \"true_label\", \"predicted_class\"]])\n",
    "\n",
    "# =====================================\n",
    "# 5) Print YOUR metrics using YOUR class\n",
    "#    (Assumes your LogisticRegression class is already defined above)\n",
    "# =====================================\n",
    "k = len(np.unique(y_true))\n",
    "n = X_infer.shape[1]\n",
    "dummy = LogisticRegression(k=k, n=n, method=\"batch\")  # weights not used for metrics\n",
    "\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "dummy.classification_report(y_true, y_pred)\n",
    "\n",
    "# If you also want the individual numbers:\n",
    "precision, recall, f1, support = dummy.precision_recall_f1_per_class(y_true, y_pred)\n",
    "macro_p, macro_r, macro_f = dummy.macro_avg(precision, recall, f1)\n",
    "weighted_p, weighted_r, weighted_f = dummy.weighted_avg(precision, recall, f1, support)\n",
    "acc = dummy.accuracy(y_true, y_pred)\n",
    "\n",
    "print(\"\\n— Extras —\")\n",
    "print(f\"Accuracy:        {acc:.4f}\")\n",
    "print(f\"Macro Precision: {macro_p:.4f} | Macro Recall: {macro_r:.4f} | Macro F1: {macro_f:.4f}\")\n",
    "print(f\"Wtd Precision:   {weighted_p:.4f} | Wtd Recall:   {weighted_r:.4f} | Wtd F1:   {weighted_f:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0586ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Connecting to MLflow...\n",
      "📦 Loading model: st125999-a3-model@Staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 16.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "\n",
      "=======================================================\n",
      "       🔬 Inference Testing on Sample Data\n",
      "=======================================================\n",
      "\n",
      "--- Testing: Compact Petrol Hatchback ---\n",
      "Input Features:\n",
      " year  max_power  mileage  brand   fuel\n",
      " 2020       82.0     22.0 Maruti Petrol\n",
      "\n",
      "Prediction:\n",
      "  -> Predicted Category: Class 1\n",
      "\n",
      "--- Testing: Luxury Diesel Sedan ---\n",
      "Input Features:\n",
      " year  max_power  mileage brand   fuel\n",
      " 2022      160.0     17.0   BMW Diesel\n",
      "\n",
      "Prediction:\n",
      "  -> Predicted Category: Class 2\n",
      "\n",
      "✅ Inference Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# =====================================\n",
    "# 1️⃣ Connect & Load Model\n",
    "# =====================================\n",
    "mlflow.set_tracking_uri(\"https://mlflow.ml.brain.cs.ait.ac.th\")\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"admin\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"password\"\n",
    "\n",
    "MODEL_NAME = \"st125999-a3-model\"\n",
    "ALIAS = \"Staging\"\n",
    "\n",
    "print(\"🔗 Connecting to MLflow...\")\n",
    "print(f\"📦 Loading model: {MODEL_NAME}@{ALIAS}\")\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{MODEL_NAME}@{ALIAS}\")\n",
    "print(\"✅ Model loaded successfully!\\n\")\n",
    "\n",
    "# =====================================\n",
    "# 2️⃣ Two Test Samples\n",
    "# =====================================\n",
    "samples = [\n",
    "    {\n",
    "        \"title\": \"Compact Petrol Hatchback\",\n",
    "        \"features\": {\n",
    "            \"year\": 2020,\n",
    "            \"max_power\": 82.0,\n",
    "            \"mileage\": 22.0,\n",
    "            \"brand\": \"Maruti\",\n",
    "            \"fuel\": \"Petrol\"\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Luxury Diesel Sedan\",\n",
    "        \"features\": {\n",
    "            \"year\": 2022,\n",
    "            \"max_power\": 160.0,\n",
    "            \"mileage\": 17.0,\n",
    "            \"brand\": \"BMW\",\n",
    "            \"fuel\": \"Diesel\"\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# =====================================\n",
    "# 3️⃣ Preprocessing + Inference\n",
    "# =====================================\n",
    "print(\"=======================================================\")\n",
    "print(\"       🔬 Inference Testing on Sample Data\")\n",
    "print(\"=======================================================\\n\")\n",
    "\n",
    "# Prepare encoders and scaler\n",
    "le_brand = LabelEncoder()\n",
    "le_fuel = LabelEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Build DataFrame for encoding/scaling\n",
    "df_all = pd.DataFrame([s[\"features\"] for s in samples])\n",
    "df_all[\"brand\"] = le_brand.fit_transform(df_all[\"brand\"])\n",
    "df_all[\"fuel\"] = le_fuel.fit_transform(df_all[\"fuel\"])\n",
    "\n",
    "# Scale numeric columns\n",
    "X_scaled = scaler.fit_transform(df_all)\n",
    "X_infer = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]  # add bias column\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(X_infer).astype(int)\n",
    "\n",
    "# =====================================\n",
    "# 4️⃣ Print Results Nicely\n",
    "# =====================================\n",
    "for i, sample in enumerate(samples):\n",
    "    title = sample[\"title\"]\n",
    "    features = pd.DataFrame([sample[\"features\"]])\n",
    "    print(f\"--- Testing: {title} ---\")\n",
    "    print(\"Input Features:\")\n",
    "    print(features.to_string(index=False))\n",
    "    print(f\"\\nPrediction:\\n  -> Predicted Category: Class {preds[i]}\\n\")\n",
    "\n",
    "print(\"✅ Inference Completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
