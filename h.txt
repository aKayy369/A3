import numpy as np
import matplotlib.pyplot as plt
import time

class LogisticRegression:
    
    def __init__(self, k, n, method, alpha=0.001, max_iter=5000, use_penalty=False, lambda_=0.01):
        """
        Parameters
        ----------
        k : int
            Number of classes
        n : int
            Number of features
        method : str
            "batch", "minibatch", or "sto"
        alpha : float
            Learning rate
        max_iter : int
            Number of iterations
        use_penalty : bool
            Whether to apply Ridge (L2) penalty
        lambda_ : float
            Regularization strength
        """
        self.k = k  
        self.n = n  
        self.alpha = alpha
        self.max_iter = max_iter
        self.method = method
        self.use_penalty = use_penalty
        self.lambda_ = lambda_

    def fit(self, X, Y):
        self.W = np.random.rand(self.n, self.k)
        self.losses = []
        
        if self.method == "batch":
            start_time = time.time()
            for i in range(self.max_iter):
                loss, grad =  self.gradient(X, Y)
                self.losses.append(loss)
                self.W = self.W - self.alpha * grad
                if i % 500 == 0:
                    print(f"Loss at iteration {i}: {loss:.4f}")
            print(f"Time taken: {time.time() - start_time:.2f}s")
            
        elif self.method == "minibatch":
            start_time = time.time()
            batch_size = int(0.3 * X.shape[0])
            for i in range(self.max_iter):
                ix = np.random.randint(0, X.shape[0])
                batch_X = X[ix:ix+batch_size]
                batch_Y = Y[ix:ix+batch_size]
                loss, grad = self.gradient(batch_X, batch_Y)
                self.losses.append(loss)
                self.W = self.W - self.alpha * grad
                if i % 500 == 0:
                    print(f"Loss at iteration {i}: {loss:.4f}")
            print(f"Time taken: {time.time() - start_time:.2f}s")
            
        elif self.method == "sto":
            start_time = time.time()
            used_ix = []
            for i in range(self.max_iter):
                idx = np.random.randint(X.shape[0])
                while i in used_ix:
                    idx = np.random.randint(X.shape[0])
                X_train = X[idx, :].reshape(1, -1)
                Y_train = Y[idx]
                loss, grad = self.gradient(X_train, Y_train)
                self.losses.append(loss)
                self.W = self.W - self.alpha * grad
                
                used_ix.append(i)
                if len(used_ix) == X.shape[0]:
                    used_ix = []
                if i % 500 == 0:
                    print(f"Loss at iteration {i}: {loss:.4f}")
            print(f"Time taken: {time.time() - start_time:.2f}s")
            
        else:
            raise ValueError('Method must be "batch", "minibatch" or "sto".')
        
        
    def gradient(self, X, Y):
        m = X.shape[0]
        h = self.h_theta(X, self.W)
        
        # Cross-entropy loss
        loss = - np.sum(Y * np.log(h + 1e-9)) / m
        
        # Add Ridge (L2) penalty
        if self.use_penalty:
            loss += (self.lambda_ / (2 * m)) * np.sum(self.W ** 2)
        
        error = h - Y
        grad = self.softmax_grad(X, error) / m
        
        # Add penalty to gradient
        if self.use_penalty:
            grad += (self.lambda_ / m) * self.W
        
        return loss, grad

    def softmax(self, theta_t_x):
        exp_scores = np.exp(theta_t_x - np.max(theta_t_x, axis=1, keepdims=True))  # stability
        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

    def softmax_grad(self, X, error):
        return  X.T @ error

    def h_theta(self, X, W):
        return self.softmax(X @ W)
    
    def predict(self, X_test):
        return np.argmax(self.h_theta(X_test, self.W), axis=1)
    
    def plot(self):
        plt.plot(np.arange(len(self.losses)), self.losses, label="Train Losses")
        plt.title("Loss Curve")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()
        plt.show()

    # -----------------------
    # Evaluation Metrics
    # -----------------------

    def accuracy(self, y_true, y_pred):
        return np.sum(y_true == y_pred) / len(y_true)
    
    def precision_recall_f1_per_class(self, y_true, y_pred):
        classes = np.unique(y_true)
        precision, recall, f1, support = {}, {}, {}, {}

        for c in classes:
            TP = np.sum((y_true == c) & (y_pred == c))
            FP = np.sum((y_true != c) & (y_pred == c))
            FN = np.sum((y_true == c) & (y_pred != c))
            prec = TP / (TP + FP) if (TP + FP) != 0 else 0
            rec = TP / (TP + FN) if (TP + FN) != 0 else 0
            f1_c = (2 * prec * rec) / (prec + rec) if (prec + rec) != 0 else 0
            precision[c], recall[c], f1[c], support[c] = prec, rec, f1_c, np.sum(y_true == c)
        return precision, recall, f1, support
    
    def macro_avg(self, precision, recall, f1):
        return np.mean(list(precision.values())), np.mean(list(recall.values())), np.mean(list(f1.values()))
    
    def weighted_avg(self, precision, recall, f1, support):
        total = np.sum(list(support.values()))
        wp = np.sum([precision[c] * support[c] for c in support]) / total
        wr = np.sum([recall[c] * support[c] for c in support]) / total
        wf = np.sum([f1[c] * support[c] for c in support]) / total
        return wp, wr, wf
    
    def classification_report(self, y_true, y_pred):
        precision, recall, f1, support = self.precision_recall_f1_per_class(y_true, y_pred)
        macro_p, macro_r, macro_f = self.macro_avg(precision, recall, f1)
        weighted_p, weighted_r, weighted_f = self.weighted_avg(precision, recall, f1, support)
        acc = self.accuracy(y_true, y_pred)

        print("Class\tPrecision\tRecall\tF1-Score\tSupport")
        print("-" * 60)
        for c in precision.keys():
            print(f"{c}\t{precision[c]:.2f}\t\t{recall[c]:.2f}\t{f1[c]:.2f}\t\t{support[c]}")
        print("-" * 60)
        print(f"Macro Avg\t{macro_p:.2f}\t\t{macro_r:.2f}\t{macro_f:.2f}")
        print(f"Weighted Avg\t{weighted_p:.2f}\t\t{weighted_r:.2f}\t{weighted_f:.2f}")
        print(f"Accuracy\t{acc:.2f}")


